{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10701349,"sourceType":"datasetVersion","datasetId":6631773},{"sourceId":220912,"sourceType":"modelInstanceVersion","modelInstanceId":188404,"modelId":210431}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import yaml\nimport os\nfrom PIL import Image\nfrom tqdm import tqdm\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import Adam\nimport torchvision.transforms as transforms\nimport torchvision\nfrom torchvision.utils import make_grid\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nDDPM_CONFIG = \"/kaggle/input/ddpm-configs/other/ddpm-configs/1/ddpm.yaml\"\nIMG_SAVE_ROOT = \"outputs/DDPM\"\nCKPT_SAVE = \"checkpoints/DDPM\"\n\nos.makedirs(IMG_SAVE_ROOT,exist_ok=True)\nos.makedirs(CKPT_SAVE,exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:56:16.970349Z","iopub.execute_input":"2025-02-09T08:56:16.970607Z","iopub.status.idle":"2025-02-09T08:56:21.221097Z","shell.execute_reply.started":"2025-02-09T08:56:16.970563Z","shell.execute_reply":"2025-02-09T08:56:21.220155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ROOT = \"/kaggle/input/celebav-randomframes/data\"\nBATCH_SIZE = 2\nIMG_SIZE = 64\nSAMPLE_STEP = 2500\nNUM_TIMESTEPS = 1000\nCKPT_PATH = \"/kaggle/input/hueshift-ddpm/pytorch/v4/1/ddpm.pth\"\nRESUME = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:56:21.222045Z","iopub.execute_input":"2025-02-09T08:56:21.222505Z","iopub.status.idle":"2025-02-09T08:56:21.226679Z","shell.execute_reply.started":"2025-02-09T08:56:21.222475Z","shell.execute_reply":"2025-02-09T08:56:21.22587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Read the config file #\nwith open(DDPM_CONFIG, 'r') as file:\n    try:\n        config = yaml.safe_load(file)\n    except yaml.YAMLError as exc:\n        print(exc)\nddpm_model_config = config['model_config']\nddpm_dataset_config = config['dataset_config']\nddpm_training_config = config['training_config']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:56:21.227509Z","iopub.execute_input":"2025-02-09T08:56:21.227804Z","iopub.status.idle":"2025-02-09T08:56:21.24985Z","shell.execute_reply.started":"2025-02-09T08:56:21.227776Z","shell.execute_reply":"2025-02-09T08:56:21.249033Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Dataset(Dataset):\n    def __init__(self , root , transform=None):\n        self.root = root\n        self.files = os.listdir(root)\n        self.len = len(self.files)\n        if transform is not None:\n            self.transforms = transforms.Compose(transform)\n        else:\n            self.transforms = None\n\n        \n    def __getitem__(self , i):\n        file = self.files[i]\n        im = cv2.imread(f'{self.root}/{file}')\n        im = cv2.cvtColor(im, cv2.COLOR_BGR2LAB)\n        if self.transforms is not None:\n            im = self.transforms(im)\n        return im\n    \n    def __len__(self):\n        return self.len","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:56:21.251685Z","iopub.execute_input":"2025-02-09T08:56:21.251887Z","iopub.status.idle":"2025-02-09T08:56:21.256816Z","shell.execute_reply.started":"2025-02-09T08:56:21.251869Z","shell.execute_reply":"2025-02-09T08:56:21.255908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LinearNoiseSchedule:\n    def __init__(self, T):\n        super().__init__()\n        beta_start = 1E-4\n        beta_end = 0.02\n\n        self.beta = torch.linspace(beta_start,beta_end,T,dtype=torch.float32)\n        self.alpha = 1 - self.beta\n        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n\n        # for forward process\n        self.sqrt_alpha_hat = torch.sqrt(self.alpha_hat) # for mean\n        self.sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat) #for std_dev\n\n        # for sampling process\n        self.one_by_sqrt_alpha = 1. / torch.sqrt(self.alpha) # for mean\n        self.one_by_sqrt_one_minus_alpha_hat = 1. / self.sqrt_one_minus_alpha_hat # for mean\n        self.sqrt_beta = torch.sqrt(self.beta) # for std_dev\n        \n    def forward(self, x0, t):\n        #separte L from A and B --> c0\n        l,c0 = torch.split(x0,[1,2],dim=1)\n        noise = torch.randn_like(c0).to(x0.device)\n\n        sqrt_alpha_hat = self.sqrt_alpha_hat.to(x0.device)[t]\n        sqrt_one_minus_alpha_hat = self.sqrt_one_minus_alpha_hat.to(x0.device)[t]\n\n        # reshape to match no of dims (b,) -> (b,c,h,w)\n        for _ in range(len(x0.shape) - 1):\n            sqrt_alpha_hat = sqrt_alpha_hat.unsqueeze(-1)\n            sqrt_one_minus_alpha_hat = sqrt_one_minus_alpha_hat.unsqueeze(-1)\n\n        mean = sqrt_alpha_hat.to(x0.device) * c0\n        std_dev = sqrt_one_minus_alpha_hat.to(x0.device)\n\n        sample = mean + std_dev * noise \n\n        sample = torch.cat([l,sample],dim=1)\n        return sample, noise # noise --> predicted by the model\n    \n    def backward(self,xt,noise_pred,t):\n        l,ct = torch.split(xt,[1,2],dim=1)\n        # Reshaping\n        one_by_sqrt_alpha = self.one_by_sqrt_alpha.to(xt.device)[t]\n        beta = self.beta.to(xt.device)[t]\n        one_by_sqrt_one_minus_alpha_hat = self.one_by_sqrt_one_minus_alpha_hat.to(xt.device)[t]\n        sqrt_beta = self.sqrt_beta.to(xt.device)[t]\n\n        # reshape to match no of dims (b,) -> (b,c,h,w)\n        for _ in range(len(xt.shape) - 1):\n            one_by_sqrt_alpha = one_by_sqrt_alpha.unsqueeze(-1)\n            one_by_sqrt_one_minus_alpha_hat = one_by_sqrt_one_minus_alpha_hat.unsqueeze(-1)\n            beta = beta.unsqueeze(-1)\n        \n        mean = one_by_sqrt_alpha * (ct - beta * one_by_sqrt_one_minus_alpha_hat * noise_pred)\n        std_dev = sqrt_beta\n\n\n        if t==0:\n            out = torch.cat([l, mean],dim=1)\n        else:\n            z = torch.randn_like(ct).to(xt.device)\n            out = torch.cat([l, mean + std_dev * z],dim=1)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:56:21.258379Z","iopub.execute_input":"2025-02-09T08:56:21.258737Z","iopub.status.idle":"2025-02-09T08:56:21.26974Z","shell.execute_reply.started":"2025-02-09T08:56:21.258708Z","shell.execute_reply":"2025-02-09T08:56:21.268858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_time_embedding(time_steps, temb_dim):\n    r\"\"\"\n    Convert time steps tensor into an embedding using the\n    sinusoidal time embedding formula\n    :param time_steps: 1D tensor of length batch size\n    :param temb_dim: Dimension of the embedding\n    :return: BxD embedding representation of B time steps\n    \"\"\"\n    assert temb_dim % 2 == 0, \"time embedding dimension must be divisible by 2\"\n    \n    # factor = 10000^(2i/d_model)\n    factor = 10000 ** ((torch.arange(\n        start=0, end=temb_dim // 2, dtype=torch.float32, device=time_steps.device) / (temb_dim // 2))\n    )\n    \n    # pos / factor\n    # timesteps B -> B, 1 -> B, temb_dim\n    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n    return t_emb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:56:21.270747Z","iopub.execute_input":"2025-02-09T08:56:21.271052Z","iopub.status.idle":"2025-02-09T08:56:21.285378Z","shell.execute_reply.started":"2025-02-09T08:56:21.271025Z","shell.execute_reply":"2025-02-09T08:56:21.284295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DownBlock(nn.Module):\n    r\"\"\"\n    Down conv block with attention.\n    Sequence of following block\n    1. Resnet block with time embedding\n    2. Attention block\n    3. Downsample\n    \"\"\"\n    \n    def __init__(self, in_channels, out_channels, t_emb_dim,\n                 down_sample, num_heads, num_layers, attn, norm_channels, cross_attn=False, context_dim=None):\n        super().__init__()\n        self.num_layers = num_layers\n        self.down_sample = down_sample\n        self.attn = attn\n        self.context_dim = context_dim\n        self.cross_attn = cross_attn\n        self.t_emb_dim = t_emb_dim\n        self.resnet_conv_first = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels,\n                              kernel_size=3, stride=1, padding=1),\n                )\n                for i in range(num_layers)\n            ]\n        )\n        if self.t_emb_dim is not None:\n            self.t_emb_layers = nn.ModuleList([\n                nn.Sequential(\n                    nn.SiLU(),\n                    nn.Linear(self.t_emb_dim, out_channels)\n                )\n                for _ in range(num_layers)\n            ])\n        self.resnet_conv_second = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(out_channels, out_channels,\n                              kernel_size=3, stride=1, padding=1),\n                )\n                for _ in range(num_layers)\n            ]\n        )\n        \n        if self.attn:\n            self.attention_norms = nn.ModuleList(\n                [nn.GroupNorm(norm_channels, out_channels)\n                 for _ in range(num_layers)]\n            )\n            \n            self.attentions = nn.ModuleList(\n                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n                 for _ in range(num_layers)]\n            )\n\n        self.residual_input_conv = nn.ModuleList(\n            [\n                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n                for i in range(num_layers)\n            ]\n        )\n\n        #DownSampling\n        self.down_sample_conv = nn.Conv2d(out_channels, out_channels,\n                                          4, 2, 1) if self.down_sample else nn.Identity()\n    \n    def forward(self, x, t_emb=None, context=None):\n        out = x\n        for i in range(self.num_layers):\n            # Resnet block of Unet\n            resnet_input = out\n            out = self.resnet_conv_first[i](out)\n            if self.t_emb_dim is not None:\n                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n            out = self.resnet_conv_second[i](out)\n            out = out + self.residual_input_conv[i](resnet_input) # residual connection\n            \n            if self.attn:\n                # Attention block of Unet\n                batch_size, channels, h, w = out.shape\n                in_attn = out.reshape(batch_size, channels, h * w)\n                in_attn = self.attention_norms[i](in_attn)\n                in_attn = in_attn.transpose(1, 2)\n                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n                out = out + out_attn\n            \n        # Downsample\n        out = self.down_sample_conv(out)\n        return out\n\n\nclass MidBlock(nn.Module):\n    r\"\"\"\n    Mid conv block with attention.\n    Sequence of following blocks\n    1. Resnet block with time embedding\n    2. Attention block\n    3. Resnet block with time embedding\n    \"\"\"\n    \n    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads, num_layers, norm_channels, cross_attn=None, context_dim=None):\n        super().__init__()\n        self.num_layers = num_layers\n        self.t_emb_dim = t_emb_dim\n        self.context_dim = context_dim\n        self.cross_attn = cross_attn\n        self.resnet_conv_first = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n                              padding=1),\n                )\n                for i in range(num_layers + 1)\n            ]\n        )\n        \n        if self.t_emb_dim is not None:\n            self.t_emb_layers = nn.ModuleList([\n                nn.Sequential(\n                    nn.SiLU(),\n                    nn.Linear(t_emb_dim, out_channels)\n                )\n                for _ in range(num_layers + 1)\n            ])\n        self.resnet_conv_second = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n                )\n                for _ in range(num_layers + 1)\n            ]\n        )\n        \n        self.attention_norms = nn.ModuleList(\n            [nn.GroupNorm(norm_channels, out_channels)\n             for _ in range(num_layers)]\n        )\n        \n        self.attentions = nn.ModuleList(\n            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n             for _ in range(num_layers)]\n        )\n        self.residual_input_conv = nn.ModuleList(\n            [\n                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n                for i in range(num_layers + 1)\n            ]\n        )\n    \n    def forward(self, x, t_emb=None, context=None):\n        out = x\n        \n        # First resnet block\n        resnet_input = out\n        out = self.resnet_conv_first[0](out)\n        if self.t_emb_dim is not None:\n            out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n        out = self.resnet_conv_second[0](out)\n        out = out + self.residual_input_conv[0](resnet_input)\n        \n        for i in range(self.num_layers):\n            # Attention Block\n            batch_size, channels, h, w = out.shape\n            in_attn = out.reshape(batch_size, channels, h * w)\n            in_attn = self.attention_norms[i](in_attn)\n            in_attn = in_attn.transpose(1, 2)\n            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n            out = out + out_attn\n                \n            \n            # Resnet Block\n            resnet_input = out\n            out = self.resnet_conv_first[i + 1](out)\n            if self.t_emb_dim is not None:\n                out = out + self.t_emb_layers[i + 1](t_emb)[:, :, None, None]\n            out = self.resnet_conv_second[i + 1](out)\n            out = out + self.residual_input_conv[i + 1](resnet_input)\n        \n        return out\n\n\nclass UpBlock(nn.Module):\n    r\"\"\"\n    Up conv block with attention.\n    Sequence of following blocks\n    1. Upsample\n    1. Concatenate Down block output\n    2. Resnet block with time embedding\n    3. Attention Block\n    \"\"\"\n    \n    def __init__(self, task, in_channels, out_channels, t_emb_dim,\n                 up_sample, num_heads, num_layers, attn, norm_channels):\n        super().__init__()\n        self.task = task\n        self.num_layers = num_layers\n        self.up_sample = up_sample\n        self.t_emb_dim = t_emb_dim\n        self.attn = attn\n        self.resnet_conv_first = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n                              padding=1),\n                )\n                for i in range(num_layers)\n            ]\n        )\n        \n        if self.t_emb_dim is not None:\n            self.t_emb_layers = nn.ModuleList([\n                nn.Sequential(\n                    nn.SiLU(),\n                    nn.Linear(t_emb_dim, out_channels)\n                )\n                for _ in range(num_layers)\n            ])\n        \n        self.resnet_conv_second = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n                )\n                for _ in range(num_layers)\n            ]\n        )\n        if self.attn:\n            self.attention_norms = nn.ModuleList(\n                [\n                    nn.GroupNorm(norm_channels, out_channels)\n                    for _ in range(num_layers)\n                ]\n            )\n            \n            self.attentions = nn.ModuleList(\n                [\n                    nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n                    for _ in range(num_layers)\n                ]\n            )\n            \n        self.residual_input_conv = nn.ModuleList(\n            [\n                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n                for i in range(num_layers)\n            ]\n        )\n        if self.task == \"unet\":\n            self.up_sample_conv = nn.ConvTranspose2d(in_channels//2, in_channels//2, 4, 2, 1) if self.up_sample else nn.Identity()\n        else: \n            self.up_sample_conv = nn.ConvTranspose2d(in_channels, in_channels, 4, 2, 1) if self.up_sample else nn.Identity()\n    \n    def forward(self, x, out_down=None, t_emb=None):\n        # Upsample\n        x = self.up_sample_conv(x)\n        \n        # Concat with Downblock output\n        # used in diffusion but not in AE(since the output of encoder should be included in input of decoder)..maintain independance    \n        if out_down is not None:\n            x = torch.cat([x, out_down], dim=1)\n        \n        out = x\n        for i in range(self.num_layers):\n            # Resnet Block\n            resnet_input = out\n            out = self.resnet_conv_first[i](out)\n            if self.t_emb_dim is not None:\n                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n            out = self.resnet_conv_second[i](out)\n            out = out + self.residual_input_conv[i](resnet_input)\n            \n            # Self Attention\n            if self.attn:\n                batch_size, channels, h, w = out.shape\n                in_attn = out.reshape(batch_size, channels, h * w)\n                in_attn = self.attention_norms[i](in_attn)\n                in_attn = in_attn.transpose(1, 2)\n                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n                out = out + out_attn\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:56:21.286398Z","iopub.execute_input":"2025-02-09T08:56:21.286671Z","iopub.status.idle":"2025-02-09T08:56:21.324655Z","shell.execute_reply.started":"2025-02-09T08:56:21.286646Z","shell.execute_reply":"2025-02-09T08:56:21.323749Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Unet(nn.Module):\n    r\"\"\"\n    Unet model comprising\n    Down blocks, Midblocks and Uplocks\n    \"\"\"\n    # im_channels will be the no of input channels (latent channels)\n    def __init__(self, in_channels, out_channels, model_config, condition=False):\n        super().__init__()\n        self.condition = condition\n        self.down_channels = model_config['DOWN_CHANNELS'] # [256, 384, 512, 768]\n        self.mid_channels = model_config['MID_CHANNELS'] # [768, 512]\n        self.t_emb_dim = model_config['TIME_EMB_DIM'] # 512\n        self.down_sample = model_config['DOWN_SAMPLE'] # [True, True, True]\n        self.num_down_layers = model_config['NUM_DOWN_LAYERS'] # 2\n        self.num_mid_layers = model_config['NUM_MID_LAYERS'] # 2\n        self.num_up_layers = model_config['NUM_UP_LAYERS'] # 2\n        self.attns = model_config['ATTN'] # [True, True, True]\n        self.norm_channels = model_config['NORM_CHANNELS'] # 32\n        self.num_heads = model_config['NUM_HEADS'] # 16\n        self.conv_out_channels = model_config['CONV_OUT_CHANNELS'] # 128\n        \n        assert self.mid_channels[0] == self.down_channels[-1]\n        assert self.mid_channels[-1] == self.down_channels[-2]\n        assert len(self.down_sample) == len(self.down_channels) - 1\n        assert len(self.attns) == len(self.down_channels) - 1\n\n\n        # Spatial Conditioning\n        if self.condition:\n            self.cond_channels = model_config['CONDITION']['COND_CHANNELS']\n            self.conv_in_concat = nn.Conv2d(in_channels + self.cond_channels,\n                                            self.down_channels[0], kernel_size=3, padding=1)\n        else:\n            self.conv_in = nn.Conv2d(in_channels, self.down_channels[0], kernel_size=3, padding=1)\n        \n        # Initial projection from sinusoidal time embedding\n        self.t_proj = nn.Sequential(\n            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n            nn.SiLU(),\n            nn.Linear(self.t_emb_dim, self.t_emb_dim)\n        )\n        \n        # only change is to add time embeddings\n        self.downs = nn.ModuleList([])\n        for i in range(len(self.down_channels) - 1):\n            self.downs.append(DownBlock(self.down_channels[i], self.down_channels[i + 1], \n                                        t_emb_dim=self.t_emb_dim,down_sample=self.down_sample[i],\n                                        num_heads=self.num_heads,\n                                        num_layers=self.num_down_layers,\n                                        attn=self.attns[i],\n                                        norm_channels=self.norm_channels))\n\n        self.mids = nn.ModuleList([])\n        for i in range(len(self.mid_channels) - 1):\n            self.mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i + 1], self.t_emb_dim,\n                                      num_heads=self.num_heads,\n                                      num_layers=self.num_mid_layers,\n                                      norm_channels=self.norm_channels))\n        \n        self.ups = nn.ModuleList([])\n        for i in reversed(range(len(self.down_channels) - 1)):\n            self.ups.append(UpBlock(\"unet\",self.down_channels[i] * 2, self.down_channels[i - 1] if i != 0 else self.conv_out_channels,\n                                    self.t_emb_dim, up_sample=self.down_sample[i],\n                                        num_heads=self.num_heads,\n                                        num_layers=self.num_up_layers,\n                                        attn=self.attns[i],\n                                        norm_channels=self.norm_channels))\n        \n        self.norm_out = nn.GroupNorm(self.norm_channels, self.conv_out_channels)\n        self.conv_out = nn.Conv2d(self.conv_out_channels, out_channels, kernel_size=3, padding=1)\n    \n    def forward(self, x, t, cond_in = None):\n        # Shapes assuming downblocks are [C1, C2, C3, C4]\n        # Shapes assuming midblocks are [C4, C4, C3]\n        # Shapes assuming downsamples are [True, True, False]\n        # B x C x H x W\n        if cond_in is not None:\n            # cond_in = self.cond_conv_in(cond_in)\n            cond_in = nn.functional.interpolate(size = x.shape[-2:])\n            x = torch.concat([x, cond_in],dim=1)\n            out = self.conv_in_concat(x)\n\n        else:\n            out = self.conv_in(x)\n\n        # B x C1 x H x W\n        \n        # t_emb -> B x t_emb_dim\n        t_emb = get_time_embedding(torch.as_tensor(t).long(), self.t_emb_dim)\n        t_emb = self.t_proj(t_emb)\n        \n        down_outs = []\n        for down in self.downs:\n            down_outs.append(out)\n            out = down(out, t_emb)\n        # down_outs  [B x C1 x H x W, B x C2 x H/2 x W/2, B x C3 x H/4 x W/4]\n        # out B x C4 x H/4 x W/4\n        \n        for mid in self.mids:\n            out = mid(out, t_emb)\n        # out B x C3 x H/4 x W/4\n        \n        for up in self.ups:\n            down_out = down_outs.pop()\n            out = up(out, down_out, t_emb)\n            # out [B x C2 x H/4 x W/4, B x C1 x H/2 x W/2, B x 16 x H x W]\n        out = self.norm_out(out)\n        out = nn.SiLU()(out)\n        out = self.conv_out(out)\n        # out B x C x H x W\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:56:21.325561Z","iopub.execute_input":"2025-02-09T08:56:21.325843Z","iopub.status.idle":"2025-02-09T08:56:21.339118Z","shell.execute_reply.started":"2025-02-09T08:56:21.325813Z","shell.execute_reply":"2025-02-09T08:56:21.338476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def lab_to_rgb(lab_image):\n    # OpenCV expects the range [0, 255] for color images\n    lab = (lab_image * 255).astype(np.uint8)  # Scale to [0, 255]\n    rgb = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)  # Convert LAB to BGR\n    return rgb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:56:21.339856Z","iopub.execute_input":"2025-02-09T08:56:21.340117Z","iopub.status.idle":"2025-02-09T08:56:21.353997Z","shell.execute_reply.started":"2025-02-09T08:56:21.340097Z","shell.execute_reply":"2025-02-09T08:56:21.353164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def sample(L, gt, model, scheduler, sample_no):\n    num_samples = L.shape[0]\n    out_shape = (num_samples,2,IMG_SIZE,IMG_SIZE)\n    xt = torch.cat([L, torch.randn(out_shape).to(DEVICE)],dim=1).to(DEVICE)\n\n    for t in reversed(range(NUM_TIMESTEPS)):\n        # Get prediction of noise\n        timestep = torch.ones(num_samples, dtype=torch.long, device=DEVICE) * t\n\n        noise_pred = model(xt, timestep)\n        \n        # Use scheduler to get x0 and xt-1\n        xt = scheduler.backward(xt, noise_pred, t)\n\n        # if t == 0:\n        #     # Decode ONLY the final iamge to save time\n        #     ims = vae.decode(xt)\n        # else:\n        #     ims = xt\n        ims = xt\n\n    # Add Ground Truths\n    save_output = ims.cpu()\n    save_output = save_output.permute(0,2,3,1).numpy() #open cv compatible\n    save_output = np.array([lab_to_rgb(img) for img in save_output]) #convert lab->rgb\n    save_output = torch.tensor(save_output).permute(0, 3, 1, 2).float() / 255.0 #convert to torch tensor image\n\n    save_input = gt.cpu()\n    save_input = save_input.permute(0,2,3,1).numpy()\n    save_input = np.array([lab_to_rgb(img) for img in save_input])\n    save_input = torch.tensor(save_input).permute(0, 3, 1, 2).float() / 255.0\n    \n    grid = make_grid(torch.cat([save_input, save_output], dim=0), nrow=num_samples)\n    img = torchvision.transforms.ToPILImage()(grid)\n\n    img.save(f\"{IMG_SAVE_ROOT}/{str(sample_no).zfill(10)}.jpg\")\n    plt.imshow(img)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:56:21.354823Z","iopub.execute_input":"2025-02-09T08:56:21.355086Z","iopub.status.idle":"2025-02-09T08:56:21.365761Z","shell.execute_reply.started":"2025-02-09T08:56:21.355057Z","shell.execute_reply":"2025-02-09T08:56:21.365119Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scheduler = LinearNoiseSchedule(T=NUM_TIMESTEPS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:56:21.366622Z","iopub.execute_input":"2025-02-09T08:56:21.366909Z","iopub.status.idle":"2025-02-09T08:56:21.433126Z","shell.execute_reply.started":"2025-02-09T08:56:21.366881Z","shell.execute_reply":"2025-02-09T08:56:21.432506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transform = [\n    transforms.ToTensor(),\n    transforms.Resize((IMG_SIZE, IMG_SIZE), Image.BICUBIC),\n    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n]\n\n\ndata_loader = DataLoader(\n    Dataset(ROOT,transform),\n    batch_size= BATCH_SIZE,\n    shuffle = True,\n    num_workers = 2\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:56:21.433886Z","iopub.execute_input":"2025-02-09T08:56:21.43411Z","iopub.status.idle":"2025-02-09T08:56:21.871261Z","shell.execute_reply.started":"2025-02-09T08:56:21.434091Z","shell.execute_reply":"2025-02-09T08:56:21.870358Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Unet(in_channels = 3, out_channels = 2, model_config = ddpm_model_config).to(DEVICE)\n\nif RESUME: \n    print(\"loading state dict\")\n    model.load_state_dict(torch.load(CKPT_PATH))\nmodel = torch.nn.DataParallel(model, device_ids = [0,1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:56:21.874042Z","iopub.execute_input":"2025-02-09T08:56:21.874301Z","iopub.status.idle":"2025-02-09T08:56:23.105881Z","shell.execute_reply.started":"2025-02-09T08:56:21.874277Z","shell.execute_reply":"2025-02-09T08:56:23.105195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"param_size = 0\nfor param in model.parameters():\n    param_size += param.nelement() * param.element_size()\nbuffer_size = 0\nfor buffer in model.buffers():\n    buffer_size += buffer.nelement() * buffer.element_size()\n\nsize_all_mb = (param_size + buffer_size) / 2**20\nprint('model size: {:.3f}MB'.format(size_all_mb))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:56:23.10676Z","iopub.execute_input":"2025-02-09T08:56:23.106968Z","iopub.status.idle":"2025-02-09T08:56:23.11394Z","shell.execute_reply.started":"2025-02-09T08:56:23.10695Z","shell.execute_reply":"2025-02-09T08:56:23.113027Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = Adam(model.parameters(),lr=1E-5)\ncriterion = torch.nn.MSELoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:56:23.114905Z","iopub.execute_input":"2025-02-09T08:56:23.115182Z","iopub.status.idle":"2025-02-09T08:56:23.127401Z","shell.execute_reply.started":"2025-02-09T08:56:23.115148Z","shell.execute_reply":"2025-02-09T08:56:23.126557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(\n        num_epochs,\n        data_loader,\n        optimizer,\n        T,\n        scheduler,\n        model,\n        criterion,\n        sample_step,\n):\n    step_count = 0\n    sample_no = 0\n    for epoch in range(num_epochs):\n        losses = []\n        for im in tqdm(data_loader):\n            step_count+=1\n            optimizer.zero_grad()\n            im = im.float().to(DEVICE)\n            L, _ = torch.split(im,[1,2],dim=1)\n            L = L.float().to(DEVICE)\n\n            # # moving from pixel space to latent space\n            # with torch.no_grad():\n            #     latent_im, _, _, _ = vae.encode(im)\n\n            t = torch.randint(0,T,(im.shape[0],)).to(DEVICE)\n\n            noisy_im, noise = scheduler.forward(im, t)\n            noise_pred = model(noisy_im, t)\n\n            loss = criterion(noise_pred, noise)\n            losses.append(loss.item())\n            loss.backward()\n            optimizer.step()\n            if step_count % sample_step == 0 or step_count==1:\n                with torch.no_grad():\n                    sample(L, im, model, scheduler, sample_no)\n                sample_no+=1\n        print(f\"{epoch} Loss {np.mean(losses)}\")\n        torch.save(model.module.state_dict(), f\"{CKPT_SAVE}/ddpm.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:56:23.128334Z","iopub.execute_input":"2025-02-09T08:56:23.12864Z","iopub.status.idle":"2025-02-09T08:56:23.138062Z","shell.execute_reply.started":"2025-02-09T08:56:23.128609Z","shell.execute_reply":"2025-02-09T08:56:23.13727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train(\n    num_epochs = 1,\n    data_loader = data_loader,\n    optimizer = optimizer,\n    T = NUM_TIMESTEPS,\n    scheduler = scheduler,\n    model = model,\n    criterion = criterion,\n    sample_step = SAMPLE_STEP\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T08:56:23.138777Z","iopub.execute_input":"2025-02-09T08:56:23.139019Z"}},"outputs":[],"execution_count":null}]}