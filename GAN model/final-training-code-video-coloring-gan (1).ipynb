{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMxghNI2VhDEcGW6CEWyMDr"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":10248389,"sourceType":"datasetVersion","datasetId":6338623},{"sourceId":11007584,"sourceType":"datasetVersion","datasetId":6852844},{"sourceId":307643,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":248441,"modelId":269957}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !unzip /content/random-frames-ucf-101.zip","metadata":{"id":"vxrqr4cX6ePN","executionInfo":{"status":"ok","timestamp":1737277697926,"user_tz":-330,"elapsed":54221,"user":{"displayName":"Singaraju B V Sreedakshinya .","userId":"04308658734104705074"}},"outputId":"92603d32-b3e3-4303-f31a-5f6f6527467b","_kg_hide-input":true,"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-04-01T13:27:49.327705Z","iopub.execute_input":"2025-04-01T13:27:49.327976Z","iopub.status.idle":"2025-04-01T13:27:49.333282Z","shell.execute_reply.started":"2025-04-01T13:27:49.327950Z","shell.execute_reply":"2025-04-01T13:27:49.332602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch.nn as nn\n\n# class VGGReconstructor(nn.Module):\n\n#   def __init__(self):\n#     super(VGGReconstructor, self).__init__()\n\n#     self.block1 = nn.Sequential(\n#         nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),\n#         nn.ReLU(),\n#         nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=2, padding=1),\n#         nn.BatchNorm2d(64),\n#         nn.ReLU()\n#     )\n\n#     self.block2 = nn.Sequential(\n#         nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n#         nn.BatchNorm2d(128),\n#         nn.ReLU(),\n#         nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1),\n#         nn.BatchNorm2d(128),\n#         nn.ReLU()        \n#     )\n\n#     self.block3 = nn.Sequential(\n#         nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n#         nn.BatchNorm2d(256),\n#         nn.ReLU(),\n#         nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n#         nn.BatchNorm2d(256),\n#         nn.ReLU(),\n#         nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=2, padding=1),\n#         nn.BatchNorm2d(256),\n#         nn.ReLU()\n#     )\n\n#     self.block4 = nn.Sequential(\n#         nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n#         nn.BatchNorm2d(512),\n#         nn.ReLU(),\n#         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n#         nn.BatchNorm2d(512),\n#         nn.ReLU(),\n#         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=2, padding=1),\n#         nn.BatchNorm2d(512),\n#         nn.ReLU()        \n#     )\n\n#     self.block5 = nn.Sequential(\n#         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n#         nn.BatchNorm2d(512),\n#         nn.ReLU(),\n#         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n#         nn.BatchNorm2d(512),\n#         nn.ReLU(),\n#         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n#         nn.BatchNorm2d(512),\n#         nn.ReLU(),\n#         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=2, padding=1),\n#         nn.BatchNorm2d(512),\n#         nn.ReLU()\n#     )\n\n#   def forward(self, x_in):\n#         out1 = self.block1(x_in)\n#         out2 = self.block2(out1)\n#         out3 = self.block3(out2)\n#         out4 = self.block4(out3)\n#         out5 = self.block5(out4)\n\n#         return out5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T13:27:49.333817Z","iopub.execute_input":"2025-04-01T13:27:49.334043Z","iopub.status.idle":"2025-04-01T13:27:49.350544Z","shell.execute_reply.started":"2025-04-01T13:27:49.334019Z","shell.execute_reply":"2025-04-01T13:27:49.349644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np # type: ignore\nimport os\nimport pandas as pd # type: ignore\nfrom tqdm import tqdm # type: ignore\nimport natsort\nimport glob\nimport torch\nfrom torch.nn import DataParallel as DDP\nimport torch.nn as nn # type: ignore\nimport torch.nn.functional as F # type: ignore\nimport torch.optim as optim # type: ignore\nfrom torch.utils.data import Dataset, DataLoader # type: ignore\nimport matplotlib.pyplot as plt # type: ignore\nimport torchvision\nimport torchvision.models as models # type: ignore\nfrom torchvision.models import VGG16_Weights# type: ignore\nfrom torch.amp import autocast, GradScaler\n\nimport torchvision.transforms as transforms # type: ignore\nfrom PIL import Image # type: ignore\n\n\nclass VGG16FeatureExtractor:\n    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n        self.model = DDP(VGGReconstructor()).to(device)\n        self.model.load_state_dict(torch.load('/kaggle/input/pre-trained-vgg-for-colouring-gan/pytorch/default/1/best_model.pth', weights_only=True), strict=False)\n        self.model.eval()\n        self.device = device\n\n    def extract_features(self, image):\n        image = image.to(self.device)\n        with torch.no_grad():\n            features = self.model(image)\n        return features\n\nclass CustomDataSet(Dataset):\n    def __init__(self, root, transform):\n        self.main_dir = root\n        self.transform = transform\n        all_imgs = os.listdir(root)\n        self.total_imgs = natsort.natsorted(all_imgs)\n\n    def __len__(self):\n        return len(self.total_imgs)\n\n    def __getitem__(self, idx):\n        img_loc = os.path.join(self.main_dir, self.total_imgs[idx])\n        image = Image.open(img_loc).convert(\"RGB\")\n        tensor_image = self.transform(image)\n        return tensor_image, self.total_imgs[idx]\n        \ndef read_keyframe_file(csv_path):\n    \"\"\"Read the keyframe file with timestamp mappings.\"\"\"\n    try:\n        with open(csv_path, 'r') as f:\n            lines = f.readlines()\n\n        data = []\n        for line in lines:\n            parts = line.strip().split()\n            keyframe_num = int(parts[1])\n            timestamp = float(parts[4])\n            data.append({'keyframe': keyframe_num, 'timestamp': timestamp})\n\n        return pd.DataFrame(data)\n    except Exception as e:\n        print(f\"Error reading file {csv_path}: {str(e)}\")\n        return None\n\n# def process_data(base_dir):\n#     all_features={}\n#     feature_extractor = VGG16FeatureExtractor()\n#     for img in tqdm(os.listdir(base_dir)):\n#       try:\n#         frame_path = os.path.join(base_dir, img)\n#         features  = feature_extractor.extract_features(frame_path)\n#         all_features[img] = {\n#             'features': features\n#         }\n#       except Exception as e:\n#         print(f\"Error processing {img}: {str(e)}\")\n#         continue\n#     return all_features\n\n# Model architecture\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\nclass ColorizationUNetWithAttentionDecoder(nn.Module):\n    def __init__(self, loaded_dataset, input_channels=1):\n        super().__init__()\n\n        # Encoder with 1x1 convolutions\n        self.enc1 = nn.Sequential(\n            nn.Conv2d(input_channels, 512,  kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True)\n        )\n\n        self.enc2 = nn.Sequential(\n            nn.Conv2d(512, 256,  kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True)\n        )\n\n        self.enc3 = nn.Sequential(\n            nn.Conv2d(256, 128,  kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True)\n        )\n\n        # Middle layer\n        self.middle = nn.Sequential(\n            nn.Conv2d(1024, 64,  kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n\n        # Decoder\n        self.dec3 = nn.Sequential(\n            nn.Conv2d(1088, 128,  kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True)\n        )\n\n        self.dec2 = nn.Sequential(\n            nn.Conv2d(256 + 128, 256,  kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True)\n        )\n\n        self.dec1 = nn.Sequential(\n            nn.Conv2d(512 + 256, 512,  kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(inplace=True)\n        )\n\n        #Attention Decoder layers\n        self.layer1a = nn.Sequential(\n            # nn.ReflectionPad2d(1),\n            nn.Conv2d(in_channels = 512, out_channels = 128,  kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU()\n        )\n\n        self.layer1b = nn.Sequential(\n            # nn.ReflectionPad2d(1),\n            nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU()\n        )\n\n        self.layer2 = nn.Sequential(\n            # nn.ReflectionPad2d(1),\n            nn.Conv2d(in_channels = 256, out_channels = 128,  kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU()\n        )\n\n        self.layer3 = nn.Sequential(\n            # nn.ReflectionPad2d(1),\n            nn.Conv2d(in_channels = 128, out_channels = 128,  kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU()\n        )\n\n        self.layer4 = nn.Sequential(\n            # nn.ReflectionPad2d(1),\n            nn.Conv2d(in_channels = 384, out_channels = 128,  kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU()\n        )\n\n        self.final_layer = nn.Sequential(\n            # nn.ReflectionPad2d(1),\n            nn.Conv2d(in_channels = 128, out_channels = 1,  kernel_size=3, stride=1, padding=1)\n        )\n\n        # Output layers\n        self.final_color = nn.Conv2d(512, 3,  kernel_size=3, stride=1, padding=1)\n        self.saliency_dec3 = nn.Conv2d(128, 1,  kernel_size=3, stride=1, padding=1)\n        self.saliency_dec2 = nn.Conv2d(256, 1,  kernel_size=3, stride=1, padding=1)\n        self.saliency_dec1 = nn.Conv2d(512, 1,  kernel_size=3, stride=1, padding=1)\n        \n        self.loaded_dataset = loaded_dataset\n        \n    def forward(self, x, img_name):\n        # Encoder path\n        enc1 = self.enc1(x)\n        enc2 = self.enc2(enc1)\n        enc3 = self.enc3(enc2)\n        # print(\"zzzzzzzzzz\")\n        # print(img_name)\n        features_list = []\n        for name in img_name:\n            # Get the actual features for this name\n            feature = self.loaded_dataset.data[name]\n            # Convert to tensor and ensure correct shape\n            feature_tensor = torch.from_numpy(feature).float()\n            if len(feature_tensor.shape) == 2:\n                feature_tensor = feature_tensor.reshape(1, *feature_tensor.shape)\n            features_list.append(feature_tensor)\n        \n        # Stack the features\n        from_vgg = torch.stack(features_list)\n        \n        # Ensure batch sizes match\n        batch_size = enc3.size(0)\n        if from_vgg.size(0) != batch_size:\n            if from_vgg.size(0) < batch_size:\n                # Repeat last feature if needed\n                repeats = from_vgg[-1:].repeat(batch_size - from_vgg.size(0), 1, 1, 1)\n                from_vgg = torch.cat([from_vgg, repeats], dim=0)\n            else:\n                # Take only what's needed\n                from_vgg = from_vgg[:batch_size]\n        from_vgg = torch.nn.functional.interpolate(from_vgg, size=enc3.shape[2:], mode=\"bilinear\", align_corners=False)\n        # from_vgg = self.helper_model.extract_features(x)\n        # enc3 = nn.functional.interpolate(enc3, size=from_vgg[1], mode=\"bilinear\", align_corners=False)\n        # print(enc3.shape)\n        # print(\"QQQQQQQQq\")\n        # print(from_vgg.shape)\n        from_vgg = from_vgg.to(enc3.device)\n        # print(\"bbbbbb\")\n        enc3_new = torch.cat([enc3, from_vgg], dim=1)\n        # print(f\"enc3 shape: {enc3.shape}\")\n        # print(f\"from_vgg shape: {from_vgg.shape}\")\n        # print(f\"enc3_new shape after cat: {enc3_new.shape}\")\n        # Middle\n        middle = self.middle(enc3_new)\n        # print(\"AAAAAAAA\")\n\n        # Decoder path with skip connections\n        dec3 = self.dec3(torch.cat([middle, enc3_new], dim=1))\n        # print(\"BBBBBBBB\")\n        dec2 = self.dec2(torch.cat([dec3, enc2], dim=1))\n        # print(\"CCCCCCCC\")\n        dec1 = self.dec1(torch.cat([dec2, enc1], dim=1))\n        # print(dec1.shape, dec2.shape, dec3.shape)\n\n        # Output of Colorization Network\n        color_output = self.final_color(dec1)\n                # saliency3 = self.saliency_dec3(dec3) saliency2 = self.saliency_dec2(dec2) saliency1 = self.saliency_dec1(dec1)\n        # print(\"DDDDDDDDDD\")\n        #Attention Decoder\n        # att1 = nn.functional.interpolate(dec1, scale_factor=2, mode='nearest')\n        att1=dec1\n        # print(\"Before\" + str(att1.shape))\n        att1 = self.layer1a(att1)\n        # print(\"EEEEEE\")\n        # att1 = nn.functional.interpolate(att1, scale_factor=1, mode='nearest')\n        x1 = self.layer1b(att1)\n        # print(\"After\" + str(x1.shape))\n        # x1 = nn.functional.interpolate(x1, scale_factor=9/11, mode='nearest')\n\n        # att2 = nn.functional.interpolate(dec2, scale_factor=2, mode='nearest')\n        att2=dec2\n        x2 = self.layer2(att2)\n        # print(\"FFFFFFf\")\n        x3 = self.layer3(dec3)\n\n        ############################\n        #        DEBUG\n        # print(x1.shape, x2.shape, x3.shape)\n        ##################################\n        x2 = x2.to(x1.device)\n        x3 = x3.to(x1.device)\n        x = torch.cat((x1, x2, x3), 1)\n        # x = nn.functional.interpolate(x, scale_factor=2)\n        x = self.layer4(x)\n\n        saliency_out = self.final_layer(x)\n        # print(\"saliency_out\" + str(saliency_out.shape))\n        return color_output, saliency_out\n\n# Modify the Dataset class to ensure correct tensor dimensions\n# class VideoFeaturesDataset(Dataset):\n#     def __init__(self, npz_path):\n#         self.npz_path = npz_path\n#         self.data = np.load(self.npz_path, mmap_mode='r')  # Memory-map for efficient access\n#         self.keys = list(self.data.files)  # Store keys for indexing\n\n#     def __len__(self):\n#         return len(self.keys)\n\n#     # def __getitem__(self, idx):\n#     #     key = self.keys[idx]\n#     #     features = self.data[key]\n\n#     #     # Ensure features have the correct shape (C, H, W)\n#     #     if len(features.shape) == 2:\n#     #         features = features.reshape(1, *features.shape)\n#     #     elif len(features.shape) == 3:\n#     #         features = features.reshape(-1, features.shape[-2], 1, 1)\n\n#     def __getitem__(self, idx):\n#         try:\n#             key = self.keys[idx]\n#             features = self.data[key]\n#             # print(features.shape)\n#             # Ensure features have the correct shape (C, H, W)\n#             if len(features.shape) == 2:\n#                 features = features.reshape(1, *features.shape)\n#             # elif len(features.shape) == 3:\n#             #     features = features.reshape(7, -1, features.shape[-1])\n\n#             # self.features.extend([torch.from_numpy(feat) for feat in features])\n#             # print(features.shape)\n#             return features\n\n#         except Exception as e:\n#             print(f\"Error in __getitem__ for idx {idx}: {e}\")\n#             return None  # Return None explicitly to trigger error\n\n# Training utilities\nclass AverageMeter:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef save_checkpoint(state, filename):\n    torch.save(state, filename)\n    print(f\"Checkpoint saved: {filename}\")\n\ndef load_checkpoint(model, optimizer, filename):\n    if os.path.isfile(filename):\n        checkpoint = torch.load(filename)\n        model.module.load_state_dict(checkpoint['state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_G'])\n        return checkpoint['epoch']\n    return 0\n\n# def train_epoch(model, train_loader, criterion, optimizer, device):\n#     model.train()\n#     losses = AverageMeter()\n\n#     with tqdm(train_loader, desc=\"Training\") as pbar:\n#         for features in pbar:\n#             features = features.to(device)\n\n#             optimizer.zero_grad()\n#             color_output, saliency_map = model(features)\n\n#             color_target = features[:, :2, :, :]\n#             color_loss = criterion(color_output, color_target)\n\n#             # Calculate saliency losses\n#             saliency_target = torch.mean(features, dim=1, keepdim=True)  # Create target from input features\n#             saliency_loss = criterion(saliency_map, saliency_target)\n\n#             # Total loss\n#             loss = color_loss + saliency_loss\n\n#             loss.backward()\n#             optimizer.step()\n\n#             losses.update(loss.item(), features.size(0))\n#             pbar.set_postfix({'Loss': f'{losses.avg:.4f}'})\n\n#     return losses.avg\n\n# def validate(model, val_loader, criterion, device):\n#     model.eval()\n#     losses = AverageMeter()\n\n#     with torch.no_grad():\n#         for features in tqdm(val_loader, desc=\"Validating\"):\n#             features = features.to(device)\n\n#             color_output, saliency_map = model(features)\n\n#             # Calculate color loss\n#             color_target = features[:, :2, :, :]\n#             color_loss = criterion(color_output, color_target)\n\n#             # Calculate saliency losses\n#             saliency_target = torch.mean(features, dim=1, keepdim=True)\n#             saliency_loss = criterion(saliency_map, saliency_target)\n\n#             # Total loss\n#             loss = color_loss + saliency_loss\n\n#             losses.update(loss.item(), features.size(0))\n\n#     return losses.avg\n","metadata":{"id":"ki-uW-jv194T","executionInfo":{"status":"ok","timestamp":1737277710528,"user_tz":-330,"elapsed":12637,"user":{"displayName":"Singaraju B V Sreedakshinya .","userId":"04308658734104705074"}},"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T13:27:49.510887Z","iopub.execute_input":"2025-04-01T13:27:49.511233Z","iopub.status.idle":"2025-04-01T13:27:56.106013Z","shell.execute_reply.started":"2025-04-01T13:27:49.511206Z","shell.execute_reply":"2025-04-01T13:27:56.105066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, input_channels):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n\n        # Use global pooling to handle 1x1 inputs\n        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n\n        # Final layer\n        self.fc = nn.Conv2d(512, 1, kernel_size=1)\n\n        self.leaky_relu = nn.LeakyReLU(0.2, inplace=True)\n        self.batch_norm2 = nn.BatchNorm2d(128)\n        self.batch_norm3 = nn.BatchNorm2d(256)\n        self.batch_norm4 = nn.BatchNorm2d(512)\n\n        self.dropout = nn.Dropout(p=0.3)\n\n    def forward(self, x):\n        x = self.dropout(self.leaky_relu(self.conv1(x)))\n        x = self.dropout(self.leaky_relu(self.batch_norm2(self.conv2(x))))\n        x = self.dropout(self.leaky_relu(self.batch_norm3(self.conv3(x))))\n        x = self.dropout(self.leaky_relu(self.batch_norm4(self.conv4(x))))\n\n        # Global pooling to handle small spatial dimensions\n        x = self.global_pool(x)\n\n        # Final prediction\n        x = self.fc(x)\n        # x = torch.sigmoid(x)\n        return x\n\nclass GANLoss_autocast_compatible:\n    def __init__(self, device):\n        self.register_buffer = lambda name, tensor: setattr(self, name, tensor)\n        self.register_buffer('real_label', torch.tensor(1.0).to(device))\n        self.register_buffer('fake_label', torch.tensor(0.0).to(device))\n        self.loss = nn.BCEWithLogitsLoss()\n        self.device = device\n\n    # def get_target_tensor(self, prediction, target_is_real):\n    #     if target_is_real:\n    #         target_tensor = self.real_label\n    #     else:\n    #         target_tensor = self.fake_label\n    #     return target_tensor.expand_as(prediction)\n\n    def get_target_tensor(self, prediction, target_value):\n        target_tensor = torch.full_like(prediction, target_value, device=self.device)\n        return target_tensor\n\n    # def __call__(self, prediction, target_is_real):\n    #     target_tensor = self.get_target_tensor(prediction, target_is_real)\n    #     return self.loss(prediction, target_tensor)\n\n    def __call__(self, prediction, target_value):\n        target_tensor = self.get_target_tensor(prediction, target_value)\n        return self.loss(prediction, target_tensor)\n\ndef load_checkpoint(model, optimizer, filename):\n    if os.path.isfile(filename):\n        checkpoint = torch.load(filename)\n        model.module.load_state_dict(checkpoint['state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_G'])\n        return checkpoint['epoch']\n    return 0\n\ndef train_gan_epoch(generator, color_discriminator, attention_discriminator,\n                   train_loader, gan_criterion_d, optimizer_G, optimizer_D_color,\n                   optimizer_D_attention, device, scaler, scaler2, Grayscale_creator, epoch):\n\n    generator.train()\n    color_discriminator.train()\n    attention_discriminator.train()\n\n    losses_G = AverageMeter()\n    losses_D_color = AverageMeter()\n    losses_D_attention = AverageMeter()\n\n    with tqdm(train_loader, desc=\"Training GAN\") as pbar:\n        for features, img_name in pbar:\n            batch_size = features.size(0)\n            features = features.to(device)\n\n            # Ground truth labels\n            # real_color = features[:, :2, :, :]\n            real_color = features[:, :, :]\n            real_saliency = torch.mean(features, dim=1, keepdim=True)\n\n            noise_factor = max(0.01, 0.1 * (0.99 ** epoch))\n            real_color_noisy = real_color + noise_factor * torch.randn_like(real_color)\n            #Train Discriminator\n            optimizer_D_color.zero_grad()\n            optimizer_D_attention.zero_grad()\n\n            # Generate fake outputs\n            # print(\"here\")\n            # print(features.shape)\n            with autocast(device_type=\"cuda\"):\n                gray_input = Grayscale_creator(features)\n                gray_input_noisy = gray_input + noise_factor * torch.randn_like(gray_input)\n                fake_color, fake_saliency = generator(gray_input, img_name)\n\n                # Color discriminator\n                pred_real_color = color_discriminator(real_color)\n                loss_D_real_color = gan_criterion_d(pred_real_color, 0.9)\n    \n                pred_fake_color = color_discriminator(fake_color.detach())\n                loss_D_fake_color = gan_criterion_d(pred_fake_color, False)\n    \n                loss_D_color = (loss_D_real_color + loss_D_fake_color) * 0.5\n                \n            # scaler.scale(loss_D_color).backward()\n\n            # with autocast():\n                \n                # Attention discriminator\n                real_weighted = real_color * real_saliency\n                fake_weighted = fake_color.detach() * fake_saliency.detach()\n    \n                pred_real_attention = attention_discriminator(real_weighted)\n                loss_D_real_attention = gan_criterion_d(pred_real_attention, 0.9)\n    \n                pred_fake_attention = attention_discriminator(fake_weighted)\n                loss_D_fake_attention = gan_criterion_d(pred_fake_attention, False)\n\n                loss_D_attention = (loss_D_real_attention + loss_D_fake_attention) * 0.5\n                \n            scaler.scale(loss_D_attention + loss_D_color).backward()\n\n            scaler.unscale_(optimizer_D_color)\n            torch.nn.utils.clip_grad_norm_(color_discriminator.parameters(), max_norm=1.0)\n            # scaler.step(optimizer_D_color)\n            # scaler.update()\n            scaler.unscale_(optimizer_D_attention)\n            torch.nn.utils.clip_grad_norm_(attention_discriminator.parameters(), max_norm=1.0)\n            scaler.step(optimizer_D_color)\n            scaler.step(optimizer_D_attention)\n            scaler.update()\n\n            #Train generators\n            optimizer_G.zero_grad()\n\n            with autocast(device_type=\"cuda\"):\n                # Color GAN loss\n                pred_fake_color = color_discriminator(fake_color)\n                loss_G_color = gan_criterion_d(pred_fake_color, 0.9)\n    \n                # Attention GAN loss\n                fake_weighted = fake_color * fake_saliency\n                pred_fake_attention = attention_discriminator(fake_weighted)\n                loss_G_attention = gan_criterion_d(pred_fake_attention, 0.9)\n    \n                # L1 losses\n                loss_L1_color = F.l1_loss(fake_color, real_color)\n                loss_L1_saliency = F.l1_loss(fake_saliency, real_saliency)\n    \n                # Combined generator loss\n                loss_G = (loss_G_color + loss_G_attention +\n                         1.0 * loss_L1_color + 0.5 * loss_L1_saliency)\n                \n            scaler2.scale(loss_G).backward()\n\n            scaler2.unscale_(optimizer_G)\n            # torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n            scaler2.step(optimizer_G)\n            scaler2.update()\n\n            # Update metrics\n            losses_G.update(loss_G.item(), batch_size)\n            losses_D_color.update(loss_D_color.item(), batch_size)\n            losses_D_attention.update(loss_D_attention.item(), batch_size)\n\n            pbar.set_postfix({\n                'G_loss': f'{losses_G.avg:.4f}',\n                'D_color_loss': f'{losses_D_color.avg:.4f}',\n                'D_attention_loss': f'{losses_D_attention.avg:.4f}'\n            })\n\n    return losses_G.avg, losses_D_color.avg, losses_D_attention.avg\n\nclass VideoFeaturesDataset(Dataset):\n    def __init__(self, npz_path):\n        self.npz_path = npz_path\n        self.data = np.load(self.npz_path, mmap_mode='r')  # Memory-map for efficient access\n        self.keys = list(self.data.files)  # Store keys for indexing\n\n    def __len__(self):\n        return len(self.keys)\n\n    def __getitem__(self, idx):\n        try:\n            if isinstance(idx, (tuple, list)):  # If batch, fetch each separately\n                features = [self.__getitem__(i) for i in idx]\n                return torch.stack(features)  # Stack into a single tensor\n            # key = self.keys[idx]\n            features = self.data[idx]\n            # print(features.shape)\n            # Ensure features have the correct shape (C, H, W)\n            if len(features.shape) == 2:\n                features = features.reshape(1, *features.shape)\n            # elif len(features.shape) == 3:\n            #     features = features.reshape(7, -1, features.shape[-1])\n\n            # self.features.extend([torch.from_numpy(feat) for feat in features])\n            # print(features.shape)\n            return torch.from_numpy(features).float()  # Convert to tensor\n\n        except Exception as e:\n            print(f\"Error in __getitem__ for idx {idx}: {e}\")\n            return None  # Return None explicitly to trigger error","metadata":{"id":"kFiQ8UMrzF55","executionInfo":{"status":"ok","timestamp":1737277710528,"user_tz":-330,"elapsed":2,"user":{"displayName":"Singaraju B V Sreedakshinya .","userId":"04308658734104705074"}},"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T13:27:56.107307Z","iopub.execute_input":"2025-04-01T13:27:56.107798Z","iopub.status.idle":"2025-04-01T13:27:56.128328Z","shell.execute_reply.started":"2025-04-01T13:27:56.107771Z","shell.execute_reply":"2025-04-01T13:27:56.127396Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    # Setup\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    batch_size = 32\n    num_epochs = 25\n    learning_rate = 0.0002\n    beta1 = 0.5\n    beta2 = 0.999\n\n    '''base_dir= \"/kaggle/input/random-frames-ucf-101/Data\"\n\n    all_features = {}\n\n    feature_extractor = VGG16FeatureExtractor()\n\n    for img in tqdm(os.listdir(base_dir)):\n      try:\n        frame_path = os.path.join(base_dir, img)\n        features  = feature_extractor.extract_features(frame_path)\n        all_features[img] = np.vstack(features)\n      except Exception as e:\n        print(f\"Error processing img: {str(e)}\")\n        continue\n\n    if all_features:\n      output_path = '/kaggle/working/extracted_features.npz'\n      np.savez_compressed(\n          output_path,\n          **{f\"{img}\": all_features[img]\n              for img in all_features.keys()}\n      )\n      print(f\"Features saved to {output_path}\")\n    else:\n        print(\"No features were extracted successfully\")'''\n\n    # Load data\n    # data = np.load('/kaggle/input/featurevgg-npz/extracted_features.npz')\n    # features_dict = {}\n    # timestamps_dict = {}\n\n    # for key in data.files:\n    #     features_dict[key] = data[key]\n\n    # print(f\"Number of videos loaded: {len(features_dict)}\")\n\n    # Create dataset and dataloaders\n    # dataset = VideoFeaturesDataset('/kaggle/input/featurevgg-npz/extracted_features.npz')\n\n    transform_image = transforms.Compose([\n            transforms.Resize(128),\n            transforms.CenterCrop(128),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n        ])\n    \n    dataset = CustomDataSet(\n    root='/kaggle/input/random-frames-ucf-101/Data',\n    transform=transform_image\n    )\n    \n    print(f\"Total number of samples: {len(dataset)}\")\n\n    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [int(0.8*len(dataset)), len(dataset)-int(0.8*len(dataset))])\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n    # print('AAAAAAA')\n\n    # Initialize models\n    sample_batch, img_name = next(iter(train_loader))\n    # print(f\"Here, {img_name}\")\n    # sample_batch = train_dataset\n    input_channels = sample_batch[0].shape[0]\n    print(\"input_channels = \"+str(input_channels))\n\n    loaded_dataset = VideoFeaturesDataset('/kaggle/input/extracted-grayscale-features/grayscale_extracted_features.npz')\n    print(f\"Total number of features loaded: {len(loaded_dataset)}\")\n    \n    generator = DDP(ColorizationUNetWithAttentionDecoder(loaded_dataset, input_channels=1)).to(device)\n    color_discriminator = DDP(Discriminator(input_channels=3)).to(device)\n    attention_discriminator = DDP(Discriminator(input_channels=3)).to(device)\n\n    # Initialize optimizers\n    optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate*2, betas=(beta1, beta2), foreach=True, weight_decay=1e-6)\n    optimizer_D_color = optim.Adam(color_discriminator.parameters(), lr=learning_rate*0.5, betas=(beta1, beta2), foreach=True, weight_decay=1e-4)\n    optimizer_D_attention = optim.Adam(attention_discriminator.parameters(), lr=learning_rate*0.5, betas=(beta1, beta2), foreach=True, weight_decay=1e-4)\n\n    scaler = GradScaler()\n    scaler_g = GradScaler()\n    Grayscale_creator = transforms.Grayscale(num_output_channels=1)\n    \n    # Loss functions\n    gan_criterion_d = GANLoss_autocast_compatible(device)\n\n    # Create output directory\n    output_dir = '/kaggle/working/model_outputs'\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Training loop\n    best_loss = float('inf')\n    start_epoch = 0\n\n    # Load checkpoint if exists\n    # checkpoint_path = os.path.join(output_dir, 'latest_checkpoint.pth')\n    checkpoint_path = ''\n    if os.path.exists(checkpoint_path):\n        start_epoch = load_checkpoint(generator, optimizer_G, checkpoint_path)\n        print(f\"Loaded checkpoint from epoch {start_epoch}\")\n\n    for epoch in range(start_epoch, num_epochs):\n        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n\n        # Train\n        g_loss, d_color_loss, d_attention_loss = train_gan_epoch(\n            generator, color_discriminator, attention_discriminator,\n            train_loader, gan_criterion_d, optimizer_G,\n            optimizer_D_color, optimizer_D_attention, device, scaler, scaler_g, Grayscale_creator, epoch\n        )\n\n        print(f\"Generator Loss: {g_loss:.4f}\")\n        print(f\"Color Discriminator Loss: {d_color_loss:.4f}\")\n        print(f\"Attention Discriminator Loss: {d_attention_loss:.4f}\")\n\n        # Save checkpoint\n        is_best = g_loss < best_loss\n        best_loss = min(g_loss, best_loss)\n\n        save_checkpoint({\n            'epoch': epoch + 1,\n            'state_dict': generator.module.state_dict(),\n            'optimizer_G': optimizer_G.state_dict(),\n            'optimizer_D_color': optimizer_D_color.state_dict(),\n            'optimizer_D_attention': optimizer_D_attention.state_dict(),\n            'best_loss': best_loss,\n        }, os.path.join(output_dir, 'latest_checkpoint.pth'))\n\n        if is_best:\n            save_checkpoint({\n                'epoch': epoch + 1,\n                'state_dict': generator.module.state_dict(),\n                'optimizer_G': optimizer_G.state_dict(),\n                'optimizer_D_color': optimizer_D_color.state_dict(),\n                'optimizer_D_attention': optimizer_D_attention.state_dict(),\n                'best_loss': best_loss,\n            }, os.path.join(output_dir, 'best_model.pth'))\n\n        # Visualize results every 5 epochs\n        if (epoch + 1) % 5 == 0:\n            generator.eval()\n            with torch.no_grad():\n                sample_features, img_name = next(iter(test_loader))\n                sample_features = sample_features.to(device)\n                Grayscale_creator = transforms.Grayscale(num_output_channels=1)\n                gray_input = Grayscale_creator(sample_features)\n                fake_color, fake_saliency = generator(gray_input, img_name)\n\n                \n                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n\n                # Save colorization outputs\n                plt.figure(figsize=(15, 5))\n                for i in range(4):\n                    out_denorm = fake_color[i] * std + mean\n                    color_img = out_denorm.cpu().numpy()\n                    # color_img = (color_img - color_img.min()) / (color_img.max() - color_img.min())\n                    # rgb_img = np.zeros((color_img.shape[1], color_img.shape[2], 3))\n                    color_img = np.transpose(color_img, (1, 2, 0))\n                    color_img = np.clip(color_img, 0, 1)\n\n                    plt.subplot(1, 4, i+1)\n                    plt.imshow(color_img)\n                    plt.axis('off')\n                plt.savefig(os.path.join(output_dir, f'updated_epoch_{epoch+1}_colorization.png'))\n                plt.close()\n\n                # Save saliency maps\n                plt.figure(figsize=(15, 5))\n                for i in range(4):\n                    out = fake_saliency[i].unsqueeze(0)\n                    out_expanded = out.expand(-1, 3, -1, -1)  # Expand to 3 channels\n                    out_denorm = out_expanded * std + mean\n                    saliency = out_denorm.cpu().numpy()\n                    # saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min())\n                    saliency = np.transpose(saliency[0], (1, 2, 0))\n                    saliency = np.clip(saliency, 0, 1)\n\n                    plt.subplot(1, 4, i+1)\n                    plt.imshow(saliency)\n                    plt.axis('off')\n                plt.savefig(os.path.join(output_dir, f'updated_epoch_{epoch+1}_saliency.png'))\n                \n                plt.close()\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"id":"r2LwcYW_ybEX","executionInfo":{"status":"ok","timestamp":1737277747369,"user_tz":-330,"elapsed":337,"user":{"displayName":"Singaraju B V Sreedakshinya .","userId":"04308658734104705074"}},"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T13:27:56.130125Z","iopub.execute_input":"2025-04-01T13:27:56.130358Z","execution_failed":"2025-04-01T14:03:02.663Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# s=4\n# data = np.load('/kaggle/input/featurevgg-npz/extracted_features.npz', mmap_mode='r')\n# keys = list(data.files)\n# # key = keys[s]\n# # features = data[key]\n# # features = features.reshape(-1, features.shape[-2], 1, 1)\n# # features.extend([torch.from_numpy(feat) for feat in features])\n# x = data['64601.jpg']\n# print(x.shape)\n# x = x.reshape(7, -1, x.shape[-1])\n# x.shape","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-01T14:03:02.663Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !rm -rf /kaggle/working/*","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-01T14:03:02.663Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# if torch.cuda.is_available():\n#     print(f'CUDA is available. Number of GPUs: {torch.cuda.device_count()}')\n# else:\n#     print('CUDA is not available. Running on CPU.')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-01T14:03:02.663Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# param_size = 0\n# for param in model.parameters():\n#     param_size += param.nelement() * param.element_size()\n# buffer_size = 0\n# for buffer in model.buffers():\n#     buffer_size += buffer.nelement() * buffer.element_size()\n\n# size_all_mb = (param_size + buffer_size) / 2**20\n# print('model size: {:.3f}MB'.format(size_all_mb))","metadata":{"id":"OSI0IAeW2-xV","executionInfo":{"status":"ok","timestamp":1737277710529,"user_tz":-330,"elapsed":3,"user":{"displayName":"Singaraju B V Sreedakshinya .","userId":"04308658734104705074"}},"trusted":true,"execution":{"execution_failed":"2025-04-01T14:03:02.664Z"}},"outputs":[],"execution_count":null}]}