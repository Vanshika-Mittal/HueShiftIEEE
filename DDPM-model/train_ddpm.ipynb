{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DDPM_CONFIG = \"/kaggle/input/ddpm-configs/other/ddpm-configs/1/ddpm.yaml\"\n",
    "ROOT = \"/kaggle/input/random-frames-ucf-101/Data\"\n",
    "TERM_OUT = open(\"terminal_output.txt\",'w')\n",
    "CKPT_PATH = \"ddpm.pth\"\n",
    "RESUME = False\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "IMG_SIZE = 64\n",
    "NUM_EPOCHS = 1\n",
    "NUM_SAMPLES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "IMG_SAVE_ROOT = \"outputs\"\n",
    "CKPT_SAVE = \"checkpoints\"\n",
    "\n",
    "\n",
    "os.makedirs(IMG_SAVE_ROOT,exist_ok=True)\n",
    "os.makedirs(CKPT_SAVE,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Read the config file #\n",
    "with open(DDPM_CONFIG, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "ddpm_model_config = config['model_config']\n",
    "ddpm_dataset_config = config['dataset_config']\n",
    "ddpm_training_config = config['training_config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self , root , transform=None):\n",
    "        self.root = root\n",
    "        self.files = os.listdir(root)\n",
    "        self.len = len(self.files)\n",
    "        if transform is not None:\n",
    "            self.transforms = transforms.Compose(transform)\n",
    "        else:\n",
    "            self.transforms = None\n",
    "\n",
    "        \n",
    "    def __getitem__(self , i):\n",
    "        file = self.files[i]\n",
    "        im = cv2.imread(f'{self.root}/{file}')\n",
    "        im = cv2.cvtColor(im, cv2.COLOR_BGR2LAB)\n",
    "        if self.transforms is not None:\n",
    "            im = self.transforms(im)\n",
    "        return im\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LinearNoiseSchedule:\n",
    "    def __init__(self, T):\n",
    "        super().__init__()\n",
    "        beta_start = 1E-4\n",
    "        beta_end = 0.02\n",
    "\n",
    "        self.beta = torch.linspace(beta_start,beta_end,T,dtype=torch.float32)\n",
    "        self.alpha = 1 - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "        # for forward process\n",
    "        self.sqrt_alpha_hat = torch.sqrt(self.alpha_hat) # for mean\n",
    "        self.sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat) #for std_dev\n",
    "\n",
    "        # for sampling process\n",
    "        self.one_by_sqrt_alpha = 1. / torch.sqrt(self.alpha) # for mean\n",
    "        self.one_by_sqrt_one_minus_alpha_hat = 1. / self.sqrt_one_minus_alpha_hat # for mean\n",
    "        self.sqrt_beta = torch.sqrt(self.beta) # for std_dev\n",
    "        \n",
    "    def forward(self, x0, t):\n",
    "        #separte L from A and B --> c0\n",
    "        l,c0 = torch.split(x0,[1,2],dim=1)\n",
    "        noise = torch.randn_like(c0).to(x0.device)\n",
    "\n",
    "        sqrt_alpha_hat = self.sqrt_alpha_hat.to(x0.device)[t]\n",
    "        sqrt_one_minus_alpha_hat = self.sqrt_one_minus_alpha_hat.to(x0.device)[t]\n",
    "\n",
    "        # reshape to match no of dims (b,) -> (b,c,h,w)\n",
    "        for _ in range(len(x0.shape) - 1):\n",
    "            sqrt_alpha_hat = sqrt_alpha_hat.unsqueeze(-1)\n",
    "            sqrt_one_minus_alpha_hat = sqrt_one_minus_alpha_hat.unsqueeze(-1)\n",
    "\n",
    "        mean = sqrt_alpha_hat.to(x0.device) * c0\n",
    "        std_dev = sqrt_one_minus_alpha_hat.to(x0.device)\n",
    "\n",
    "        sample = mean + std_dev * noise \n",
    "\n",
    "        sample = torch.cat([l,sample],dim=1)\n",
    "        return sample, noise # noise --> predicted by the model\n",
    "    \n",
    "    def backward(self,xt,noise_pred,t):\n",
    "        l,ct = torch.split(xt,[1,2],dim=1)\n",
    "        # Reshaping\n",
    "        one_by_sqrt_alpha = self.one_by_sqrt_alpha.to(xt.device)[t]\n",
    "        beta = self.beta.to(xt.device)[t]\n",
    "        one_by_sqrt_one_minus_alpha_hat = self.one_by_sqrt_one_minus_alpha_hat.to(xt.device)[t]\n",
    "        sqrt_beta = self.sqrt_beta.to(xt.device)[t]\n",
    "\n",
    "        # reshape to match no of dims (b,) -> (b,c,h,w)\n",
    "        for _ in range(len(xt.shape) - 1):\n",
    "            one_by_sqrt_alpha = one_by_sqrt_alpha.unsqueeze(-1)\n",
    "            one_by_sqrt_one_minus_alpha_hat = one_by_sqrt_one_minus_alpha_hat.unsqueeze(-1)\n",
    "            beta = beta.unsqueeze(-1)\n",
    "        \n",
    "        mean = one_by_sqrt_alpha * (ct - beta * one_by_sqrt_one_minus_alpha_hat * noise_pred)\n",
    "        std_dev = sqrt_beta\n",
    "\n",
    "\n",
    "        if t==0:\n",
    "            out = torch.cat([l, mean],dim=1)\n",
    "        else:\n",
    "            z = torch.randn_like(ct).to(xt.device)\n",
    "            out = torch.cat([l, mean + std_dev * z],dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_time_embedding(time_steps, temb_dim):\n",
    "    r\"\"\"\n",
    "    Convert time steps tensor into an embedding using the\n",
    "    sinusoidal time embedding formula\n",
    "    :param time_steps: 1D tensor of length batch size\n",
    "    :param temb_dim: Dimension of the embedding\n",
    "    :return: BxD embedding representation of B time steps\n",
    "    \"\"\"\n",
    "    assert temb_dim % 2 == 0, \"time embedding dimension must be divisible by 2\"\n",
    "    \n",
    "    # factor = 10000^(2i/d_model)\n",
    "    factor = 10000 ** ((torch.arange(\n",
    "        start=0, end=temb_dim // 2, dtype=torch.float32, device=time_steps.device) / (temb_dim // 2))\n",
    "    )\n",
    "    \n",
    "    # pos / factor\n",
    "    # timesteps B -> B, 1 -> B, temb_dim\n",
    "    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n",
    "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n",
    "    return t_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Down conv block with attention.\n",
    "    Sequence of following block\n",
    "    1. Resnet block with time embedding\n",
    "    2. Attention block\n",
    "    3. Downsample\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim,\n",
    "                 down_sample, num_heads, num_layers, attn, norm_channels, cross_attn=False, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.down_sample = down_sample\n",
    "        self.attn = attn\n",
    "        self.context_dim = context_dim\n",
    "        self.cross_attn = cross_attn\n",
    "        self.t_emb_dim = t_emb_dim\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels,\n",
    "                              kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        if self.t_emb_dim is not None:\n",
    "            self.t_emb_layers = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(self.t_emb_dim, out_channels)\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ])\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels,\n",
    "                              kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if self.attn:\n",
    "            self.attention_norms = nn.ModuleList(\n",
    "                [nn.GroupNorm(norm_channels, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            \n",
    "            self.attentions = nn.ModuleList(\n",
    "                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        #DownSampling\n",
    "        self.down_sample_conv = nn.Conv2d(out_channels, out_channels,\n",
    "                                          4, 2, 1) if self.down_sample else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, t_emb=None, context=None):\n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            # Resnet block of Unet\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            if self.t_emb_dim is not None:\n",
    "                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input) # residual connection\n",
    "            \n",
    "            if self.attn:\n",
    "                # Attention block of Unet\n",
    "                batch_size, channels, h, w = out.shape\n",
    "                in_attn = out.reshape(batch_size, channels, h * w)\n",
    "                in_attn = self.attention_norms[i](in_attn)\n",
    "                in_attn = in_attn.transpose(1, 2)\n",
    "                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "                out = out + out_attn\n",
    "            \n",
    "        # Downsample\n",
    "        out = self.down_sample_conv(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MidBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Mid conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Resnet block with time embedding\n",
    "    2. Attention block\n",
    "    3. Resnet block with time embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads, num_layers, norm_channels, cross_attn=None, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.t_emb_dim = t_emb_dim\n",
    "        self.context_dim = context_dim\n",
    "        self.cross_attn = cross_attn\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if self.t_emb_dim is not None:\n",
    "            self.t_emb_layers = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(t_emb_dim, out_channels)\n",
    "                )\n",
    "                for _ in range(num_layers + 1)\n",
    "            ])\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [nn.GroupNorm(norm_channels, out_channels)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "        self.attentions = nn.ModuleList(\n",
    "            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t_emb=None, context=None):\n",
    "        out = x\n",
    "        \n",
    "        # First resnet block\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first[0](out)\n",
    "        if self.t_emb_dim is not None:\n",
    "            out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n",
    "        out = self.resnet_conv_second[0](out)\n",
    "        out = out + self.residual_input_conv[0](resnet_input)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            # Attention Block\n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "                \n",
    "            \n",
    "            # Resnet Block\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i + 1](out)\n",
    "            if self.t_emb_dim is not None:\n",
    "                out = out + self.t_emb_layers[i + 1](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i + 1](out)\n",
    "            out = out + self.residual_input_conv[i + 1](resnet_input)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Up conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Upsample\n",
    "    1. Concatenate Down block output\n",
    "    2. Resnet block with time embedding\n",
    "    3. Attention Block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, task, in_channels, out_channels, t_emb_dim,\n",
    "                 up_sample, num_heads, num_layers, attn, norm_channels):\n",
    "        super().__init__()\n",
    "        self.task = task\n",
    "        self.num_layers = num_layers\n",
    "        self.up_sample = up_sample\n",
    "        self.t_emb_dim = t_emb_dim\n",
    "        self.attn = attn\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if self.t_emb_dim is not None:\n",
    "            self.t_emb_layers = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(t_emb_dim, out_channels)\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ])\n",
    "        \n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        if self.attn:\n",
    "            self.attention_norms = nn.ModuleList(\n",
    "                [\n",
    "                    nn.GroupNorm(norm_channels, out_channels)\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            self.attentions = nn.ModuleList(\n",
    "                [\n",
    "                    nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        if self.task == \"unet\":\n",
    "            self.up_sample_conv = nn.ConvTranspose2d(in_channels//2, in_channels//2, 4, 2, 1) if self.up_sample else nn.Identity()\n",
    "        else: \n",
    "            self.up_sample_conv = nn.ConvTranspose2d(in_channels, in_channels, 4, 2, 1) if self.up_sample else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, out_down=None, t_emb=None):\n",
    "        # Upsample\n",
    "        x = self.up_sample_conv(x)\n",
    "        \n",
    "        # Concat with Downblock output\n",
    "        # used in diffusion but not in AE(since the output of encoder should be included in input of decoder)..maintain independance    \n",
    "        if out_down is not None:\n",
    "            x = torch.cat([x, out_down], dim=1)\n",
    "        \n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            # Resnet Block\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            if self.t_emb_dim is not None:\n",
    "                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "            \n",
    "            # Self Attention\n",
    "            if self.attn:\n",
    "                batch_size, channels, h, w = out.shape\n",
    "                in_attn = out.reshape(batch_size, channels, h * w)\n",
    "                in_attn = self.attention_norms[i](in_attn)\n",
    "                in_attn = in_attn.transpose(1, 2)\n",
    "                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "                out = out + out_attn\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    r\"\"\"\n",
    "    Unet model comprising\n",
    "    Down blocks, Midblocks and Uplocks\n",
    "    \"\"\"\n",
    "    # im_channels will be the no of input channels (latent channels)\n",
    "    def __init__(self, in_channels, out_channels, model_config, condition=False):\n",
    "        super().__init__()\n",
    "        self.condition = condition\n",
    "        self.down_channels = model_config['DOWN_CHANNELS'] # [256, 384, 512, 768]\n",
    "        self.mid_channels = model_config['MID_CHANNELS'] # [768, 512]\n",
    "        self.t_emb_dim = model_config['TIME_EMB_DIM'] # 512\n",
    "        self.down_sample = model_config['DOWN_SAMPLE'] # [True, True, True]\n",
    "        self.num_down_layers = model_config['NUM_DOWN_LAYERS'] # 2\n",
    "        self.num_mid_layers = model_config['NUM_MID_LAYERS'] # 2\n",
    "        self.num_up_layers = model_config['NUM_UP_LAYERS'] # 2\n",
    "        self.attns = model_config['ATTN'] # [True, True, True]\n",
    "        self.norm_channels = model_config['NORM_CHANNELS'] # 32\n",
    "        self.num_heads = model_config['NUM_HEADS'] # 16\n",
    "        self.conv_out_channels = model_config['CONV_OUT_CHANNELS'] # 128\n",
    "        \n",
    "        assert self.mid_channels[0] == self.down_channels[-1]\n",
    "        assert self.mid_channels[-1] == self.down_channels[-2]\n",
    "        assert len(self.down_sample) == len(self.down_channels) - 1\n",
    "        assert len(self.attns) == len(self.down_channels) - 1\n",
    "\n",
    "\n",
    "        # Spatial Conditioning\n",
    "        if self.condition:\n",
    "            self.cond_channels = model_config['CONDITION']['COND_CHANNELS']\n",
    "            self.conv_in_concat = nn.Conv2d(in_channels + self.cond_channels,\n",
    "                                            self.down_channels[0], kernel_size=3, padding=1)\n",
    "        else:\n",
    "            self.conv_in = nn.Conv2d(in_channels, self.down_channels[0], kernel_size=3, padding=1)\n",
    "        \n",
    "        # Initial projection from sinusoidal time embedding\n",
    "        self.t_proj = nn.Sequential(\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim)\n",
    "        )\n",
    "        \n",
    "        # only change is to add time embeddings\n",
    "        self.downs = nn.ModuleList([])\n",
    "        for i in range(len(self.down_channels) - 1):\n",
    "            self.downs.append(DownBlock(self.down_channels[i], self.down_channels[i + 1], \n",
    "                                        t_emb_dim=self.t_emb_dim,down_sample=self.down_sample[i],\n",
    "                                        num_heads=self.num_heads,\n",
    "                                        num_layers=self.num_down_layers,\n",
    "                                        attn=self.attns[i],\n",
    "                                        norm_channels=self.norm_channels))\n",
    "\n",
    "        self.mids = nn.ModuleList([])\n",
    "        for i in range(len(self.mid_channels) - 1):\n",
    "            self.mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i + 1], self.t_emb_dim,\n",
    "                                      num_heads=self.num_heads,\n",
    "                                      num_layers=self.num_mid_layers,\n",
    "                                      norm_channels=self.norm_channels))\n",
    "        \n",
    "        self.ups = nn.ModuleList([])\n",
    "        for i in reversed(range(len(self.down_channels) - 1)):\n",
    "            self.ups.append(UpBlock(\"unet\",self.down_channels[i] * 2, self.down_channels[i - 1] if i != 0 else self.conv_out_channels,\n",
    "                                    self.t_emb_dim, up_sample=self.down_sample[i],\n",
    "                                        num_heads=self.num_heads,\n",
    "                                        num_layers=self.num_up_layers,\n",
    "                                        attn=self.attns[i],\n",
    "                                        norm_channels=self.norm_channels))\n",
    "        \n",
    "        self.norm_out = nn.GroupNorm(self.norm_channels, self.conv_out_channels)\n",
    "        self.conv_out = nn.Conv2d(self.conv_out_channels, out_channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x, t, cond_in = None):\n",
    "        # Shapes assuming downblocks are [C1, C2, C3, C4]\n",
    "        # Shapes assuming midblocks are [C4, C4, C3]\n",
    "        # Shapes assuming downsamples are [True, True, False]\n",
    "        # B x C x H x W\n",
    "        if cond_in is not None:\n",
    "            # cond_in = self.cond_conv_in(cond_in)\n",
    "            cond_in = nn.functional.interpolate(size = x.shape[-2:])\n",
    "            x = torch.concat([x, cond_in],dim=1)\n",
    "            out = self.conv_in_concat(x)\n",
    "\n",
    "        else:\n",
    "            out = self.conv_in(x)\n",
    "\n",
    "        # B x C1 x H x W\n",
    "        \n",
    "        # t_emb -> B x t_emb_dim\n",
    "        t_emb = get_time_embedding(torch.as_tensor(t).long(), self.t_emb_dim)\n",
    "        t_emb = self.t_proj(t_emb)\n",
    "        \n",
    "        down_outs = []\n",
    "        for down in self.downs:\n",
    "            down_outs.append(out)\n",
    "            out = down(out, t_emb)\n",
    "        # down_outs  [B x C1 x H x W, B x C2 x H/2 x W/2, B x C3 x H/4 x W/4]\n",
    "        # out B x C4 x H/4 x W/4\n",
    "        \n",
    "        for mid in self.mids:\n",
    "            out = mid(out, t_emb)\n",
    "        # out B x C3 x H/4 x W/4\n",
    "        \n",
    "        for up in self.ups:\n",
    "            down_out = down_outs.pop()\n",
    "            out = up(out, down_out, t_emb)\n",
    "            # out [B x C2 x H/4 x W/4, B x C1 x H/2 x W/2, B x 16 x H x W]\n",
    "        out = self.norm_out(out)\n",
    "        out = nn.SiLU()(out)\n",
    "        out = self.conv_out(out)\n",
    "        # out B x C x H x W\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def lab_to_rgb(lab_image):\n",
    "    # OpenCV expects the range [0, 255] for color images\n",
    "    lab = (lab_image * 255).astype(np.uint8)  # Scale to [0, 255]\n",
    "    rgb = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)  # Convert LAB to BGR\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sample(sample_loader, model, scheduler, sample_no):\n",
    "    print(\"Sampling\")\n",
    "    print(\"Sampling\",file =TERM_OUT, flush=True)\n",
    "    \n",
    "    gt = next(iter(sample_loader))\n",
    "    gt = gt.float().to(DEVICE)\n",
    "    L, _ = torch.split(gt,[1,2],dim=1)\n",
    "    L = L.float().to(DEVICE)\n",
    "\n",
    "    num_samples = L.shape[0]\n",
    "    out_shape = (num_samples,2,IMG_SIZE,IMG_SIZE)\n",
    "    xt = torch.cat([L, torch.randn(out_shape).to(DEVICE)],dim=1).to(DEVICE)\n",
    "\n",
    "    for t in reversed(range(NUM_TIMESTEPS)):\n",
    "        # Get prediction of noise\n",
    "        timestep = torch.ones(num_samples, dtype=torch.long, device=DEVICE) * t\n",
    "\n",
    "        noise_pred = model(xt, timestep)\n",
    "        \n",
    "        # Use scheduler to get x0 and xt-1\n",
    "        xt = scheduler.backward(xt, noise_pred, t)\n",
    "        ims = xt\n",
    "\n",
    "    # Add Ground Truths\n",
    "    save_output = ims.cpu()\n",
    "    save_output = save_output.permute(0,2,3,1).numpy() #open cv compatible\n",
    "    save_output = np.array([lab_to_rgb(img) for img in save_output]) #convert lab->rgb\n",
    "    save_output = torch.tensor(save_output).permute(0, 3, 1, 2).float() / 255.0 #convert to torch tensor image\n",
    "\n",
    "    save_input = gt.cpu()\n",
    "    save_input = save_input.permute(0,2,3,1).numpy()\n",
    "    save_input = np.array([lab_to_rgb(img) for img in save_input])\n",
    "    save_input = torch.tensor(save_input).permute(0, 3, 1, 2).float() / 255.0\n",
    "    \n",
    "    grid = make_grid(torch.cat([save_input, save_output], dim=0), nrow=num_samples)\n",
    "    img = torchvision.transforms.ToPILImage()(grid)\n",
    "\n",
    "    img.save(f\"{IMG_SAVE_ROOT}/{str(sample_no).zfill(10)}.jpg\")\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scheduler = LinearNoiseSchedule(T=NUM_TIMESTEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "transform = [\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE), Image.BICUBIC),\n",
    "]\n",
    "\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    Dataset(ROOT,transform),\n",
    "    batch_size= BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = 2\n",
    ")\n",
    "sample_loader = DataLoader(\n",
    "    Dataset(ROOT,transform),\n",
    "    batch_size= NUM_SAMPLES,\n",
    "    shuffle = True,\n",
    "    num_workers = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = Unet(in_channels = 3, out_channels = 2, model_config = ddpm_model_config).to(DEVICE)\n",
    "if RESUME: \n",
    "    print(\"loading state dict\")\n",
    "    print(\"loading state dict\",file=TERM_OUT,flush=True)\n",
    "    model.load_state_dict(torch.load(CKPT_PATH))\n",
    "model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(),lr=1E-5)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(\n",
    "        num_epochs,\n",
    "        data_loader,\n",
    "        optimizer,\n",
    "        T,\n",
    "        scheduler,\n",
    "        model,\n",
    "        criterion,\n",
    "        sample_step,\n",
    "        sample_loader\n",
    "):\n",
    "    step_count = 0\n",
    "    sample_no = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(epoch)\n",
    "        print(epoch,file=TERM_OUT, flush=True)\n",
    "        losses = []\n",
    "        model.train()\n",
    "        for im in tqdm(data_loader):\n",
    "            step_count+=1\n",
    "            optimizer.zero_grad()\n",
    "            im = im.float().to(DEVICE)\n",
    "            L, _ = torch.split(im,[1,2],dim=1)\n",
    "            L = L.float().to(DEVICE)\n",
    "\n",
    "            t = torch.randint(0,T,(im.shape[0],)).to(DEVICE)\n",
    "\n",
    "            noisy_im, noise = scheduler.forward(im, t)\n",
    "            noise_pred = model(noisy_im, t)\n",
    "\n",
    "            loss = criterion(noise_pred, noise)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if step_count % sample_step == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    sample(sample_loader, model, scheduler, sample_no)\n",
    "                sample_no+=1\n",
    "                model.train()\n",
    "        print(f\"{epoch} Loss {np.mean(losses)}\")\n",
    "        print(f\"{epoch} Loss {np.mean(losses)}\",file=TERM_OUT, flush=True)\n",
    "        torch.save(model.module.state_dict(), f\"{CKPT_SAVE}/ddpm.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Training\")\n",
    "print(\"Training\",file =TERM_OUT, flush=True)\n",
    "train(\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    data_loader = data_loader,\n",
    "    optimizer = optimizer,\n",
    "    T = NUM_TIMESTEPS,\n",
    "    scheduler = scheduler,\n",
    "    model = model,\n",
    "    criterion = criterion,\n",
    "    sample_step = SAMPLE_STEP,\n",
    "    sample_loader=sample_loader\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6338623,
     "sourceId": 10248389,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 210431,
     "modelInstanceId": 188404,
     "sourceId": 220912,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 281176,
     "modelInstanceId": 260022,
     "sourceId": 304701,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "hue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
