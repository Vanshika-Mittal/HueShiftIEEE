{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d87a603",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T09:07:27.018646Z",
     "iopub.status.busy": "2025-03-29T09:07:27.018411Z",
     "iopub.status.idle": "2025-03-29T09:07:27.024108Z",
     "shell.execute_reply": "2025-03-29T09:07:27.023482Z"
    },
    "papermill": {
     "duration": 0.01095,
     "end_time": "2025-03-29T09:07:27.025282",
     "exception": false,
     "start_time": "2025-03-29T09:07:27.014332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DDPM_CONFIG = \"/kaggle/input/ddpm-configs/other/ddpm-configs/1/ddpm.yaml\"\n",
    "ROOT = \"/kaggle/input/ucf-test-frames/Sample_Frames\"\n",
    "TERM_OUT = open(\"terminal_output.txt\",'w')\n",
    "CKPT_PATH = \"/kaggle/input/vastai_hueshift_epoch8/pytorch/default/1/ddpm.pth\"\n",
    "RESUME = True\n",
    "\n",
    "IMG_SIZE = 64\n",
    "NUM_SAMPLES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc681846",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T09:07:27.031698Z",
     "iopub.status.busy": "2025-03-29T09:07:27.031486Z",
     "iopub.status.idle": "2025-03-29T09:07:27.034840Z",
     "shell.execute_reply": "2025-03-29T09:07:27.034095Z"
    },
    "papermill": {
     "duration": 0.008039,
     "end_time": "2025-03-29T09:07:27.036302",
     "exception": false,
     "start_time": "2025-03-29T09:07:27.028263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "IMG_SAVE_ROOT = \"outputs\"\n",
    "os.makedirs(IMG_SAVE_ROOT,exist_ok=True)\n",
    "NUM_TIMESTEPS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed5c976",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T09:07:27.042755Z",
     "iopub.status.busy": "2025-03-29T09:07:27.042541Z",
     "iopub.status.idle": "2025-03-29T09:07:37.436980Z",
     "shell.execute_reply": "2025-03-29T09:07:37.436268Z"
    },
    "papermill": {
     "duration": 10.399297,
     "end_time": "2025-03-29T09:07:37.438561",
     "exception": false,
     "start_time": "2025-03-29T09:07:27.039264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ffa8d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T09:07:37.445630Z",
     "iopub.status.busy": "2025-03-29T09:07:37.445299Z",
     "iopub.status.idle": "2025-03-29T09:07:37.457373Z",
     "shell.execute_reply": "2025-03-29T09:07:37.456761Z"
    },
    "papermill": {
     "duration": 0.016784,
     "end_time": "2025-03-29T09:07:37.458562",
     "exception": false,
     "start_time": "2025-03-29T09:07:37.441778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the config file #\n",
    "with open(DDPM_CONFIG, 'r') as file:\n",
    "    try:\n",
    "        config = yaml.safe_load(file)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "ddpm_model_config = config['model_config']\n",
    "ddpm_dataset_config = config['dataset_config']\n",
    "ddpm_training_config = config['training_config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d909b7ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T09:07:37.465103Z",
     "iopub.status.busy": "2025-03-29T09:07:37.464862Z",
     "iopub.status.idle": "2025-03-29T09:07:37.474529Z",
     "shell.execute_reply": "2025-03-29T09:07:37.473941Z"
    },
    "papermill": {
     "duration": 0.014097,
     "end_time": "2025-03-29T09:07:37.475595",
     "exception": false,
     "start_time": "2025-03-29T09:07:37.461498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LinearNoiseSchedule:\n",
    "    def __init__(self, T):\n",
    "        super().__init__()\n",
    "        beta_start = 1E-4\n",
    "        beta_end = 0.02\n",
    "\n",
    "        self.beta = torch.linspace(beta_start,beta_end,T,dtype=torch.float32)\n",
    "        self.alpha = 1 - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "        # for forward process\n",
    "        self.sqrt_alpha_hat = torch.sqrt(self.alpha_hat) # for mean\n",
    "        self.sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat) #for std_dev\n",
    "\n",
    "        # for sampling process\n",
    "        self.one_by_sqrt_alpha = 1. / torch.sqrt(self.alpha) # for mean\n",
    "        self.one_by_sqrt_one_minus_alpha_hat = 1. / self.sqrt_one_minus_alpha_hat # for mean\n",
    "        self.sqrt_beta = torch.sqrt(self.beta) # for std_dev\n",
    "        \n",
    "    def forward(self, x0, t):\n",
    "        #separte L from A and B --> c0\n",
    "        l,c0 = torch.split(x0,[1,2],dim=1)\n",
    "        noise = torch.randn_like(c0).to(x0.device)\n",
    "\n",
    "        sqrt_alpha_hat = self.sqrt_alpha_hat.to(x0.device)[t]\n",
    "        sqrt_one_minus_alpha_hat = self.sqrt_one_minus_alpha_hat.to(x0.device)[t]\n",
    "\n",
    "        # reshape to match no of dims (b,) -> (b,c,h,w)\n",
    "        for _ in range(len(x0.shape) - 1):\n",
    "            sqrt_alpha_hat = sqrt_alpha_hat.unsqueeze(-1)\n",
    "            sqrt_one_minus_alpha_hat = sqrt_one_minus_alpha_hat.unsqueeze(-1)\n",
    "\n",
    "        mean = sqrt_alpha_hat.to(x0.device) * c0\n",
    "        std_dev = sqrt_one_minus_alpha_hat.to(x0.device)\n",
    "\n",
    "        sample = mean + std_dev * noise \n",
    "\n",
    "        sample = torch.cat([l,sample],dim=1)\n",
    "        return sample, noise # noise --> predicted by the model\n",
    "    \n",
    "    def backward(self,xt,noise_pred,t):\n",
    "        l,ct = torch.split(xt,[1,2],dim=1)\n",
    "        # Reshaping\n",
    "        one_by_sqrt_alpha = self.one_by_sqrt_alpha.to(xt.device)[t]\n",
    "        beta = self.beta.to(xt.device)[t]\n",
    "        one_by_sqrt_one_minus_alpha_hat = self.one_by_sqrt_one_minus_alpha_hat.to(xt.device)[t]\n",
    "        sqrt_beta = self.sqrt_beta.to(xt.device)[t]\n",
    "\n",
    "        # reshape to match no of dims (b,) -> (b,c,h,w)\n",
    "        for _ in range(len(xt.shape) - 1):\n",
    "            one_by_sqrt_alpha = one_by_sqrt_alpha.unsqueeze(-1)\n",
    "            one_by_sqrt_one_minus_alpha_hat = one_by_sqrt_one_minus_alpha_hat.unsqueeze(-1)\n",
    "            beta = beta.unsqueeze(-1)\n",
    "        \n",
    "        mean = one_by_sqrt_alpha * (ct - beta * one_by_sqrt_one_minus_alpha_hat * noise_pred)\n",
    "        std_dev = sqrt_beta\n",
    "\n",
    "\n",
    "        if t==0:\n",
    "            out = torch.cat([l, mean],dim=1)\n",
    "        else:\n",
    "            z = torch.randn_like(ct).to(xt.device)\n",
    "            out = torch.cat([l, mean + std_dev * z],dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4974df56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T09:07:37.482213Z",
     "iopub.status.busy": "2025-03-29T09:07:37.482005Z",
     "iopub.status.idle": "2025-03-29T09:07:37.485955Z",
     "shell.execute_reply": "2025-03-29T09:07:37.485354Z"
    },
    "papermill": {
     "duration": 0.008618,
     "end_time": "2025-03-29T09:07:37.487249",
     "exception": false,
     "start_time": "2025-03-29T09:07:37.478631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_time_embedding(time_steps, temb_dim):\n",
    "    r\"\"\"\n",
    "    Convert time steps tensor into an embedding using the\n",
    "    sinusoidal time embedding formula\n",
    "    :param time_steps: 1D tensor of length batch size\n",
    "    :param temb_dim: Dimension of the embedding\n",
    "    :return: BxD embedding representation of B time steps\n",
    "    \"\"\"\n",
    "    assert temb_dim % 2 == 0, \"time embedding dimension must be divisible by 2\"\n",
    "    \n",
    "    # factor = 10000^(2i/d_model)\n",
    "    factor = 10000 ** ((torch.arange(\n",
    "        start=0, end=temb_dim // 2, dtype=torch.float32, device=time_steps.device) / (temb_dim // 2))\n",
    "    )\n",
    "    \n",
    "    # pos / factor\n",
    "    # timesteps B -> B, 1 -> B, temb_dim\n",
    "    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n",
    "    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim=-1)\n",
    "    return t_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc9c75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T09:07:37.497029Z",
     "iopub.status.busy": "2025-03-29T09:07:37.496646Z",
     "iopub.status.idle": "2025-03-29T09:07:37.529229Z",
     "shell.execute_reply": "2025-03-29T09:07:37.528573Z"
    },
    "papermill": {
     "duration": 0.039942,
     "end_time": "2025-03-29T09:07:37.530476",
     "exception": false,
     "start_time": "2025-03-29T09:07:37.490534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DownBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Down conv block with attention.\n",
    "    Sequence of following block\n",
    "    1. Resnet block with time embedding\n",
    "    2. Attention block\n",
    "    3. Downsample\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim,\n",
    "                 down_sample, num_heads, num_layers, attn, norm_channels, cross_attn=False, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.down_sample = down_sample\n",
    "        self.attn = attn\n",
    "        self.context_dim = context_dim\n",
    "        self.cross_attn = cross_attn\n",
    "        self.t_emb_dim = t_emb_dim\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels,\n",
    "                              kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        if self.t_emb_dim is not None:\n",
    "            self.t_emb_layers = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(self.t_emb_dim, out_channels)\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ])\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels,\n",
    "                              kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if self.attn:\n",
    "            self.attention_norms = nn.ModuleList(\n",
    "                [nn.GroupNorm(norm_channels, out_channels)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "            \n",
    "            self.attentions = nn.ModuleList(\n",
    "                [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                 for _ in range(num_layers)]\n",
    "            )\n",
    "\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        #DownSampling\n",
    "        self.down_sample_conv = nn.Conv2d(out_channels, out_channels,\n",
    "                                          4, 2, 1) if self.down_sample else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, t_emb=None, context=None):\n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            # Resnet block of Unet\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            if self.t_emb_dim is not None:\n",
    "                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input) # residual connection\n",
    "            \n",
    "            if self.attn:\n",
    "                # Attention block of Unet\n",
    "                batch_size, channels, h, w = out.shape\n",
    "                in_attn = out.reshape(batch_size, channels, h * w)\n",
    "                in_attn = self.attention_norms[i](in_attn)\n",
    "                in_attn = in_attn.transpose(1, 2)\n",
    "                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "                out = out + out_attn\n",
    "            \n",
    "        # Downsample\n",
    "        out = self.down_sample_conv(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MidBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Mid conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Resnet block with time embedding\n",
    "    2. Attention block\n",
    "    3. Resnet block with time embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads, num_layers, norm_channels, cross_attn=None, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.t_emb_dim = t_emb_dim\n",
    "        self.context_dim = context_dim\n",
    "        self.cross_attn = cross_attn\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if self.t_emb_dim is not None:\n",
    "            self.t_emb_layers = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(t_emb_dim, out_channels)\n",
    "                )\n",
    "                for _ in range(num_layers + 1)\n",
    "            ])\n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.attention_norms = nn.ModuleList(\n",
    "            [nn.GroupNorm(norm_channels, out_channels)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "        self.attentions = nn.ModuleList(\n",
    "            [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "             for _ in range(num_layers)]\n",
    "        )\n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers + 1)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, t_emb=None, context=None):\n",
    "        out = x\n",
    "        \n",
    "        # First resnet block\n",
    "        resnet_input = out\n",
    "        out = self.resnet_conv_first[0](out)\n",
    "        if self.t_emb_dim is not None:\n",
    "            out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n",
    "        out = self.resnet_conv_second[0](out)\n",
    "        out = out + self.residual_input_conv[0](resnet_input)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            # Attention Block\n",
    "            batch_size, channels, h, w = out.shape\n",
    "            in_attn = out.reshape(batch_size, channels, h * w)\n",
    "            in_attn = self.attention_norms[i](in_attn)\n",
    "            in_attn = in_attn.transpose(1, 2)\n",
    "            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "            out = out + out_attn\n",
    "                \n",
    "            \n",
    "            # Resnet Block\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i + 1](out)\n",
    "            if self.t_emb_dim is not None:\n",
    "                out = out + self.t_emb_layers[i + 1](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i + 1](out)\n",
    "            out = out + self.residual_input_conv[i + 1](resnet_input)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    r\"\"\"\n",
    "    Up conv block with attention.\n",
    "    Sequence of following blocks\n",
    "    1. Upsample\n",
    "    1. Concatenate Down block output\n",
    "    2. Resnet block with time embedding\n",
    "    3. Attention Block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, task, in_channels, out_channels, t_emb_dim,\n",
    "                 up_sample, num_heads, num_layers, attn, norm_channels):\n",
    "        super().__init__()\n",
    "        self.task = task\n",
    "        self.num_layers = num_layers\n",
    "        self.up_sample = up_sample\n",
    "        self.t_emb_dim = t_emb_dim\n",
    "        self.attn = attn\n",
    "        self.resnet_conv_first = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=3, stride=1,\n",
    "                              padding=1),\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if self.t_emb_dim is not None:\n",
    "            self.t_emb_layers = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(t_emb_dim, out_channels)\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ])\n",
    "        \n",
    "        self.resnet_conv_second = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.GroupNorm(norm_channels, out_channels),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        if self.attn:\n",
    "            self.attention_norms = nn.ModuleList(\n",
    "                [\n",
    "                    nn.GroupNorm(norm_channels, out_channels)\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            self.attentions = nn.ModuleList(\n",
    "                [\n",
    "                    nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
    "                    for _ in range(num_layers)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "        self.residual_input_conv = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        if self.task == \"unet\":\n",
    "            self.up_sample_conv = nn.ConvTranspose2d(in_channels//2, in_channels//2, 4, 2, 1) if self.up_sample else nn.Identity()\n",
    "        else: \n",
    "            self.up_sample_conv = nn.ConvTranspose2d(in_channels, in_channels, 4, 2, 1) if self.up_sample else nn.Identity()\n",
    "    \n",
    "    def forward(self, x, out_down=None, t_emb=None):\n",
    "        # Upsample\n",
    "        x = self.up_sample_conv(x)\n",
    "        \n",
    "        # Concat with Downblock output\n",
    "        # used in diffusion but not in AE(since the output of encoder should be included in input of decoder)..maintain independance    \n",
    "        if out_down is not None:\n",
    "            x = torch.cat([x, out_down], dim=1)\n",
    "        \n",
    "        out = x\n",
    "        for i in range(self.num_layers):\n",
    "            # Resnet Block\n",
    "            resnet_input = out\n",
    "            out = self.resnet_conv_first[i](out)\n",
    "            if self.t_emb_dim is not None:\n",
    "                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n",
    "            out = self.resnet_conv_second[i](out)\n",
    "            out = out + self.residual_input_conv[i](resnet_input)\n",
    "            \n",
    "            # Self Attention\n",
    "            if self.attn:\n",
    "                batch_size, channels, h, w = out.shape\n",
    "                in_attn = out.reshape(batch_size, channels, h * w)\n",
    "                in_attn = self.attention_norms[i](in_attn)\n",
    "                in_attn = in_attn.transpose(1, 2)\n",
    "                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n",
    "                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
    "                out = out + out_attn\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53913ecf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T09:07:37.540005Z",
     "iopub.status.busy": "2025-03-29T09:07:37.539723Z",
     "iopub.status.idle": "2025-03-29T09:07:37.552236Z",
     "shell.execute_reply": "2025-03-29T09:07:37.551413Z"
    },
    "papermill": {
     "duration": 0.017948,
     "end_time": "2025-03-29T09:07:37.553541",
     "exception": false,
     "start_time": "2025-03-29T09:07:37.535593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    r\"\"\"\n",
    "    Unet model comprising\n",
    "    Down blocks, Midblocks and Uplocks\n",
    "    \"\"\"\n",
    "    # im_channels will be the no of input channels (latent channels)\n",
    "    def __init__(self, in_channels, out_channels, model_config, condition=False):\n",
    "        super().__init__()\n",
    "        self.condition = condition\n",
    "        self.down_channels = model_config['DOWN_CHANNELS'] # [256, 384, 512, 768]\n",
    "        self.mid_channels = model_config['MID_CHANNELS'] # [768, 512]\n",
    "        self.t_emb_dim = model_config['TIME_EMB_DIM'] # 512\n",
    "        self.down_sample = model_config['DOWN_SAMPLE'] # [True, True, True]\n",
    "        self.num_down_layers = model_config['NUM_DOWN_LAYERS'] # 2\n",
    "        self.num_mid_layers = model_config['NUM_MID_LAYERS'] # 2\n",
    "        self.num_up_layers = model_config['NUM_UP_LAYERS'] # 2\n",
    "        self.attns = model_config['ATTN'] # [True, True, True]\n",
    "        self.norm_channels = model_config['NORM_CHANNELS'] # 32\n",
    "        self.num_heads = model_config['NUM_HEADS'] # 16\n",
    "        self.conv_out_channels = model_config['CONV_OUT_CHANNELS'] # 128\n",
    "        \n",
    "        assert self.mid_channels[0] == self.down_channels[-1]\n",
    "        assert self.mid_channels[-1] == self.down_channels[-2]\n",
    "        assert len(self.down_sample) == len(self.down_channels) - 1\n",
    "        assert len(self.attns) == len(self.down_channels) - 1\n",
    "\n",
    "\n",
    "        # Spatial Conditioning\n",
    "        if self.condition:\n",
    "            self.cond_channels = model_config['CONDITION']['COND_CHANNELS']\n",
    "            self.conv_in_concat = nn.Conv2d(in_channels + self.cond_channels,\n",
    "                                            self.down_channels[0], kernel_size=3, padding=1)\n",
    "        else:\n",
    "            self.conv_in = nn.Conv2d(in_channels, self.down_channels[0], kernel_size=3, padding=1)\n",
    "        \n",
    "        # Initial projection from sinusoidal time embedding\n",
    "        self.t_proj = nn.Sequential(\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.t_emb_dim, self.t_emb_dim)\n",
    "        )\n",
    "        \n",
    "        # only change is to add time embeddings\n",
    "        self.downs = nn.ModuleList([])\n",
    "        for i in range(len(self.down_channels) - 1):\n",
    "            self.downs.append(DownBlock(self.down_channels[i], self.down_channels[i + 1], \n",
    "                                        t_emb_dim=self.t_emb_dim,down_sample=self.down_sample[i],\n",
    "                                        num_heads=self.num_heads,\n",
    "                                        num_layers=self.num_down_layers,\n",
    "                                        attn=self.attns[i],\n",
    "                                        norm_channels=self.norm_channels))\n",
    "\n",
    "        self.mids = nn.ModuleList([])\n",
    "        for i in range(len(self.mid_channels) - 1):\n",
    "            self.mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i + 1], self.t_emb_dim,\n",
    "                                      num_heads=self.num_heads,\n",
    "                                      num_layers=self.num_mid_layers,\n",
    "                                      norm_channels=self.norm_channels))\n",
    "        \n",
    "        self.ups = nn.ModuleList([])\n",
    "        for i in reversed(range(len(self.down_channels) - 1)):\n",
    "            self.ups.append(UpBlock(\"unet\",self.down_channels[i] * 2, self.down_channels[i - 1] if i != 0 else self.conv_out_channels,\n",
    "                                    self.t_emb_dim, up_sample=self.down_sample[i],\n",
    "                                        num_heads=self.num_heads,\n",
    "                                        num_layers=self.num_up_layers,\n",
    "                                        attn=self.attns[i],\n",
    "                                        norm_channels=self.norm_channels))\n",
    "        \n",
    "        self.norm_out = nn.GroupNorm(self.norm_channels, self.conv_out_channels)\n",
    "        self.conv_out = nn.Conv2d(self.conv_out_channels, out_channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x, t, cond_in = None):\n",
    "        # Shapes assuming downblocks are [C1, C2, C3, C4]\n",
    "        # Shapes assuming midblocks are [C4, C4, C3]\n",
    "        # Shapes assuming downsamples are [True, True, False]\n",
    "        # B x C x H x W\n",
    "        if cond_in is not None:\n",
    "            # cond_in = self.cond_conv_in(cond_in)\n",
    "            cond_in = nn.functional.interpolate(size = x.shape[-2:])\n",
    "            x = torch.concat([x, cond_in],dim=1)\n",
    "            out = self.conv_in_concat(x)\n",
    "\n",
    "        else:\n",
    "            out = self.conv_in(x)\n",
    "\n",
    "        # B x C1 x H x W\n",
    "        \n",
    "        # t_emb -> B x t_emb_dim\n",
    "        t_emb = get_time_embedding(torch.as_tensor(t).long(), self.t_emb_dim)\n",
    "        t_emb = self.t_proj(t_emb)\n",
    "        \n",
    "        down_outs = []\n",
    "        for down in self.downs:\n",
    "            down_outs.append(out)\n",
    "            out = down(out, t_emb)\n",
    "        # down_outs  [B x C1 x H x W, B x C2 x H/2 x W/2, B x C3 x H/4 x W/4]\n",
    "        # out B x C4 x H/4 x W/4\n",
    "        \n",
    "        for mid in self.mids:\n",
    "            out = mid(out, t_emb)\n",
    "        # out B x C3 x H/4 x W/4\n",
    "        \n",
    "        for up in self.ups:\n",
    "            down_out = down_outs.pop()\n",
    "            out = up(out, down_out, t_emb)\n",
    "            # out [B x C2 x H/4 x W/4, B x C1 x H/2 x W/2, B x 16 x H x W]\n",
    "        out = self.norm_out(out)\n",
    "        out = nn.SiLU()(out)\n",
    "        out = self.conv_out(out)\n",
    "        # out B x C x H x W\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a61f1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T09:07:37.560209Z",
     "iopub.status.busy": "2025-03-29T09:07:37.560000Z",
     "iopub.status.idle": "2025-03-29T09:07:37.563291Z",
     "shell.execute_reply": "2025-03-29T09:07:37.562660Z"
    },
    "papermill": {
     "duration": 0.007907,
     "end_time": "2025-03-29T09:07:37.564452",
     "exception": false,
     "start_time": "2025-03-29T09:07:37.556545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lab_to_rgb(lab_image):\n",
    "    # OpenCV expects the range [0, 255] for color images\n",
    "    lab = (lab_image * 255).astype(np.uint8)  # Scale to [0, 255]\n",
    "    rgb = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)  # Convert LAB to BGR\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22d2875",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T09:07:37.571165Z",
     "iopub.status.busy": "2025-03-29T09:07:37.570895Z",
     "iopub.status.idle": "2025-03-29T09:07:37.574858Z",
     "shell.execute_reply": "2025-03-29T09:07:37.574267Z"
    },
    "papermill": {
     "duration": 0.008451,
     "end_time": "2025-03-29T09:07:37.575920",
     "exception": false,
     "start_time": "2025-03-29T09:07:37.567469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = [\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE), Image.BICUBIC),\n",
    "]\n",
    "\n",
    "transformer = transforms.Compose(transform)\n",
    "\n",
    "def get_frame(path):\n",
    "    im = cv2.imread(path)\n",
    "    im = cv2.cvtColor(im, cv2.COLOR_BGR2LAB)\n",
    "    im = transformer(im)\n",
    "    im = im.unsqueeze(0)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd74ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T09:07:37.582851Z",
     "iopub.status.busy": "2025-03-29T09:07:37.582640Z",
     "iopub.status.idle": "2025-03-29T09:07:37.588331Z",
     "shell.execute_reply": "2025-03-29T09:07:37.587685Z"
    },
    "papermill": {
     "duration": 0.01045,
     "end_time": "2025-03-29T09:07:37.589479",
     "exception": false,
     "start_time": "2025-03-29T09:07:37.579029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample(in_path, out_path, model, scheduler):\n",
    "    print(\"Sampling\")\n",
    "    print(\"Sampling\",file =TERM_OUT, flush=True)\n",
    "    \n",
    "    gt = get_frame(in_path)\n",
    "    gt = gt.float().to(DEVICE)\n",
    "    L, _ = torch.split(gt,[1,2],dim=1)\n",
    "    L = L.float().to(DEVICE)\n",
    "\n",
    "    num_samples = L.shape[0]\n",
    "    out_shape = (num_samples,2,IMG_SIZE,IMG_SIZE)\n",
    "    xt = torch.cat([L, torch.randn(out_shape).to(DEVICE)],dim=1).to(DEVICE)\n",
    "\n",
    "    for t in tqdm(reversed(range(NUM_TIMESTEPS))):\n",
    "        # Get prediction of noise\n",
    "        timestep = torch.ones(num_samples, dtype=torch.long, device=DEVICE) * t\n",
    "\n",
    "        noise_pred = model(xt, timestep)\n",
    "        \n",
    "        # Use scheduler to get x0 and xt-1\n",
    "        xt = scheduler.backward(xt, noise_pred, t)\n",
    "        ims = xt\n",
    "\n",
    "    # Add Ground Truths\n",
    "    save_output = ims.cpu()\n",
    "    save_output = save_output.permute(0,2,3,1).numpy() #open cv compatible\n",
    "    save_output = np.array([lab_to_rgb(img) for img in save_output]) #convert lab->rgb\n",
    "    save_output = torch.tensor(save_output).permute(0, 3, 1, 2).float() / 255.0 #convert to torch tensor image\n",
    "    \n",
    "    grid = make_grid(save_output, nrow=1)\n",
    "    img = torchvision.transforms.ToPILImage()(grid)\n",
    "\n",
    "    img.save(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f759df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T09:07:37.597094Z",
     "iopub.status.busy": "2025-03-29T09:07:37.596733Z",
     "iopub.status.idle": "2025-03-29T09:07:37.708792Z",
     "shell.execute_reply": "2025-03-29T09:07:37.708193Z"
    },
    "papermill": {
     "duration": 0.11764,
     "end_time": "2025-03-29T09:07:37.710310",
     "exception": false,
     "start_time": "2025-03-29T09:07:37.592670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scheduler = LinearNoiseSchedule(T=NUM_TIMESTEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c5c630",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T09:07:37.717889Z",
     "iopub.status.busy": "2025-03-29T09:07:37.717669Z",
     "iopub.status.idle": "2025-03-29T09:07:43.370191Z",
     "shell.execute_reply": "2025-03-29T09:07:43.369474Z"
    },
    "papermill": {
     "duration": 5.657786,
     "end_time": "2025-03-29T09:07:43.371800",
     "exception": false,
     "start_time": "2025-03-29T09:07:37.714014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Unet(in_channels = 3, out_channels = 2, model_config = ddpm_model_config).to(DEVICE)\n",
    "if RESUME: \n",
    "    print(\"loading state dict\")\n",
    "    print(\"loading state dict\",file=TERM_OUT,flush=True)\n",
    "    model.load_state_dict(torch.load(CKPT_PATH))\n",
    "model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474be02d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T09:07:43.379641Z",
     "iopub.status.busy": "2025-03-29T09:07:43.379402Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-03-29T09:07:43.375536",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for video in os.listdir(ROOT):\n",
    "    for frame in os.listdir(f\"{ROOT}/{video}\"):\n",
    "        in_path = f\"{ROOT}/{video}/{frame}\"\n",
    "        os.makedirs(f\"{IMG_SAVE_ROOT}/{video}\",exist_ok=True)\n",
    "        out_path = f\"{IMG_SAVE_ROOT}/{video}/{frame}\"\n",
    "        print(in_path)\n",
    "        with torch.no_grad():\n",
    "            sample(in_path, out_path, model, scheduler)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6995642,
     "sourceId": 11204115,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 210431,
     "modelInstanceId": 188404,
     "sourceId": 220912,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 282209,
     "modelInstanceId": 261056,
     "sourceId": 305992,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "hue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-29T09:07:23.527148",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
