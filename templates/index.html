{% extends "base.html" %}

{% block title %}Home - Hue Shift{% endblock %}

{% block content %}
<!-- Hero Section -->
<div class="jumbotron p-5 bg-light rounded">
    <div class="row">
        <div class="col-md-7">
            <h1 class="display-4">HueShift: Breathing Life into Every Frame</h1>
            <p class="lead mt-3">
                HueShift transforms grayscale videos into vibrant color using two different deep learning approaches: 
                Diffusion Models and Generative Adversarial Networks (GANs). Our goal was to develop and compare 
                these methods, with a special focus on ensuring temporal consistency and realistic coloration across frames. 
            </p>
            <div class="mt-4">
                <a href="{{ url_for('gallery') }}" class="btn btn-primary btn-lg">View Gallery</a>
            </div>
        </div>
        <div class="col-md-5">
            <video class="img-fluid rounded shadow" autoplay loop muted>
                <source src="{{ url_for('static', filename='gallery/diffusion/final_videos/v-ApplyLipstick-g05-c04.mp4') }}" type="video/mp4">
            </video>
        </div>
    </div>
</div>

<!-- Why Video Colorization Section -->
<!--
<section class="my-5 bg-light py-5">
    <div class="container">
        <h2 class="text-center mb-4">Why Video Colorization?</h2>
        <div class="row">
            <div class="col-lg-8 mx-auto">
                <p>Colorization enhances visual quality and usability of grayscale media, opening new possibilities for:</p>
                <div class="row mt-4">
                    <div class="col-md-6">
                        <div class="card mb-3 h-100">
                            <div class="card-body">
                                <h5 class="card-title"><i class="bi bi-film text-primary me-2"></i>Historical Film Restoration</h5>
                                <p class="card-text">Revive historical footage with realistic color, making history more engaging.</p>
                            </div>
                        </div>
                    </div>
                    <div class="col-md-6">
                        <div class="card mb-3 h-100">
                            <div class="card-body">
                                <h5 class="card-title"><i class="bi bi-palette text-primary me-2"></i>Artistic Enhancement</h5>
                                <p class="card-text">Transform creative works, giving new life to old artistic visuals.</p>
                            </div>
                        </div>
                    </div>
                    <div class="col-md-6">
                        <div class="card mb-3 h-100">
                            <div class="card-body">
                                <h5 class="card-title"><i class="bi bi-camera-reels text-primary me-2"></i>Media Production</h5>
                                <p class="card-text">Streamline post-production workflows with automated colorization.</p>
                            </div>
                        </div>
                    </div>
                    <div class="col-md-6">
                        <div class="card mb-3 h-100">
                            <div class="card-body">
                                <h5 class="card-title"><i class="bi bi-book text-primary me-2"></i>Educational Content</h5>
                                <p class="card-text">Enhance learning materials with vibrant color, improving engagement.</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>
-->

<!-- Our Dual Approach Section -->
<section class="my-5">
    <div class="container">
        <h2 class="text-center mb-5">Our Dual Approach</h2>
        
        <!-- Diffusion Model -->
        <div class="row align-items-center mb-5">
            <div class="col-lg-6">
                <h3>1. Diffusion-Based Approach</h3>
                <p>Our diffusion model works by iteratively adding and removing noise:</p>
                
                <ul>
                    <li><strong>LAB Color Space Processing</strong> - Separates luminance ("L" channel) from color ("A" and "B" channels), allowing us to denoise only color components</li>
                    <li><strong>U-Net Architecture</strong> - A 3-channel LAB input with noised AB channels passes through encoder, bottleneck, and decoder with skip connections</li>
                    <li><strong>Noise Scheduling</strong> - Carefully calibrated variance schedule controls the noise addition/removal process</li>
                    <li><strong>Forward Process</strong> - Gaussian noise is added to color channels in increasing amounts over T timesteps</li>
                    <li><strong>Reverse Process</strong> - The model learns to predict and remove noise iteratively, conditioned on grayscale input</li>
                    <li><strong>Resolution Enhancement</strong> - Bicubic interpolation upscales low-resolution outputs while preserving original grayscale details</li>
                    <li><strong>Neural Deflickering</strong> - Flawed atlas approach identifies and corrects temporal inconsistencies between frames</li>
                </ul>
                
                <p class="mt-3">
                    Our model was trained on vast.ai GPUs using keyframes from the UCF101 dataset. The diffusion process 
                    allows for high-quality colorization by gradually learning to reverse the noise addition process, guided 
                    by the grayscale input.
                </p>
                
                <div class="mt-3 mb-3">
                    <div class="text-center">
                        <video class="img-fluid rounded shadow-sm" autoplay loop muted style="max-height: 300px;">
                            <source src="{{ url_for('static', filename='gallery/diffusion/final_videos/v-Drumming-g06-c01.mp4') }}" type="video/mp4">
                        </video>
                        <small class="d-block text-muted mt-2">Diffusion Sample 3 - Drumming (Final Output)</small>
                    </div>
                </div>
                
                <a href="{{ url_for('gallery') }}" class="btn btn-outline-primary mt-2">See Diffusion Results</a>
            </div>
            <div class="col-lg-6">
                <div class="text-center">
                    <img src="{{ url_for('static', filename='landingpage/diffusion.png') }}" alt="Diffusion Process" class="img-fluid rounded shadow-sm mb-4">
                    <img src="{{ url_for('static', filename='landingpage/ddpmarch.png') }}" alt="DDPM Architecture" class="img-fluid rounded shadow-sm mb-4">
                    <img src="{{ url_for('static', filename='landingpage/ddpm.png') }}" alt="DDPM Process" class="img-fluid rounded shadow mb-4">
                    <img src="{{ url_for('static', filename='landingpage/timeattention.png') }}" alt="Time Attention" class="img-fluid rounded shadow">
                </div>
            </div>
        </div>
        
        <!-- GAN Model -->
        <div class="row align-items-center mt-5">
            <div class="col-lg-6">
                <div class="text-center">
                    <img src="{{ url_for('static', filename='landingpage/ganarch.png') }}" alt="GAN Architecture" class="img-fluid rounded shadow-sm mb-4">
                    <img src="{{ url_for('static', filename='landingpage/discriminator.png') }}" alt="Discriminator" class="img-fluid rounded shadow-sm mb-4">
                    <img src="{{ url_for('static', filename='landingpage/gen.png') }}" alt="Generator Architecture" class="img-fluid rounded shadow">
                </div>
            </div>
            <div class="col-lg-6">
                <h3>2. GAN-Based Approach</h3>
                <p>Our GAN implementation uses saliency maps to guide colorization:</p>
                
                <ul>
                    <li><strong>SCGAN-Based Generator</strong> - Modified with channel reduction at deeper layers for improved training stability</li>
                    <li><strong>Saliency Detection</strong> - Pyramid Feature Attention Network identifies visually important regions</li>
                    <li><strong>70×70 PatchGAN Discriminators</strong>:
                        <ul>
                            <li>Standard discriminator - Enforces global color realism</li>
                            <li>Attention discriminator - Focuses on salient regions for detail refinement</li>
                        </ul>
                    </li>
                    <li><strong>Loss Functions</strong> - Balanced combination of adversarial, L1, and perceptual losses</li>
                    <li><strong>Optical Flow</strong> - FastFlowNet tracks motion between frames to maintain color consistency</li>
                    <li><strong>Adaptive Color Propagation</strong> - Warps colors from keyframes to subsequent frames based on motion vectors</li>
                </ul>
                
                <p class="mt-3">
                    We deviated from the original SCGAN design by reducing channel counts as network depth increased, 
                    improving efficiency and reducing overfitting. The dual-discriminator setup enhances both global 
                    color realism and local detail accuracy.
                </p>
                
                <div class="mt-3 mb-3">
                    <div class="text-center">
                        <video class="img-fluid rounded shadow-sm" autoplay loop muted style="max-height: 600px;">
                            <source src="{{ url_for('static', filename='gallery/gan/slightly-complex/BenchPress_v_BenchPress_g01_c01.webm') }}" type="video/webm">
                        </video>
                        <small class="d-block text-muted mt-2">GAN Sample 12 - BenchPress</small>
                    </div>
                </div>
                
                <a href="{{ url_for('gallery') }}?tab=gan" class="btn btn-outline-primary mt-2">See GAN Results</a>
            </div>
        </div>
    </div>
</section>

<!-- Sample Results -->
<section class="my-5">
    <div class="container">
        <h2 class="text-center mb-4">Sample Results</h2>
        <div class="row">
            <div class="col-md-6">
                <div class="card mb-4">
                    <div class="card-header bg-primary text-white">
                        <h5 class="mb-0">Diffusion Model</h5>
                    </div>
                    <div class="card-body p-0">
                        <video class="w-100" autoplay loop muted>
                            <source src="{{ url_for('static', filename='gallery/diffusion/v_HandstandWalking_g21_c04/concatenated/output.mp4') }}" type="video/mp4">
                        </video>
                    </div>
                    <div class="card-footer">
                        <p class="mb-0">Sample 7 - Handstand Walking</p>
                    </div>
                </div>  
            </div>
            <div class="col-md-6">
                <div class="card mb-4">
                    <div class="card-header bg-primary text-white">
                        <h5 class="mb-0">GAN Model</h5>
                    </div>
                    <div class="card-body p-0">
                        <video class="w-100" autoplay loop muted>
                            <source src="{{ url_for('static', filename='gallery/gan/simple/BabyCrawling_v_BabyCrawling_g01_c02.webm') }}" type="video/webm">
                        </video>
                    </div>
                    <div class="card-footer">
                        <p class="mb-0">Sample 10 - Baby Crawling</p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Technical Challenges -->
<section class="my-5 bg-light py-5">
    <div class="container">
        <h2 class="text-center mb-4">Technical Challenges</h2>        
        <div class="mt-4">
            <div class="card border-0 shadow-sm">
                <div class="card-body">
                    <h4 class="card-title"><i class="bi bi-arrow-repeat text-primary me-2"></i>Achieving Temporal Consistency</h4>
                    
                    <div class="row mb-4">
                        <div class="col-md-12">
                            <div class="text-center">
                                <video class="img-fluid rounded shadow-sm" autoplay loop muted style="max-height: 500px; width: 85%;">
                                    <source src="{{ url_for('static', filename='gallery/diffusion/v_CricketShot_g04_c04/stage_1/010000/global_info_v_CricketShot_g04_c04.mp4') }}" type="video/mp4">
                                </video>
                                <small class="d-block text-muted mt-2">Global info visualization - Cricket Shot</small>
                            </div>
                        </div>
                    </div>
                    
                    <p>Our videos were deflickered through a two-stage neural approach:</p>
                    
                    <div class="row">
                        <div class="col-md-6">
                            <h5 class="h6">Atlas Generation Stage</h5>
                            <p>Video frames were processed by mapping each pixel's (x,y,t) coordinates to a consistent 2D atlas space using a 6-layer MLP network without positional encoding. Colors were reconstructed using an 8-layer MLP with positional encoding at 10 frequency bands. This mapping was optimized through a combination of RGB reconstruction loss (weight 5000), optical flow consistency loss (weight 500), rigidity loss (weight 1.0), and gradient preservation loss (weight 1000) for 10,000 iterations at 768×432 resolution.</p>
                        </div>
                        <div class="col-md-6">
                            <h5 class="h6">Neural Filtering Stage</h5>
                            <p>The second stage applied a UNet-based neural filter with 32 initial features followed by a TransformNet with ConvLSTM for temporal consistency. This refined the atlas-reconstructed frames to preserve details while maintaining temporal consistency. The final output, stored in the "final" directory at the original resolution, shows the video with flickering successfully removed while preserving natural motion and details.</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- References Section -->
<section class="my-5">
    <div class="container">
        <h2 class="text-center mb-4">References & Resources</h2>
        <div class="row">
            <div class="col-lg-8 mx-auto">
                <div class="list-group">
                    <a href="https://arxiv.org/abs/2006.11239" target="_blank" class="list-group-item list-group-item-action">
                        <div class="d-flex w-100 justify-content-between">
                            <h5 class="mb-1">Denoising Diffusion Probabilistic Models</h5>
                        </div>
                        <p class="mb-1">The foundation for our diffusion-based approach</p>
                    </a>
                    <a href="https://arxiv.org/abs/2303.08120" target="_blank" class="list-group-item list-group-item-action">
                        <div class="d-flex w-100 justify-content-between">
                            <h5 class="mb-1">Blind Video Deflickering by Neural Filtering with a Flawed Atlas</h5>
                        </div>
                        <p class="mb-1">Used for ensuring temporal consistency in our diffusion approach</p>
                    </a>
                    <a href="https://arxiv.org/abs/2011.05108" target="_blank" class="list-group-item list-group-item-action">
                        <div class="d-flex w-100 justify-content-between">
                            <h5 class="mb-1">SCGAN: Saliency Map-guided Colorization with Generative Adversarial Network</h5>
                        </div>
                        <p class="mb-1">The basis for our GAN-based approach</p>
                    </a>
                </div>
                <div class="mt-4 text-center">
                    <a href="https://github.com/Kazedaa/Hueshift-Video-Coloring" target="_blank" class="btn btn-outline-primary me-2">
                        <i class="bi bi-github me-1"></i>Diffusion Approach
                    </a>
                    <a href="https://github.com/SreeDakshinya/HueShift-Video-Coloring/tree/main" target="_blank" class="btn btn-outline-primary">
                        <i class="bi bi-github me-1"></i>GAN Approach
                    </a>
                </div>
            </div>
        </div>
    </div>
</section>

{% endblock %} 