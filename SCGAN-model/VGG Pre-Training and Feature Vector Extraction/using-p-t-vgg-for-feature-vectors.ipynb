{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "384d5dfc",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-04-05T08:06:14.174323Z",
     "iopub.status.busy": "2025-04-05T08:06:14.173660Z",
     "iopub.status.idle": "2025-04-05T08:06:14.179539Z",
     "shell.execute_reply": "2025-04-05T08:06:14.178595Z"
    },
    "executionInfo": {
     "elapsed": 54221,
     "status": "ok",
     "timestamp": 1737277697926,
     "user": {
      "displayName": "Singaraju B V Sreedakshinya .",
      "userId": "04308658734104705074"
     },
     "user_tz": -330
    },
    "id": "vxrqr4cX6ePN",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "92603d32-b3e3-4303-f31a-5f6f6527467b",
    "papermill": {
     "duration": 0.013362,
     "end_time": "2025-04-05T08:06:14.181084",
     "exception": false,
     "start_time": "2025-04-05T08:06:14.167722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !unzip /content/random-frames-ucf-101.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b00eb7",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-04-05T08:06:14.190706Z",
     "iopub.status.busy": "2025-04-05T08:06:14.190391Z",
     "iopub.status.idle": "2025-04-05T08:06:14.194974Z",
     "shell.execute_reply": "2025-04-05T08:06:14.194251Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.010709,
     "end_time": "2025-04-05T08:06:14.196262",
     "exception": false,
     "start_time": "2025-04-05T08:06:14.185553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# class VGGReconstructor(nn.Module):\n",
    "\n",
    "#   def __init__(self):\n",
    "#     super(VGGReconstructor, self).__init__()\n",
    "\n",
    "#     self.block1 = nn.Sequential(\n",
    "#         nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=2, padding=1),\n",
    "#         nn.BatchNorm2d(64),\n",
    "#         nn.ReLU()\n",
    "#     )\n",
    "\n",
    "#     self.block2 = nn.Sequential(\n",
    "#         nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.BatchNorm2d(128),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1),\n",
    "#         nn.BatchNorm2d(128),\n",
    "#         nn.ReLU()        \n",
    "#     )\n",
    "\n",
    "#     self.block3 = nn.Sequential(\n",
    "#         nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.BatchNorm2d(256),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.BatchNorm2d(256),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=2, padding=1),\n",
    "#         nn.BatchNorm2d(256),\n",
    "#         nn.ReLU()\n",
    "#     )\n",
    "\n",
    "#     self.block4 = nn.Sequential(\n",
    "#         nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.BatchNorm2d(512),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.BatchNorm2d(512),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=2, padding=1),\n",
    "#         nn.BatchNorm2d(512),\n",
    "#         nn.ReLU()        \n",
    "#     )\n",
    "\n",
    "#     self.block5 = nn.Sequential(\n",
    "#         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.BatchNorm2d(512),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.BatchNorm2d(512),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.BatchNorm2d(512),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=2, padding=1),\n",
    "#         nn.BatchNorm2d(512),\n",
    "#         nn.ReLU()\n",
    "#     )\n",
    "\n",
    "#     self.post5 = nn.Sequential(\n",
    "#         nn.Upsample(scale_factor=2, mode='nearest')\n",
    "#     )\n",
    "\n",
    "#     self.block6 = nn.Sequential(\n",
    "#         nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Upsample(scale_factor=2, mode='nearest')\n",
    "#     )\n",
    "\n",
    "#     self.block7 = nn.Sequential(\n",
    "#         nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Upsample(scale_factor=2, mode='nearest')\n",
    "#     )\n",
    "\n",
    "#     self.block8 = nn.Sequential(\n",
    "#         nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Upsample(scale_factor=2, mode='nearest')\n",
    "#     )\n",
    "\n",
    "#     self.block9 = nn.Sequential(\n",
    "#         nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.ReLU(),\n",
    "#         nn.ConvTranspose2d(in_channels=64, out_channels=1, kernel_size=4, stride=2, padding=1)        \n",
    "#     )\n",
    "\n",
    "#   def forward(self, x_in):\n",
    "#         out1 = self.block1(x_in)\n",
    "#         out2 = self.block2(out1)\n",
    "#         out3 = self.block3(out2)\n",
    "#         out4 = self.block4(out3)\n",
    "#         out5 = self.block5(out4)\n",
    "#         out5 = self.post5(out5)\n",
    "#         out6 = self.block6(torch.cat([out4, out5], dim=1))\n",
    "#         out7 = self.block7(torch.cat([out3, out6], dim=1))\n",
    "#         out8 = self.block8(torch.cat([out2, out7], dim=1))\n",
    "#         out9 = self.block9(torch.cat([out1, out8], dim=1))\n",
    "\n",
    "#         return out9 #torch.nn.functional.interpolate(out9, size=x_in.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afdfc0ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T08:06:14.204859Z",
     "iopub.status.busy": "2025-04-05T08:06:14.204599Z",
     "iopub.status.idle": "2025-04-05T08:06:19.755692Z",
     "shell.execute_reply": "2025-04-05T08:06:19.754945Z"
    },
    "papermill": {
     "duration": 5.557186,
     "end_time": "2025-04-05T08:06:19.757602",
     "exception": false,
     "start_time": "2025-04-05T08:06:14.200416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class VGGReconstructor(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(VGGReconstructor, self).__init__()\n",
    "\n",
    "    self.block1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.block2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.ReLU()        \n",
    "    )\n",
    "\n",
    "    self.block3 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "    self.block4 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(512),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(512),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(512),\n",
    "        nn.ReLU()        \n",
    "    )\n",
    "\n",
    "    self.block5 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(512),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(512),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(512),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(512),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "  def forward(self, x_in):\n",
    "        out1 = self.block1(x_in)\n",
    "        out2 = self.block2(out1)\n",
    "        out3 = self.block3(out2)\n",
    "        out4 = self.block4(out3)\n",
    "        out5 = self.block5(out4)\n",
    "\n",
    "        return out5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5642696a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T08:06:19.766210Z",
     "iopub.status.busy": "2025-04-05T08:06:19.765824Z",
     "iopub.status.idle": "2025-04-05T08:06:26.462259Z",
     "shell.execute_reply": "2025-04-05T08:06:26.461545Z"
    },
    "papermill": {
     "duration": 6.702219,
     "end_time": "2025-04-05T08:06:26.463799",
     "exception": false,
     "start_time": "2025-04-05T08:06:19.761580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # type: ignore\n",
    "import os\n",
    "import pandas as pd # type: ignore\n",
    "from tqdm import tqdm # type: ignore\n",
    "import glob\n",
    "import torch\n",
    "from torch.nn import DataParallel as DDP\n",
    "import torch.nn as nn # type: ignore\n",
    "import torch.nn.functional as F # type: ignore\n",
    "import torch.optim as optim # type: ignore\n",
    "from torch.utils.data import Dataset, DataLoader # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "import torchvision\n",
    "import torchvision.models as models # type: ignore\n",
    "import torchvision.transforms as transforms # type: ignore\n",
    "from PIL import Image # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e07c5704",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T08:06:26.472440Z",
     "iopub.status.busy": "2025-04-05T08:06:26.472057Z",
     "iopub.status.idle": "2025-04-05T08:06:26.476510Z",
     "shell.execute_reply": "2025-04-05T08:06:26.475696Z"
    },
    "executionInfo": {
     "elapsed": 12637,
     "status": "ok",
     "timestamp": 1737277710528,
     "user": {
      "displayName": "Singaraju B V Sreedakshinya .",
      "userId": "04308658734104705074"
     },
     "user_tz": -330
    },
    "id": "ki-uW-jv194T",
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.009857,
     "end_time": "2025-04-05T08:06:26.477742",
     "exception": false,
     "start_time": "2025-04-05T08:06:26.467885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np # type: ignore\n",
    "# import os\n",
    "# import pandas as pd # type: ignore\n",
    "# from tqdm import tqdm # type: ignore\n",
    "# import glob\n",
    "# import torch\n",
    "# from torch.nn import DataParallel as DDP\n",
    "# import torch.nn as nn # type: ignore\n",
    "# import torch.nn.functional as F # type: ignore\n",
    "# import torch.optim as optim # type: ignore\n",
    "# from torch.utils.data import Dataset, DataLoader # type: ignore\n",
    "# import matplotlib.pyplot as plt # type: ignore\n",
    "# import torchvision\n",
    "# import torchvision.models as models # type: ignore\n",
    "# import torchvision.transforms as transforms # type: ignore\n",
    "# from PIL import Image # type: ignore\n",
    "\n",
    "# # Training utilities\n",
    "# class AverageMeter:\n",
    "#     def __init__(self):\n",
    "#         self.reset()\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.val = 0\n",
    "#         self.avg = 0\n",
    "#         self.sum = 0\n",
    "#         self.count = 0\n",
    "\n",
    "#     def update(self, val, n=1):\n",
    "#         self.val = val\n",
    "#         self.sum += val * n\n",
    "#         self.count += n\n",
    "#         self.avg = self.sum / self.count\n",
    "\n",
    "# def save_checkpoint(state, filename):\n",
    "#     torch.save(state, filename)\n",
    "#     print(f\"Checkpoint saved: {filename}\")\n",
    "\n",
    "# def load_checkpoint(model, optimizer, filename):\n",
    "#     if os.path.isfile(filename):\n",
    "#         checkpoint = torch.load(filename)\n",
    "#         model.module.load_state_dict(checkpoint['state_dict'])\n",
    "#         optimizer.load_state_dict(checkpoint['optimizer_G'])\n",
    "#         return checkpoint['epoch']\n",
    "#     return 0\n",
    "\n",
    "# # def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "# #     model.train()\n",
    "# #     losses = AverageMeter()\n",
    "\n",
    "# #     with tqdm(train_loader, desc=\"Training\") as pbar:\n",
    "# #         for features in pbar:\n",
    "# #             features = features.to(device)\n",
    "\n",
    "# #             optimizer.zero_grad()\n",
    "# #             color_output = model(features)\n",
    "\n",
    "            \n",
    "\n",
    "# #             # Total loss\n",
    "# #             loss = color_loss \n",
    "\n",
    "# #             loss.backward()\n",
    "# #             optimizer.step()\n",
    "\n",
    "# #             losses.update(loss.item(), features.size(0))\n",
    "# #             pbar.set_postfix({'Loss': f'{losses.avg:.4f}'})\n",
    "\n",
    "# #     return losses.avg\n",
    "\n",
    "# # def validate(model, val_loader, criterion, device):\n",
    "# #     model.eval()\n",
    "# #     losses = AverageMeter()\n",
    "\n",
    "# #     with torch.no_grad():\n",
    "# #         for features in tqdm(val_loader, desc=\"Validating\"):\n",
    "# #             features = features.to(device)\n",
    "\n",
    "# #             color_output, saliency_map = model(features)\n",
    "\n",
    "# #             # Calculate color loss\n",
    "# #             color_target = features[:, :2, :, :]\n",
    "# #             color_loss = criterion(color_output, color_target)\n",
    "\n",
    "# #             # Calculate saliency losses\n",
    "# #             saliency_target = torch.mean(features, dim=1, keepdim=True)\n",
    "# #             saliency_loss = criterion(saliency_map, saliency_target)\n",
    "\n",
    "# #             # Total loss\n",
    "# #             loss = color_loss + saliency_loss\n",
    "\n",
    "# #             losses.update(loss.item(), features.size(0))\n",
    "\n",
    "# #     return losses.avg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f293bdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T08:06:26.485931Z",
     "iopub.status.busy": "2025-04-05T08:06:26.485624Z",
     "iopub.status.idle": "2025-04-05T08:06:26.490387Z",
     "shell.execute_reply": "2025-04-05T08:06:26.489720Z"
    },
    "executionInfo": {
     "elapsed": 337,
     "status": "ok",
     "timestamp": 1737277747369,
     "user": {
      "displayName": "Singaraju B V Sreedakshinya .",
      "userId": "04308658734104705074"
     },
     "user_tz": -330
    },
    "id": "r2LwcYW_ybEX",
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.010385,
     "end_time": "2025-04-05T08:06:26.491664",
     "exception": false,
     "start_time": "2025-04-05T08:06:26.481279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# import torchvision.datasets as datasets\n",
    "# import natsort\n",
    "# from torch.nn import DataParallel as DDP\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# class CustomDataSet(Dataset):\n",
    "#     def __init__(self, root, transform):\n",
    "#         self.main_dir = root\n",
    "#         self.transform = transform\n",
    "#         all_imgs = os.listdir(root)\n",
    "#         self.total_imgs = natsort.natsorted(all_imgs)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.total_imgs)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_loc = os.path.join(self.main_dir, self.total_imgs[idx])\n",
    "#         image = Image.open(img_loc).convert(\"RGB\")\n",
    "#         tensor_image = self.transform(image)\n",
    "#         return tensor_image\n",
    "        \n",
    "# def main():\n",
    "#     # Setup\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     batch_size = 32\n",
    "#     num_epochs = 50\n",
    "#     learning_rate = 0.0002\n",
    "#     beta1 = 0.5\n",
    "#     beta2 = 0.999\n",
    "#     print('BBBBB')\n",
    "#     output_dir = '/kaggle/working/'\n",
    "#     model = DDP(VGGReconstructor()).to(device)\n",
    "#     loss_fn = nn.MSELoss()\n",
    "#     optimizer = torch.optim.Adam(params=model.parameters(), lr = learning_rate)\n",
    "\n",
    "#     transform_image = transforms.Compose([\n",
    "#             transforms.Resize(128),\n",
    "#             transforms.CenterCrop(128),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                  std=[0.229, 0.224, 0.225]),\n",
    "#             transforms.Grayscale(num_output_channels=1)\n",
    "#         ])\n",
    "    \n",
    "#     dataset = CustomDataSet(\n",
    "#     root='/kaggle/input/random-frames-ucf-101/Data',\n",
    "#     transform=transform_image\n",
    "#     )\n",
    "#     print('CCCCC')\n",
    "#     # from torch.utils.data import Subset\n",
    "#     # train_indices = range(0, int(0.75*len(dataset)))\n",
    "#     # test_indices = range(int(0.75*len(dataset)), len(dataset))\n",
    "#     # train_dataset = Subset(dataset, train_indices)\n",
    "#     # test_dataset = Subset(dataset, test_indices)\n",
    "#     train_dataset, test_dataset = torch.utils.data.random_split(dataset, [int(0.8*len(dataset)), len(dataset)-int(0.8*len(dataset))])\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "#     print('AAAAAAA')\n",
    "#     best_loss = float('inf')\n",
    "#     for epoch in range(num_epochs):\n",
    "#         print({'Epoch': f'{epoch}'})\n",
    "#         model.train()\n",
    "#         losses = AverageMeter()\n",
    "        \n",
    "#         with tqdm(train_loader, desc=\"Training\") as pbar:\n",
    "#             for features in pbar:\n",
    "#                 optimizer.zero_grad()\n",
    "#                 x_in = features.to(device)\n",
    "#                 y = x_in\n",
    "#                 y_out = model(x_in)\n",
    "#                 loss = loss_fn(y_out, y) #/ y.shape[0]\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#                 losses.update(loss.item(), features.size(0))\n",
    "#                 # pbar.set_postfix({'Loss': f'{losses.avg:.4f}'})\n",
    "\n",
    "        \n",
    "#         # Save checkpoint\n",
    "#         is_best = losses.avg < best_loss\n",
    "#         best_loss = min(losses.avg, best_loss)\n",
    "    \n",
    "#         save_checkpoint({\n",
    "#             'epoch': epoch + 1,\n",
    "#             'state_dict': model.module.state_dict(),\n",
    "#             'optimizer': optimizer.state_dict(),\n",
    "#             'best_loss': best_loss,\n",
    "#         }, os.path.join(output_dir, 'latest_checkpoint.pth'))\n",
    "    \n",
    "#         if is_best:\n",
    "#             save_checkpoint({\n",
    "#                 'epoch': epoch + 1,\n",
    "#                 'state_dict': model.module.state_dict(),\n",
    "#                 'optimizer': optimizer.state_dict(),\n",
    "#                 'best_loss': best_loss,\n",
    "#             }, os.path.join(output_dir, 'best_model.pth'))\n",
    "            \n",
    "#     #Visualize results every 5 epochs\n",
    "#         if (epoch + 1) % 5 == 0:\n",
    "#             model.eval()\n",
    "#             with torch.no_grad():\n",
    "#                 sample_batch = next(iter(train_loader))\n",
    "#                 sample_features = sample_batch.to(device)\n",
    "#                 out = model(sample_features)\n",
    "#                 out = out[0].unsqueeze(0)\n",
    "#                 # sample_features = sample_features[0].unsqueeze(0)\n",
    "                \n",
    "#                 # Save colorization outputs\n",
    "#                 # plt.figure(figsize=(15, 5))\n",
    "#                 # out_img = out.cpu().numpy()\n",
    "#                 #     # color_img = (color_img - color_img.min()) / (color_img.max() - color_img.min())\n",
    "#                 #     # rgb_img = np.zeros((color_img.shape[1], color_img.shape[2], 3))\n",
    "#                 #     # rgb_img[:, :, 1:] = np.transpose(color_img, (1, 2, 0))\n",
    "\n",
    "#                 #     # plt.subplot(1, 4, i+1)\n",
    "#                 # color_img = np.transpose(out_img[0], (1, 2, 0))\n",
    "#                 # plt.imshow(color_img)\n",
    "#                 # plt.axis('off')\n",
    "#                 # Denormalize the output image\n",
    "#                 mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
    "#                 std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
    "#                 out_denorm = out * std + mean\n",
    "#                 # in_denorm = sample_features * std + mean\n",
    "                \n",
    "#                 # Convert to displayable format\n",
    "#                 out_img = out_denorm.cpu().numpy()\n",
    "#                 # in_img = in_denorm.cpu().numpy()\n",
    "#                 color_img = np.transpose(out_img[0], (1, 2, 0))\n",
    "#                 # in_img = np.transpose(in_img[0], (1, 2, 0))\n",
    "                \n",
    "#                 # Ensure values are in valid range for display\n",
    "#                 color_img = np.clip(color_img, 0, 1)\n",
    "#                 # in_img = np.clip(in_img, 0, 1)\n",
    "                \n",
    "#                 plt.imshow(color_img)\n",
    "#                 plt.axis('off')\n",
    "#                 plt.savefig(os.path.join(output_dir, f'epoch_{epoch+1}_out.png'))\n",
    "#                 plt.close()\n",
    "\n",
    "#                 # plt.imshow(in_img)\n",
    "#                 # plt.axis('off')\n",
    "#                 # plt.savefig(os.path.join(output_dir, f'epoch_{epoch+1}_input.png'))\n",
    "#                 # plt.close()\n",
    "        \n",
    "#         print(f\"Average loss after epoch: {losses.avg}\")\n",
    "#     print(f\"Train Loss: {losses.avg}\")\n",
    "\n",
    "    \n",
    "#     model.eval()\n",
    "#     losses = AverageMeter()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for features in tqdm(test_loader, desc=\"Validating\"):\n",
    "#             x_in = features.to(device)\n",
    "#             y = x_in\n",
    "#             y_out = model(x_in)\n",
    "#             loss = loss_fn(y_out, y)\n",
    "#             losses.update(loss.item(), features.size(0))\n",
    "\n",
    "#     print(f\"Test Loss: {losses.avg:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c459efaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T08:06:26.499583Z",
     "iopub.status.busy": "2025-04-05T08:06:26.499367Z",
     "iopub.status.idle": "2025-04-05T08:06:26.502072Z",
     "shell.execute_reply": "2025-04-05T08:06:26.501445Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.007986,
     "end_time": "2025-04-05T08:06:26.503341",
     "exception": false,
     "start_time": "2025-04-05T08:06:26.495355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# s=4\n",
    "# data = np.load('/kaggle/input/featurevgg-npz/extracted_features.npz', mmap_mode='r')\n",
    "# keys = list(data.files)\n",
    "# # key = keys[s]\n",
    "# # features = data[key]\n",
    "# # features = features.reshape(-1, features.shape[-2], 1, 1)\n",
    "# # features.extend([torch.from_numpy(feat) for feat in features])\n",
    "# x = data['64601.jpg']\n",
    "# print(x.shape)\n",
    "# x = x.reshape(7, -1, x.shape[-1])\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20e74c83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T08:06:26.511123Z",
     "iopub.status.busy": "2025-04-05T08:06:26.510909Z",
     "iopub.status.idle": "2025-04-05T08:06:26.514071Z",
     "shell.execute_reply": "2025-04-05T08:06:26.513480Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.008553,
     "end_time": "2025-04-05T08:06:26.515380",
     "exception": false,
     "start_time": "2025-04-05T08:06:26.506827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# batch_size=16    \n",
    "# from torch.utils.data import Dataset, DataLoader # type: ignore\n",
    "# from torch.utils.data import Subset\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import natsort\n",
    "# import torchvision.transforms as transforms # type: ignore\n",
    "\n",
    "# class CustomDataSet(Dataset):\n",
    "#     def __init__(self, root, transform):\n",
    "#         self.main_dir = root\n",
    "#         self.transform = transform\n",
    "#         all_imgs = os.listdir(root)\n",
    "#         self.total_imgs = natsort.natsorted(all_imgs)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.total_imgs)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_loc = os.path.join(self.main_dir, self.total_imgs[idx])\n",
    "#         image = Image.open(img_loc).convert(\"RGB\")\n",
    "#         tensor_image = self.transform(image)\n",
    "#         return tensor_image\n",
    "\n",
    "# transform_image = transforms.Compose([\n",
    "#             transforms.Resize(128),\n",
    "#             transforms.CenterCrop(128),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                  std=[0.229, 0.224, 0.225])\n",
    "#         ])\n",
    "# dataset = CustomDataSet(\n",
    "# root='/kaggle/input/random-frames-ucf-101/Data',\n",
    "# transform=transform_image\n",
    "# )\n",
    "\n",
    "# train_indices = range(0, int(0.75*len(dataset)))\n",
    "# test_indices = range(int(0.75*len(dataset)), len(dataset))\n",
    "# train_dataset = Subset(dataset, train_indices)\n",
    "# test_dataset = Subset(dataset, test_indices)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# np.squeeze(next(iter(test_loader))[0]).shape\n",
    "# with tqdm(train_loader, desc=\"Training\") as pbar:\n",
    "#             for features in pbar:\n",
    "#                 print(features.shape)\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de81c7f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T08:06:26.523145Z",
     "iopub.status.busy": "2025-04-05T08:06:26.522907Z",
     "iopub.status.idle": "2025-04-05T08:06:26.525953Z",
     "shell.execute_reply": "2025-04-05T08:06:26.525141Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.008408,
     "end_time": "2025-04-05T08:06:26.527359",
     "exception": false,
     "start_time": "2025-04-05T08:06:26.518951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm -rf /kaggle/working/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f33667dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T08:06:26.535433Z",
     "iopub.status.busy": "2025-04-05T08:06:26.535221Z",
     "iopub.status.idle": "2025-04-05T08:06:26.538063Z",
     "shell.execute_reply": "2025-04-05T08:06:26.537391Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.008261,
     "end_time": "2025-04-05T08:06:26.539331",
     "exception": false,
     "start_time": "2025-04-05T08:06:26.531070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# if torch.cuda.is_available():\n",
    "#     print(f'CUDA is available. Number of GPUs: {torch.cuda.device_count()}')\n",
    "# else:\n",
    "#     print('CUDA is not available. Running on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3da6862",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T08:06:26.547182Z",
     "iopub.status.busy": "2025-04-05T08:06:26.546971Z",
     "iopub.status.idle": "2025-04-05T08:06:26.549753Z",
     "shell.execute_reply": "2025-04-05T08:06:26.548928Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1737277710529,
     "user": {
      "displayName": "Singaraju B V Sreedakshinya .",
      "userId": "04308658734104705074"
     },
     "user_tz": -330
    },
    "id": "OSI0IAeW2-xV",
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.008241,
     "end_time": "2025-04-05T08:06:26.551225",
     "exception": false,
     "start_time": "2025-04-05T08:06:26.542984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# param_size = 0\n",
    "# for param in model.parameters():\n",
    "#     param_size += param.nelement() * param.element_size()\n",
    "# buffer_size = 0\n",
    "# for buffer in model.buffers():\n",
    "#     buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "# size_all_mb = (param_size + buffer_size) / 2**20\n",
    "# print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d22a7aa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T08:06:26.559464Z",
     "iopub.status.busy": "2025-04-05T08:06:26.559229Z",
     "iopub.status.idle": "2025-04-05T08:07:09.752067Z",
     "shell.execute_reply": "2025-04-05T08:07:09.751100Z"
    },
    "papermill": {
     "duration": 43.19828,
     "end_time": "2025-04-05T08:07:09.753428",
     "exception": false,
     "start_time": "2025-04-05T08:06:26.555148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing slightly-complex/BodyWeightSquats_v_BodyWeightSquats_g06_c02: 100%|██████████| 22/22 [00:02<00:00,  9.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/slightly-complex/BodyWeightSquats_v_BodyWeightSquats_g06_c02/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing slightly-complex/Billiards_v_Billiards_g06_c02: 100%|██████████| 58/58 [00:01<00:00, 49.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/slightly-complex/Billiards_v_Billiards_g06_c02/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing slightly-complex/CricketShot_v_CricketShot_g15_c03: 100%|██████████| 12/12 [00:00<00:00, 61.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/slightly-complex/CricketShot_v_CricketShot_g15_c03/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing slightly-complex/BenchPress_v_BenchPress_g01_c01: 100%|██████████| 44/44 [00:00<00:00, 49.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/slightly-complex/BenchPress_v_BenchPress_g01_c01/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing slightly-complex/Billiards_v_Billiards_g01_c01: 100%|██████████| 30/30 [00:00<00:00, 57.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/slightly-complex/Billiards_v_Billiards_g01_c01/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing slightly-complex/Biking_v_Biking_g18_c02: 100%|██████████| 24/24 [00:00<00:00, 54.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/slightly-complex/Biking_v_Biking_g18_c02/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing slightly-complex/Archery_v_Archery_g10_c01: 100%|██████████| 2/2 [00:00<00:00, 59.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/slightly-complex/Archery_v_Archery_g10_c01/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing slightly-complex/CleanAndJerk_v_CleanAndJerk_g10_c03: 100%|██████████| 116/116 [00:02<00:00, 51.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/slightly-complex/CleanAndJerk_v_CleanAndJerk_g10_c03/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing slightly-complex/CricketBowling_v_CricketBowling_g08_c04: 100%|██████████| 6/6 [00:00<00:00, 52.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/slightly-complex/CricketBowling_v_CricketBowling_g08_c04/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing slightly-complex/BenchPress_v_BenchPress_g03_c03: 100%|██████████| 46/46 [00:00<00:00, 51.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/slightly-complex/BenchPress_v_BenchPress_g03_c03/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing simple/CuttingInKitchen_v_CuttingInKitchen_g01_c01: 100%|██████████| 72/72 [00:01<00:00, 45.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/simple/CuttingInKitchen_v_CuttingInKitchen_g01_c01/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing simple/BabyCrawling_v_BabyCrawling_g05_c01: 100%|██████████| 144/144 [00:02<00:00, 50.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/simple/BabyCrawling_v_BabyCrawling_g05_c01/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing simple/ApplyEyeMakeup_v_ApplyEyeMakeup_g01_c01: 100%|██████████| 20/20 [00:00<00:00, 52.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/simple/ApplyEyeMakeup_v_ApplyEyeMakeup_g01_c01/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing simple/ApplyLipstick_v_ApplyLipstick_g03_c02: 100%|██████████| 8/8 [00:00<00:00, 56.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/simple/ApplyLipstick_v_ApplyLipstick_g03_c02/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing simple/ApplyEyeMakeup_v_ApplyEyeMakeup_g12_c04: 100%|██████████| 10/10 [00:00<00:00, 58.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/simple/ApplyEyeMakeup_v_ApplyEyeMakeup_g12_c04/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing simple/BabyCrawling_v_BabyCrawling_g01_c02: 100%|██████████| 40/40 [00:00<00:00, 54.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/simple/BabyCrawling_v_BabyCrawling_g01_c02/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing simple/ApplyEyeMakeup_v_ApplyEyeMakeup_g21_c04: 100%|██████████| 40/40 [00:00<00:00, 55.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/simple/ApplyEyeMakeup_v_ApplyEyeMakeup_g21_c04/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing simple/BrushingTeeth_v_BrushingTeeth_g01_c01: 100%|██████████| 24/24 [00:00<00:00, 57.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/simple/BrushingTeeth_v_BrushingTeeth_g01_c01/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing simple/ApplyLipstick_v_ApplyLipstick_g01_c01: 100%|██████████| 72/72 [00:01<00:00, 53.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/simple/ApplyLipstick_v_ApplyLipstick_g01_c01/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing simple/ApplyLipstick_v_ApplyLipstick_g05_c02: 100%|██████████| 42/42 [00:00<00:00, 56.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/simple/ApplyLipstick_v_ApplyLipstick_g05_c02/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing very-complex/BoxingSpeedBag_v_BoxingSpeedBag_g19_c02: 100%|██████████| 92/92 [00:01<00:00, 52.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/very-complex/BoxingSpeedBag_v_BoxingSpeedBag_g19_c02/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing very-complex/BlowingCandles_v_BlowingCandles_g09_c01: 100%|██████████| 50/50 [00:00<00:00, 55.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/very-complex/BlowingCandles_v_BlowingCandles_g09_c01/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing very-complex/Basketball_v_BasketballDunk_g17_c02: 100%|██████████| 4/4 [00:00<00:00, 60.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/very-complex/Basketball_v_BasketballDunk_g17_c02/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing very-complex/BandMarching_v_BandMarching_g25_c06: 100%|██████████| 22/22 [00:00<00:00, 52.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/very-complex/BandMarching_v_BandMarching_g25_c06/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing very-complex/BlowDryHair_v_BlowDryHair_g22_c01: 100%|██████████| 74/74 [00:01<00:00, 51.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/very-complex/BlowDryHair_v_BlowDryHair_g22_c01/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing very-complex/BalanceBeam_v_BalanceBeam_g12_c04: 100%|██████████| 50/50 [00:00<00:00, 55.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/very-complex/BalanceBeam_v_BalanceBeam_g12_c04/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing very-complex/BlowingCandles_v_BlowingCandles_g25_c01: 100%|██████████| 46/46 [00:00<00:00, 56.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/very-complex/BlowingCandles_v_BlowingCandles_g25_c01/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing very-complex/BlowingCandles_v_BlowingCandles_g23_c01: 100%|██████████| 2/2 [00:00<00:00, 56.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/very-complex/BlowingCandles_v_BlowingCandles_g23_c01/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing very-complex/BreastStroke_v_BreastStroke_g17_c02: 100%|██████████| 112/112 [00:02<00:00, 53.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/very-complex/BreastStroke_v_BreastStroke_g17_c02/grayscale_extracted_features.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing very-complex/Bowling_v_Bowling_g12_c02: 100%|██████████| 22/22 [00:00<00:00, 53.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to /kaggle/working/model_outputs/very-complex/Bowling_v_Bowling_g12_c02/grayscale_extracted_features.npz\n"
     ]
    }
   ],
   "source": [
    "class VGG16FeatureExtractor:\n",
    "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model = DDP(VGGReconstructor()).to(device)\n",
    "        self.model.load_state_dict(torch.load('/kaggle/input/pre-trained-vgg-for-colouring-gan/pytorch/default/1/best_model.pth', weights_only=True), strict=False)\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(128),\n",
    "            transforms.CenterCrop(128),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "            transforms.Grayscale(num_output_channels=1)\n",
    "        ])\n",
    "\n",
    "    def extract_features(self, image_path):\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.transform(image).unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features = self.model(image)\n",
    "        return features.cpu().numpy()\n",
    "        \n",
    "\n",
    "def main():\n",
    "    dataset_dir = '/kaggle/input/project-expo-2025-hueshift-gallery-dataset/keyframes-dataset'\n",
    "    output_dir = '/kaggle/working/model_outputs'\n",
    "\n",
    "    for type_folder in os.listdir(dataset_dir):\n",
    "        type_path = os.path.join(dataset_dir, type_folder)\n",
    "        if not os.path.isdir(type_path):\n",
    "            continue\n",
    "\n",
    "        output_type_dir = os.path.join(output_dir, type_folder)\n",
    "        os.makedirs(output_type_dir, exist_ok=True)\n",
    "\n",
    "        for vid_folder in os.listdir(type_path):\n",
    "            vid_path = os.path.join(type_path, vid_folder)\n",
    "            if not os.path.isdir(vid_path):\n",
    "                continue\n",
    "\n",
    "            output_vid_dir = os.path.join(output_type_dir, vid_folder)\n",
    "            os.makedirs(output_vid_dir, exist_ok=True)\n",
    "\n",
    "            all_features = {}\n",
    "            feature_extractor = VGG16FeatureExtractor()\n",
    "\n",
    "            for img in tqdm(os.listdir(vid_path), desc=f\"Processing {type_folder}/{vid_folder}\"):\n",
    "                try:\n",
    "                    frame_path = os.path.join(vid_path, img)\n",
    "                    features = feature_extractor.extract_features(frame_path)\n",
    "                    all_features[img] = np.vstack(features)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            if all_features:\n",
    "                output_path = os.path.join(output_vid_dir, 'grayscale_extracted_features.npz')\n",
    "                np.savez_compressed(\n",
    "                    output_path,\n",
    "                    **{img: all_features[img] for img in all_features}\n",
    "                )\n",
    "                print(f\"Features saved to {output_path}\")\n",
    "            else:\n",
    "                print(f\" No features extracted for {type_folder}/{vid_folder}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33ea9448",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T08:07:09.791568Z",
     "iopub.status.busy": "2025-04-05T08:07:09.791290Z",
     "iopub.status.idle": "2025-04-05T08:07:09.794603Z",
     "shell.execute_reply": "2025-04-05T08:07:09.793695Z"
    },
    "papermill": {
     "duration": 0.023692,
     "end_time": "2025-04-05T08:07:09.795946",
     "exception": false,
     "start_time": "2025-04-05T08:07:09.772254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm -rf /kaggle/working/*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMxghNI2VhDEcGW6CEWyMDr",
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7054053,
     "sourceId": 11282573,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 259590,
     "modelInstanceId": 237919,
     "sourceId": 277778,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 62.326712,
   "end_time": "2025-04-05T08:07:12.826958",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-05T08:06:10.500246",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
