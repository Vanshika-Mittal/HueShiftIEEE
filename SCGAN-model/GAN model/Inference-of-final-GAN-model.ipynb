{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28fe01ce",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-04-05T09:45:22.957357Z",
     "iopub.status.busy": "2025-04-05T09:45:22.957104Z",
     "iopub.status.idle": "2025-04-05T09:45:22.961148Z",
     "shell.execute_reply": "2025-04-05T09:45:22.960363Z"
    },
    "executionInfo": {
     "elapsed": 54221,
     "status": "ok",
     "timestamp": 1737277697926,
     "user": {
      "displayName": "Singaraju B V Sreedakshinya .",
      "userId": "04308658734104705074"
     },
     "user_tz": -330
    },
    "id": "vxrqr4cX6ePN",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "92603d32-b3e3-4303-f31a-5f6f6527467b",
    "papermill": {
     "duration": 0.009939,
     "end_time": "2025-04-05T09:45:22.962353",
     "exception": false,
     "start_time": "2025-04-05T09:45:22.952414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !unzip /content/random-frames-ucf-101.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c9a61b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T09:45:22.969722Z",
     "iopub.status.busy": "2025-04-05T09:45:22.969487Z",
     "iopub.status.idle": "2025-04-05T09:45:22.973034Z",
     "shell.execute_reply": "2025-04-05T09:45:22.972266Z"
    },
    "papermill": {
     "duration": 0.008381,
     "end_time": "2025-04-05T09:45:22.974260",
     "exception": false,
     "start_time": "2025-04-05T09:45:22.965879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# class VGGReconstructor(nn.Module):\n",
    "\n",
    "#   def __init__(self):\n",
    "#     super(VGGReconstructor, self).__init__()\n",
    "\n",
    "#     self.block1 = nn.Sequential(\n",
    "#         nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=2, padding=1),\n",
    "#         nn.BatchNorm2d(64),\n",
    "#         nn.ReLU()\n",
    "#     )\n",
    "\n",
    "#     self.block2 = nn.Sequential(\n",
    "#         nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.BatchNorm2d(128),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1),\n",
    "#         nn.BatchNorm2d(128),\n",
    "#         nn.ReLU()        \n",
    "#     )\n",
    "\n",
    "#     self.block3 = nn.Sequential(\n",
    "#         nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.BatchNorm2d(256),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.BatchNorm2d(256),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=2, padding=1),\n",
    "#         nn.BatchNorm2d(256),\n",
    "#         nn.ReLU()\n",
    "#     )\n",
    "\n",
    "#     self.block4 = nn.Sequential(\n",
    "#         nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.BatchNorm2d(512),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.BatchNorm2d(512),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=2, padding=1),\n",
    "#         nn.BatchNorm2d(512),\n",
    "#         nn.ReLU()        \n",
    "#     )\n",
    "\n",
    "#     self.block5 = nn.Sequential(\n",
    "#         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.BatchNorm2d(512),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.BatchNorm2d(512),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "#         nn.BatchNorm2d(512),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=2, padding=1),\n",
    "#         nn.BatchNorm2d(512),\n",
    "#         nn.ReLU()\n",
    "#     )\n",
    "\n",
    "#   def forward(self, x_in):\n",
    "#         out1 = self.block1(x_in)\n",
    "#         out2 = self.block2(out1)\n",
    "#         out3 = self.block3(out2)\n",
    "#         out4 = self.block4(out3)\n",
    "#         out5 = self.block5(out4)\n",
    "\n",
    "#         return out5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6058315d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T09:45:22.981439Z",
     "iopub.status.busy": "2025-04-05T09:45:22.981214Z",
     "iopub.status.idle": "2025-04-05T09:45:30.889362Z",
     "shell.execute_reply": "2025-04-05T09:45:30.888696Z"
    },
    "executionInfo": {
     "elapsed": 12637,
     "status": "ok",
     "timestamp": 1737277710528,
     "user": {
      "displayName": "Singaraju B V Sreedakshinya .",
      "userId": "04308658734104705074"
     },
     "user_tz": -330
    },
    "id": "ki-uW-jv194T",
    "papermill": {
     "duration": 7.913693,
     "end_time": "2025-04-05T09:45:30.891048",
     "exception": false,
     "start_time": "2025-04-05T09:45:22.977355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # type: ignore\n",
    "import os\n",
    "import pandas as pd # type: ignore\n",
    "from tqdm import tqdm # type: ignore\n",
    "import natsort\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "import torch\n",
    "from torch.nn import DataParallel as DDP\n",
    "import torch.nn as nn # type: ignore\n",
    "import torch.nn.functional as F # type: ignore\n",
    "import torch.optim as optim # type: ignore\n",
    "from torch.utils.data import Dataset, DataLoader # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "import torchvision\n",
    "import torchvision.models as models # type: ignore\n",
    "from torchvision.models import VGG16_Weights# type: ignore\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "import torchvision.transforms as transforms # type: ignore\n",
    "from PIL import Image # type: ignore\n",
    "\n",
    "\n",
    "class VGG16FeatureExtractor:\n",
    "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model = DDP(VGGReconstructor()).to(device)\n",
    "        self.model.load_state_dict(torch.load('/kaggle/input/pre-trained-vgg-for-colouring-gan/pytorch/default/1/best_model.pth', weights_only=True), strict=False)\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "\n",
    "    def extract_features(self, image):\n",
    "        image = image.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            features = self.model(image)\n",
    "        return features\n",
    "\n",
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, root, transform):\n",
    "        self.main_dir = root\n",
    "        self.transform = transform\n",
    "        all_imgs = os.listdir(root)\n",
    "        self.total_imgs = natsort.natsorted(all_imgs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.total_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = os.path.join(self.main_dir, self.total_imgs[idx])\n",
    "        image = Image.open(img_loc).convert(\"RGB\")\n",
    "        tensor_image = self.transform(image)\n",
    "        return tensor_image, self.total_imgs[idx]\n",
    "        \n",
    "def read_keyframe_file(csv_path):\n",
    "    \"\"\"Read the keyframe file with timestamp mappings.\"\"\"\n",
    "    try:\n",
    "        with open(csv_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        data = []\n",
    "        for line in lines:\n",
    "            parts = line.strip().split()\n",
    "            keyframe_num = int(parts[1])\n",
    "            timestamp = float(parts[4])\n",
    "            data.append({'keyframe': keyframe_num, 'timestamp': timestamp})\n",
    "\n",
    "        return pd.DataFrame(data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {csv_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# def process_data(base_dir):\n",
    "#     all_features={}\n",
    "#     feature_extractor = VGG16FeatureExtractor()\n",
    "#     for img in tqdm(os.listdir(base_dir)):\n",
    "#       try:\n",
    "#         frame_path = os.path.join(base_dir, img)\n",
    "#         features  = feature_extractor.extract_features(frame_path)\n",
    "#         all_features[img] = {\n",
    "#             'features': features\n",
    "#         }\n",
    "#       except Exception as e:\n",
    "#         print(f\"Error processing {img}: {str(e)}\")\n",
    "#         continue\n",
    "#     return all_features\n",
    "\n",
    "# Model architecture\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class ColorizationUNetWithAttentionDecoder(nn.Module):\n",
    "    def __init__(self, loaded_dataset, input_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder with 1x1 convolutions\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Middle layer\n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv2d(1024, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.Conv2d(1088, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.Conv2d(256 + 128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(512 + 256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        #Attention Decoder layers\n",
    "        self.layer1a = nn.Sequential(\n",
    "            # nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_channels = 512, out_channels = 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.layer1b = nn.Sequential(\n",
    "            # nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            # nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_channels = 256, out_channels = 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            # nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            # nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_channels = 384, out_channels = 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "            # nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_channels = 128, out_channels = 1, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "        # Output layers\n",
    "        self.final_color = nn.Conv2d(512, 3, kernel_size=3, stride=1, padding=1)\n",
    "        self.saliency_dec3 = nn.Conv2d(128, 1, kernel_size=3, stride=1, padding=1)\n",
    "        self.saliency_dec2 = nn.Conv2d(256, 1, kernel_size=3, stride=1, padding=1)\n",
    "        self.saliency_dec1 = nn.Conv2d(512, 1, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.loaded_dataset = loaded_dataset\n",
    "\n",
    "        # self.helper_model = VGG16FeatureExtractor()\n",
    "        \n",
    "    def forward(self, x, img_name):\n",
    "        # Encoder path\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(enc1)\n",
    "        enc3 = self.enc3(enc2)\n",
    "        # print(\"zzzzzzzzzz\")\n",
    "        # print(img_name)\n",
    "        features_list = []\n",
    "        for name in img_name:\n",
    "            # Get the actual features for this name\n",
    "            feature = self.loaded_dataset.data[name]\n",
    "            # Convert to tensor and ensure correct shape\n",
    "            feature_tensor = torch.from_numpy(feature).float()\n",
    "            if len(feature_tensor.shape) == 2:\n",
    "                feature_tensor = feature_tensor.reshape(1, *feature_tensor.shape)\n",
    "            features_list.append(feature_tensor)\n",
    "        \n",
    "        # Stack the features\n",
    "        from_vgg = torch.stack(features_list)\n",
    "        \n",
    "        # Ensure batch sizes match\n",
    "        batch_size = enc3.size(0)\n",
    "        if from_vgg.size(0) != batch_size:\n",
    "            if from_vgg.size(0) < batch_size:\n",
    "                # Repeat last feature if needed\n",
    "                repeats = from_vgg[-1:].repeat(batch_size - from_vgg.size(0), 1, 1, 1)\n",
    "                from_vgg = torch.cat([from_vgg, repeats], dim=0)\n",
    "            else:\n",
    "                # Take only what's needed\n",
    "                from_vgg = from_vgg[:batch_size]\n",
    "        from_vgg = torch.nn.functional.interpolate(from_vgg, size=enc3.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        # from_vgg = self.helper_model.extract_features(x)\n",
    "        # enc3 = nn.functional.interpolate(enc3, size=from_vgg[1], mode=\"bilinear\", align_corners=False)\n",
    "        # print(enc3.shape)\n",
    "        # print(\"QQQQQQQQq\")\n",
    "        # print(from_vgg.shape)\n",
    "        from_vgg = from_vgg.to(enc3.device)\n",
    "        # print(\"bbbbbb\")\n",
    "        enc3_new = torch.cat([enc3, from_vgg], dim=1)\n",
    "        # print(f\"enc3 shape: {enc3.shape}\")\n",
    "        # print(f\"from_vgg shape: {from_vgg.shape}\")\n",
    "        # print(f\"enc3_new shape after cat: {enc3_new.shape}\")\n",
    "        # Middle\n",
    "        middle = self.middle(enc3_new)\n",
    "        # print(\"AAAAAAAA\")\n",
    "\n",
    "        # Decoder path with skip connections\n",
    "        dec3 = self.dec3(torch.cat([middle, enc3_new], dim=1))\n",
    "        # print(\"BBBBBBBB\")\n",
    "        dec2 = self.dec2(torch.cat([dec3, enc2], dim=1))\n",
    "        # print(\"CCCCCCCC\")\n",
    "        dec1 = self.dec1(torch.cat([dec2, enc1], dim=1))\n",
    "        # print(dec1.shape, dec2.shape, dec3.shape)\n",
    "\n",
    "        # Output of Colorization Network\n",
    "        color_output = self.final_color(dec1)\n",
    "                # saliency3 = self.saliency_dec3(dec3) saliency2 = self.saliency_dec2(dec2) saliency1 = self.saliency_dec1(dec1)\n",
    "        # print(\"DDDDDDDDDD\")\n",
    "        #Attention Decoder\n",
    "        # att1 = nn.functional.interpolate(dec1, scale_factor=2, mode='nearest')\n",
    "        att1=dec1\n",
    "        # print(\"Before\" + str(att1.shape))\n",
    "        att1 = self.layer1a(att1)\n",
    "        # print(\"EEEEEE\")\n",
    "        # att1 = nn.functional.interpolate(att1, scale_factor=1, mode='nearest')\n",
    "        x1 = self.layer1b(att1)\n",
    "        # print(\"After\" + str(x1.shape))\n",
    "        # x1 = nn.functional.interpolate(x1, scale_factor=9/11, mode='nearest')\n",
    "\n",
    "        # att2 = nn.functional.interpolate(dec2, scale_factor=2, mode='nearest')\n",
    "        att2=dec2\n",
    "        x2 = self.layer2(att2)\n",
    "        # print(\"FFFFFFf\")\n",
    "        x3 = self.layer3(dec3)\n",
    "\n",
    "        ############################\n",
    "        #        DEBUG\n",
    "        # print(x1.shape, x2.shape, x3.shape)\n",
    "        ##################################\n",
    "        x2 = x2.to(x1.device)\n",
    "        x3 = x3.to(x1.device)\n",
    "        x = torch.cat((x1, x2, x3), 1)\n",
    "        # x = nn.functional.interpolate(x, scale_factor=2)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        saliency_out = self.final_layer(x)\n",
    "        # print(\"saliency_out\" + str(saliency_out.shape))\n",
    "        return color_output, saliency_out\n",
    "\n",
    "# Modify the Dataset class to ensure correct tensor dimensions\n",
    "# class VideoFeaturesDataset(Dataset):\n",
    "#     def __init__(self, npz_path):\n",
    "#         self.npz_path = npz_path\n",
    "#         self.data = np.load(self.npz_path, mmap_mode='r')  # Memory-map for efficient access\n",
    "#         self.keys = list(self.data.files)  # Store keys for indexing\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.keys)\n",
    "\n",
    "#     # def __getitem__(self, idx):\n",
    "#     #     key = self.keys[idx]\n",
    "#     #     features = self.data[key]\n",
    "\n",
    "#     #     # Ensure features have the correct shape (C, H, W)\n",
    "#     #     if len(features.shape) == 2:\n",
    "#     #         features = features.reshape(1, *features.shape)\n",
    "#     #     elif len(features.shape) == 3:\n",
    "#     #         features = features.reshape(-1, features.shape[-2], 1, 1)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         try:\n",
    "#             key = self.keys[idx]\n",
    "#             features = self.data[key]\n",
    "#             # print(features.shape)\n",
    "#             # Ensure features have the correct shape (C, H, W)\n",
    "#             if len(features.shape) == 2:\n",
    "#                 features = features.reshape(1, *features.shape)\n",
    "#             # elif len(features.shape) == 3:\n",
    "#             #     features = features.reshape(7, -1, features.shape[-1])\n",
    "\n",
    "#             # self.features.extend([torch.from_numpy(feat) for feat in features])\n",
    "#             # print(features.shape)\n",
    "#             return features\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error in __getitem__ for idx {idx}: {e}\")\n",
    "#             return None  # Return None explicitly to trigger error\n",
    "\n",
    "# Training utilities\n",
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def save_checkpoint(state, filename):\n",
    "    torch.save(state, filename)\n",
    "    print(f\"Checkpoint saved: {filename}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, filename):\n",
    "    if os.path.isfile(filename):\n",
    "        checkpoint = torch.load(filename, weights_only=True)\n",
    "        model.module.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_G'])\n",
    "        return checkpoint['epoch']\n",
    "    return 0\n",
    "\n",
    "# def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "#     model.train()\n",
    "#     losses = AverageMeter()\n",
    "\n",
    "#     with tqdm(train_loader, desc=\"Training\") as pbar:\n",
    "#         for features in pbar:\n",
    "#             features = features.to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             color_output, saliency_map = model(features)\n",
    "\n",
    "#             color_target = features[:, :2, :, :]\n",
    "#             color_loss = criterion(color_output, color_target)\n",
    "\n",
    "#             # Calculate saliency losses\n",
    "#             saliency_target = torch.mean(features, dim=1, keepdim=True)  # Create target from input features\n",
    "#             saliency_loss = criterion(saliency_map, saliency_target)\n",
    "\n",
    "#             # Total loss\n",
    "#             loss = color_loss + saliency_loss\n",
    "\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             losses.update(loss.item(), features.size(0))\n",
    "#             pbar.set_postfix({'Loss': f'{losses.avg:.4f}'})\n",
    "\n",
    "#     return losses.avg\n",
    "\n",
    "# def validate(model, val_loader, criterion, device):\n",
    "#     model.eval()\n",
    "#     losses = AverageMeter()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for features in tqdm(val_loader, desc=\"Validating\"):\n",
    "#             features = features.to(device)\n",
    "\n",
    "#             color_output, saliency_map = model(features)\n",
    "\n",
    "#             # Calculate color loss\n",
    "#             color_target = features[:, :2, :, :]\n",
    "#             color_loss = criterion(color_output, color_target)\n",
    "\n",
    "#             # Calculate saliency losses\n",
    "#             saliency_target = torch.mean(features, dim=1, keepdim=True)\n",
    "#             saliency_loss = criterion(saliency_map, saliency_target)\n",
    "\n",
    "#             # Total loss\n",
    "#             loss = color_loss + saliency_loss\n",
    "\n",
    "#             losses.update(loss.item(), features.size(0))\n",
    "\n",
    "#     return losses.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cff3561",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T09:45:30.899076Z",
     "iopub.status.busy": "2025-04-05T09:45:30.898691Z",
     "iopub.status.idle": "2025-04-05T09:45:30.917854Z",
     "shell.execute_reply": "2025-04-05T09:45:30.917037Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1737277710528,
     "user": {
      "displayName": "Singaraju B V Sreedakshinya .",
      "userId": "04308658734104705074"
     },
     "user_tz": -330
    },
    "id": "kFiQ8UMrzF55",
    "papermill": {
     "duration": 0.024373,
     "end_time": "2025-04-05T09:45:30.919137",
     "exception": false,
     "start_time": "2025-04-05T09:45:30.894764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Use global pooling to handle 1x1 inputs\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Final layer\n",
    "        self.fc = nn.Conv2d(512, 1, kernel_size=1)\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(128)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(256)\n",
    "        self.batch_norm4 = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.leaky_relu(self.conv1(x)))\n",
    "        x = self.dropout(self.leaky_relu(self.batch_norm2(self.conv2(x))))\n",
    "        x = self.dropout(self.leaky_relu(self.batch_norm3(self.conv3(x))))\n",
    "        x = self.dropout(self.leaky_relu(self.batch_norm4(self.conv4(x))))\n",
    "\n",
    "        # Global pooling to handle small spatial dimensions\n",
    "        x = self.global_pool(x)\n",
    "\n",
    "        # Final prediction\n",
    "        x = self.fc(x)\n",
    "        # x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class GANLoss_autocast_compatible:\n",
    "    def __init__(self, device):\n",
    "        self.register_buffer = lambda name, tensor: setattr(self, name, tensor)\n",
    "        self.register_buffer('real_label', torch.tensor(1.0).to(device))\n",
    "        self.register_buffer('fake_label', torch.tensor(0.0).to(device))\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "        self.device = device\n",
    "\n",
    "    # def get_target_tensor(self, prediction, target_is_real):\n",
    "    #     if target_is_real:\n",
    "    #         target_tensor = self.real_label\n",
    "    #     else:\n",
    "    #         target_tensor = self.fake_label\n",
    "    #     return target_tensor.expand_as(prediction)\n",
    "\n",
    "    def get_target_tensor(self, prediction, target_value):\n",
    "        target_tensor = torch.full_like(prediction, target_value, device=self.device)\n",
    "        return target_tensor\n",
    "\n",
    "    # def __call__(self, prediction, target_is_real):\n",
    "    #     target_tensor = self.get_target_tensor(prediction, target_is_real)\n",
    "    #     return self.loss(prediction, target_tensor)\n",
    "\n",
    "    def __call__(self, prediction, target_value):\n",
    "        target_tensor = self.get_target_tensor(prediction, target_value)\n",
    "        return self.loss(prediction, target_tensor)\n",
    "\n",
    "def load_checkpoint(model, optimizer, filename):\n",
    "    if os.path.isfile(filename):\n",
    "        checkpoint = torch.load(filename, weights_only=True)\n",
    "        model.module.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_G'])\n",
    "        return checkpoint['epoch']\n",
    "    return 0\n",
    "\n",
    "def train_gan_epoch(generator, color_discriminator, attention_discriminator,\n",
    "                   train_loader, gan_criterion_d, optimizer_G, optimizer_D_color,\n",
    "                   optimizer_D_attention, device, scaler, scaler2, Grayscale_creator, epoch):\n",
    "\n",
    "    generator.train()\n",
    "    color_discriminator.train()\n",
    "    attention_discriminator.train()\n",
    "\n",
    "    losses_G = AverageMeter()\n",
    "    losses_D_color = AverageMeter()\n",
    "    losses_D_attention = AverageMeter()\n",
    "\n",
    "    with tqdm(train_loader, desc=\"Training GAN\") as pbar:\n",
    "        for features, img_name in pbar:\n",
    "            batch_size = features.size(0)\n",
    "            features = features.to(device)\n",
    "\n",
    "            # Ground truth labels\n",
    "            # real_color = features[:, :2, :, :]\n",
    "            real_color = features[:, :, :]\n",
    "            real_saliency = torch.mean(features, dim=1, keepdim=True)\n",
    "\n",
    "            noise_factor = max(0.01, 0.1 * (0.99 ** epoch))\n",
    "            real_color_noisy = real_color + noise_factor * torch.randn_like(real_color)\n",
    "\n",
    "            #Train Discriminator\n",
    "            optimizer_D_color.zero_grad()\n",
    "            optimizer_D_attention.zero_grad()\n",
    "\n",
    "            # Generate fake outputs\n",
    "            # print(\"here\")\n",
    "            # print(features.shape)\n",
    "            with autocast(device_type=\"cuda\"):\n",
    "                gray_input = Grayscale_creator(features)\n",
    "                gray_input_noisy = gray_input + noise_factor * torch.randn_like(gray_input)\n",
    "                fake_color, fake_saliency = generator(gray_input, img_name)\n",
    "\n",
    "                # Color discriminator\n",
    "                pred_real_color = color_discriminator(real_color)\n",
    "                loss_D_real_color = gan_criterion_d(pred_real_color, 0.9)\n",
    "    \n",
    "                pred_fake_color = color_discriminator(fake_color.detach())\n",
    "                loss_D_fake_color = gan_criterion_d(pred_fake_color, False)\n",
    "    \n",
    "                loss_D_color = (loss_D_real_color + loss_D_fake_color) * 0.5\n",
    "                \n",
    "            # scaler.scale(loss_D_color).backward()\n",
    "\n",
    "            # with autocast():\n",
    "                \n",
    "                # Attention discriminator\n",
    "                real_weighted = real_color * real_saliency\n",
    "                fake_weighted = fake_color.detach() * fake_saliency.detach()\n",
    "    \n",
    "                pred_real_attention = attention_discriminator(real_weighted)\n",
    "                loss_D_real_attention = gan_criterion_d(pred_real_attention, 0.9)\n",
    "    \n",
    "                pred_fake_attention = attention_discriminator(fake_weighted)\n",
    "                loss_D_fake_attention = gan_criterion_d(pred_fake_attention, False)\n",
    "\n",
    "                loss_D_attention = (loss_D_real_attention + loss_D_fake_attention) * 0.5\n",
    "                \n",
    "            scaler.scale(loss_D_attention + loss_D_color).backward()\n",
    "\n",
    "            scaler.unscale_(optimizer_D_color)\n",
    "            torch.nn.utils.clip_grad_norm_(color_discriminator.parameters(), max_norm=1.0)\n",
    "            # scaler.step(optimizer_D_color)\n",
    "            # scaler.update()\n",
    "            scaler.unscale_(optimizer_D_attention)\n",
    "            torch.nn.utils.clip_grad_norm_(attention_discriminator.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer_D_color)\n",
    "            scaler.step(optimizer_D_attention)\n",
    "            scaler.update()\n",
    "\n",
    "            #Train generators\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            with autocast(device_type=\"cuda\"):\n",
    "                # Color GAN loss\n",
    "                pred_fake_color = color_discriminator(fake_color)\n",
    "                loss_G_color = gan_criterion_d(pred_fake_color, 0.9)\n",
    "    \n",
    "                # Attention GAN loss\n",
    "                fake_weighted = fake_color * fake_saliency\n",
    "                pred_fake_attention = attention_discriminator(fake_weighted)\n",
    "                loss_G_attention = gan_criterion_d(pred_fake_attention, 0.9)\n",
    "    \n",
    "                # L1 losses\n",
    "                loss_L1_color = F.l1_loss(fake_color, real_color)\n",
    "                loss_L1_saliency = F.l1_loss(fake_saliency, real_saliency)\n",
    "    \n",
    "                # Combined generator loss\n",
    "                loss_G = (loss_G_color + loss_G_attention +\n",
    "                         1.0 * loss_L1_color + 0.5 * loss_L1_saliency)\n",
    "                \n",
    "            scaler2.scale(loss_G).backward()\n",
    "\n",
    "            scaler2.unscale_(optimizer_G)\n",
    "            # torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "            scaler2.step(optimizer_G)\n",
    "            scaler2.update()\n",
    "\n",
    "            # Update metrics\n",
    "            losses_G.update(loss_G.item(), batch_size)\n",
    "            losses_D_color.update(loss_D_color.item(), batch_size)\n",
    "            losses_D_attention.update(loss_D_attention.item(), batch_size)\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                'G_loss': f'{losses_G.avg:.4f}',\n",
    "                'D_color_loss': f'{losses_D_color.avg:.4f}',\n",
    "                'D_attention_loss': f'{losses_D_attention.avg:.4f}'\n",
    "            })\n",
    "\n",
    "    return losses_G.avg, losses_D_color.avg, losses_D_attention.avg\n",
    "\n",
    "class VideoFeaturesDataset(Dataset):\n",
    "    def __init__(self, npz_path):\n",
    "        self.npz_path = npz_path\n",
    "        self.data = np.load(self.npz_path, mmap_mode='r')  # Memory-map for efficient access\n",
    "        self.keys = list(self.data.files)  # Store keys for indexing\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            if isinstance(idx, (tuple, list)):  # If batch, fetch each separately\n",
    "                features = [self.__getitem__(i) for i in idx]\n",
    "                return torch.stack(features)  # Stack into a single tensor\n",
    "            # key = self.keys[idx]\n",
    "            features = self.data[idx]\n",
    "            # print(features.shape)\n",
    "            # Ensure features have the correct shape (C, H, W)\n",
    "            if len(features.shape) == 2:\n",
    "                features = features.reshape(1, *features.shape)\n",
    "            # elif len(features.shape) == 3:\n",
    "            #     features = features.reshape(7, -1, features.shape[-1])\n",
    "\n",
    "            # self.features.extend([torch.from_numpy(feat) for feat in features])\n",
    "            # print(features.shape)\n",
    "            return torch.from_numpy(features).float()  # Convert to tensor\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in __getitem__ for idx {idx}: {e}\")\n",
    "            return None  # Return None explicitly to trigger error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52ef0e08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T09:45:30.926258Z",
     "iopub.status.busy": "2025-04-05T09:45:30.926055Z",
     "iopub.status.idle": "2025-04-05T09:47:24.540901Z",
     "shell.execute_reply": "2025-04-05T09:47:24.539856Z"
    },
    "executionInfo": {
     "elapsed": 337,
     "status": "ok",
     "timestamp": 1737277747369,
     "user": {
      "displayName": "Singaraju B V Sreedakshinya .",
      "userId": "04308658734104705074"
     },
     "user_tz": -330
    },
    "id": "r2LwcYW_ybEX",
    "papermill": {
     "duration": 113.620493,
     "end_time": "2025-04-05T09:47:24.542755",
     "exception": false,
     "start_time": "2025-04-05T09:45:30.922262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 22\n",
      "input_channels = 3\n",
      "Total number of features loaded: 22\n",
      "Total number of samples: 12\n",
      "input_channels = 3\n",
      "Total number of features loaded: 12\n",
      "Total number of samples: 30\n",
      "input_channels = 3\n",
      "Total number of features loaded: 30\n",
      "Total number of samples: 24\n",
      "input_channels = 3\n",
      "Total number of features loaded: 24\n",
      "Total number of samples: 2\n",
      "input_channels = 3\n",
      "Total number of features loaded: 2\n",
      "Total number of samples: 6\n",
      "input_channels = 3\n",
      "Total number of features loaded: 6\n",
      "Total number of samples: 46\n",
      "input_channels = 3\n",
      "Total number of features loaded: 46\n",
      "Total number of samples: 20\n",
      "input_channels = 3\n",
      "Total number of features loaded: 20\n",
      "Total number of samples: 8\n",
      "input_channels = 3\n",
      "Total number of features loaded: 8\n",
      "Total number of samples: 10\n",
      "input_channels = 3\n",
      "Total number of features loaded: 10\n",
      "Total number of samples: 40\n",
      "input_channels = 3\n",
      "Total number of features loaded: 40\n",
      "Total number of samples: 40\n",
      "input_channels = 3\n",
      "Total number of features loaded: 40\n",
      "Total number of samples: 24\n",
      "input_channels = 3\n",
      "Total number of features loaded: 24\n",
      "Total number of samples: 42\n",
      "input_channels = 3\n",
      "Total number of features loaded: 42\n",
      "Total number of samples: 50\n",
      "input_channels = 3\n",
      "Total number of features loaded: 50\n",
      "Total number of samples: 4\n",
      "input_channels = 3\n",
      "Total number of features loaded: 4\n",
      "Total number of samples: 22\n",
      "input_channels = 3\n",
      "Total number of features loaded: 22\n",
      "Total number of samples: 50\n",
      "input_channels = 3\n",
      "Total number of features loaded: 50\n",
      "Total number of samples: 46\n",
      "input_channels = 3\n",
      "Total number of features loaded: 46\n",
      "Total number of samples: 2\n",
      "input_channels = 3\n",
      "Total number of features loaded: 2\n",
      "Total number of samples: 22\n",
      "input_channels = 3\n",
      "Total number of features loaded: 22\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    batch_size = 1\n",
    "    num_epochs = 3\n",
    "    learning_rate = 0.0002\n",
    "    beta1 = 0.5\n",
    "    beta2 = 0.999\n",
    "\n",
    "    '''base_dir= \"/kaggle/input/random-frames-ucf-101/Data\"\n",
    "\n",
    "    all_features = {}\n",
    "\n",
    "    feature_extractor = VGG16FeatureExtractor()\n",
    "\n",
    "    for img in tqdm(os.listdir(base_dir)):\n",
    "      try:\n",
    "        frame_path = os.path.join(base_dir, img)\n",
    "        features  = feature_extractor.extract_features(frame_path)\n",
    "        all_features[img] = np.vstack(features)\n",
    "      except Exception as e:\n",
    "        print(f\"Error processing img: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "    if all_features:\n",
    "      output_path = '/kaggle/working/extracted_features.npz'\n",
    "      np.savez_compressed(\n",
    "          output_path,\n",
    "          **{f\"{img}\": all_features[img]\n",
    "              for img in all_features.keys()}\n",
    "      )\n",
    "      print(f\"Features saved to {output_path}\")\n",
    "    else:\n",
    "        print(\"No features were extracted successfully\")'''\n",
    "\n",
    "    # Load data\n",
    "    # data = np.load('/kaggle/input/featurevgg-npz/extracted_features.npz')\n",
    "    # features_dict = {}\n",
    "    # timestamps_dict = {}\n",
    "\n",
    "    # for key in data.files:\n",
    "    #     features_dict[key] = data[key]\n",
    "\n",
    "    # print(f\"Number of videos loaded: {len(features_dict)}\")\n",
    "\n",
    "    # Create dataset and dataloaders\n",
    "    # dataset = VideoFeaturesDataset('/kaggle/input/featurevgg-npz/extracted_features.npz')\n",
    "\n",
    "    transform_image = transforms.Compose([\n",
    "            transforms.Resize(128),\n",
    "            transforms.CenterCrop(128),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    dataset_dir = '/kaggle/input/gallery-keyframes/keyframes-dataset'\n",
    "    output_dir = '/kaggle/working/model_outputs'\n",
    "\n",
    "    for type_folder in os.listdir(dataset_dir):\n",
    "        type_path = os.path.join(dataset_dir, type_folder)\n",
    "        if not os.path.isdir(type_path):\n",
    "            continue\n",
    "\n",
    "        output_type_dir = os.path.join(output_dir, type_folder)\n",
    "        os.makedirs(output_type_dir, exist_ok=True)\n",
    "\n",
    "        for vid_folder in os.listdir(type_path):\n",
    "            torch.cuda.empty_cache()\n",
    "            if vid_folder in ['BenchPress_v_BenchPress_g01_c01', 'ApplyLipstick_v_ApplyLipstick_g01_c01', 'BabyCrawling_v_BabyCrawling_g05_c01', 'CuttingInKitchen_v_CuttingInKitchen_g01_c01', 'CleanAndJerk_v_CleanAndJerk_g10_c03', 'Billiards_v_Billiards_g06_c02', 'BlowDryHair_v_BlowDryHair_g22_c01', 'BoxingSpeedBag_v_BoxingSpeedBag_g19_c02', 'BreastStroke_v_BreastStroke_g17_c02']:\n",
    "                continue\n",
    "            vid_path = os.path.join(type_path, vid_folder)\n",
    "            if not os.path.isdir(vid_path):\n",
    "                continue\n",
    "\n",
    "            output_vid_dir = os.path.join(output_type_dir, vid_folder)\n",
    "            os.makedirs(output_vid_dir, exist_ok=True)\n",
    "\n",
    "            #################\n",
    "            \n",
    "            dataset = CustomDataSet(\n",
    "            root=vid_path,\n",
    "            transform=transform_image\n",
    "            )\n",
    "            \n",
    "            print(f\"Total number of samples: {len(dataset)}\")\n",
    "        \n",
    "            # train_dataset, test_dataset = torch.utils.data.random_split(dataset, [int(0.8*len(dataset)), len(dataset)-int(0.8*len(dataset))])\n",
    "            # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            test_loader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)\n",
    "            # print('AAAAAAA')\n",
    "        \n",
    "            # Initialize models\n",
    "            sample_batch, img_name = next(iter(test_loader))\n",
    "            # print(f\"Here, {img_name}\")\n",
    "            # sample_batch = train_dataset\n",
    "            input_channels = sample_batch[0].shape[0]\n",
    "            print(\"input_channels = \"+str(input_channels))\n",
    "        \n",
    "            l_d_path = os.path.join('/kaggle/input/feature-vectors-for-gallery-samples/model_outputs', type_folder, vid_folder, 'grayscale_extracted_features.npz')\n",
    "            loaded_dataset = VideoFeaturesDataset(l_d_path)\n",
    "            print(f\"Total number of features loaded: {len(loaded_dataset)}\")\n",
    "            \n",
    "            generator = DDP(ColorizationUNetWithAttentionDecoder(loaded_dataset, input_channels=1)).to(device)\n",
    "            color_discriminator = DDP(Discriminator(input_channels=3)).to(device)\n",
    "            attention_discriminator = DDP(Discriminator(input_channels=3)).to(device)\n",
    "        \n",
    "            # Initialize optimizers\n",
    "            optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate*2, betas=(beta1, beta2), foreach=True, weight_decay=1e-6)\n",
    "            optimizer_D_color = optim.Adam(color_discriminator.parameters(), lr=learning_rate*0.5, betas=(beta1, beta2), foreach=True, weight_decay=1e-4)\n",
    "            optimizer_D_attention = optim.Adam(attention_discriminator.parameters(), lr=learning_rate*0.5, betas=(beta1, beta2), foreach=True, weight_decay=1e-4)\n",
    "        \n",
    "            load_checkpoint(generator, optimizer_G, '/kaggle/input/epoch-26-colorization-gan/pytorch/default/2/best_model (3).pth')\n",
    "            # load_checkpoint(color_discriminator, optimizer_D_color, '/kaggle/input/coloring-gan-23rd-epoch-checkpoint/pytorch/default/1/latest_checkpoint_Epoch_23.pth')\n",
    "            # load_checkpoint(attention_discriminator, optimizer_D_attention, '/kaggle/input/coloring-gan-23rd-epoch-checkpoint/pytorch/default/1/latest_checkpoint_Epoch_23.pth')\n",
    "        \n",
    "            scaler = GradScaler()\n",
    "            scaler_g = GradScaler()\n",
    "            Grayscale_creator = transforms.Grayscale(num_output_channels=1)\n",
    "            \n",
    "            # Loss functions\n",
    "            gan_criterion_d = GANLoss_autocast_compatible(device)\n",
    "        \n",
    "            # Create output directory\n",
    "            # output_dir = '/kaggle/working/model_outputs'\n",
    "            # os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "            # Training loop\n",
    "            best_loss = float('inf')\n",
    "            start_epoch = 0\n",
    "        \n",
    "            # Load checkpoint if exists\n",
    "            # checkpoint_path = os.path.join(output_dir, 'latest_checkpoint_Epoch_23.pth')\n",
    "        \n",
    "            num_inputs = len(dataset)\n",
    "            generator.eval()\n",
    "            with torch.no_grad():\n",
    "                sample_features, img_name = next(iter(test_loader))\n",
    "                sample_features = sample_features.to(device)\n",
    "                Grayscale_creator = transforms.Grayscale(num_output_channels=1)\n",
    "                gray_input = Grayscale_creator(sample_features)\n",
    "                fake_color, fake_saliency = generator(gray_input, img_name)\n",
    "        \n",
    "                fake_color = fake_color.to('cpu')\n",
    "                fake_saliency = fake_saliency.to('cpu')\n",
    "                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to('cpu')\n",
    "                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to('cpu')\n",
    "        \n",
    "                # Save colorization outputs\n",
    "                # plt.figure(figsize=(15, 5))\n",
    "                \n",
    "                output_col_dir = os.path.join(output_vid_dir, 'Color')\n",
    "                os.makedirs(output_col_dir, exist_ok=True)\n",
    "                for i in range(len(dataset)):\n",
    "                    out_denorm = fake_color[i] * std + mean\n",
    "                    color_img = out_denorm.cpu().numpy()\n",
    "                    color_img = np.transpose(color_img, (1, 2, 0))\n",
    "                    color_img = np.clip(color_img, 0, 1)\n",
    "        \n",
    "                    # plt.subplot(1, 4, i+1)\n",
    "                    plt.imshow(color_img)\n",
    "                    plt.axis('off')\n",
    "                    plt.savefig(os.path.join(output_col_dir, f'test_video_colorization_{i}.png'))\n",
    "                    plt.close()\n",
    "        \n",
    "                # Save saliency maps\n",
    "                # plt.figure(figsize=(15, 5))\n",
    "                output_sal_dir = os.path.join(output_vid_dir, 'Saliency')\n",
    "                os.makedirs(output_sal_dir, exist_ok=True)\n",
    "                for i in range(len(dataset)):\n",
    "                    out = fake_saliency[i].unsqueeze(0)\n",
    "                    out_expanded = out.expand(-1, 3, -1, -1)  # Expand to 3 channels\n",
    "                    out_denorm = out_expanded * std + mean\n",
    "                    saliency = out_denorm.cpu().numpy()\n",
    "                    saliency = np.transpose(saliency[0], (1, 2, 0))\n",
    "                    saliency = np.clip(saliency, 0, 1)\n",
    "        \n",
    "                    # plt.subplot(1, 4, i+1)\n",
    "                    plt.imshow(saliency)\n",
    "                    plt.axis('off')\n",
    "                plt.savefig(os.path.join(output_sal_dir, f'test_video_saliency{i}.png'))\n",
    "                \n",
    "                plt.close()\n",
    "    '''if os.path.exists(checkpoint_path):\n",
    "        start_epoch = load_checkpoint(generator, optimizer_G, checkpoint_path)\n",
    "        print(f\"Loaded checkpoint from epoch {start_epoch}\")\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        # Train\n",
    "        g_loss, d_color_loss, d_attention_loss = train_gan_epoch(\n",
    "            generator, color_discriminator, attention_discriminator,\n",
    "            train_loader, gan_criterion_d, optimizer_G,\n",
    "            optimizer_D_color, optimizer_D_attention, device, scaler, scaler_g, Grayscale_creator, epoch\n",
    "        )\n",
    "\n",
    "        print(f\"Generator Loss: {g_loss:.4f}\")\n",
    "        print(f\"Color Discriminator Loss: {d_color_loss:.4f}\")\n",
    "        print(f\"Attention Discriminator Loss: {d_attention_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        is_best = g_loss < best_loss\n",
    "        best_loss = min(g_loss, best_loss)\n",
    "\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': generator.module.state_dict(),\n",
    "            'optimizer_G': optimizer_G.state_dict(),\n",
    "            'optimizer_D_color': optimizer_D_color.state_dict(),\n",
    "            'optimizer_D_attention': optimizer_D_attention.state_dict(),\n",
    "            'best_loss': best_loss,\n",
    "        }, os.path.join(output_dir, 'latest_checkpoint.pth'))\n",
    "\n",
    "        if is_best:\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': generator.module.state_dict(),\n",
    "                'optimizer_G': optimizer_G.state_dict(),\n",
    "                'optimizer_D_color': optimizer_D_color.state_dict(),\n",
    "                'optimizer_D_attention': optimizer_D_attention.state_dict(),\n",
    "                'best_loss': best_loss,\n",
    "            }, os.path.join(output_dir, 'best_model.pth'))\n",
    "\n",
    "        # Visualize results every 5 epochs\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            generator.eval()\n",
    "            with torch.no_grad():\n",
    "                sample_features, img_name = next(iter(test_loader))\n",
    "                sample_features = sample_features.to(device)\n",
    "                Grayscale_creator = transforms.Grayscale(num_output_channels=1)\n",
    "                gray_input = Grayscale_creator(sample_features)\n",
    "                fake_color, fake_saliency = generator(gray_input, img_name)\n",
    "\n",
    "                \n",
    "                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
    "                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
    "\n",
    "                # Save colorization outputs\n",
    "                plt.figure(figsize=(15, 5))\n",
    "                for i in range(4):\n",
    "                    out_denorm = fake_color[i] * std + mean\n",
    "                    color_img = out_denorm.cpu().numpy()\n",
    "                    # color_img = (color_img - color_img.min()) / (color_img.max() - color_img.min())\n",
    "                    # rgb_img = np.zeros((color_img.shape[1], color_img.shape[2], 3))\n",
    "                    color_img = np.transpose(color_img, (1, 2, 0))\n",
    "                    color_img = np.clip(color_img, 0, 1)\n",
    "\n",
    "                    plt.subplot(1, 4, i+1)\n",
    "                    plt.imshow(color_img)\n",
    "                    plt.axis('off')\n",
    "                plt.savefig(os.path.join(output_dir, f'updated_epoch_{epoch+1}_colorization.png'))\n",
    "                plt.close()\n",
    "\n",
    "                # Save saliency maps\n",
    "                plt.figure(figsize=(15, 5))\n",
    "                for i in range(4):\n",
    "                    out = fake_saliency[i].unsqueeze(0)\n",
    "                    out_expanded = out.expand(-1, 3, -1, -1)  # Expand to 3 channels\n",
    "                    out_denorm = out_expanded * std + mean\n",
    "                    saliency = out_denorm.cpu().numpy()\n",
    "                    # saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min())\n",
    "                    saliency = np.transpose(saliency[0], (1, 2, 0))\n",
    "                    saliency = np.clip(saliency, 0, 1)\n",
    "\n",
    "                    plt.subplot(1, 4, i+1)\n",
    "                    plt.imshow(saliency)\n",
    "                    plt.axis('off')\n",
    "                plt.savefig(os.path.join(output_dir, f'updated_epoch_{epoch+1}_saliency.png'))\n",
    "                \n",
    "                plt.close()'''\n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67e9372c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T09:47:24.553962Z",
     "iopub.status.busy": "2025-04-05T09:47:24.553719Z",
     "iopub.status.idle": "2025-04-05T09:47:24.556643Z",
     "shell.execute_reply": "2025-04-05T09:47:24.555965Z"
    },
    "papermill": {
     "duration": 0.009825,
     "end_time": "2025-04-05T09:47:24.557911",
     "exception": false,
     "start_time": "2025-04-05T09:47:24.548086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# s=4\n",
    "# data = np.load('/kaggle/input/featurevgg-npz/extracted_features.npz', mmap_mode='r')\n",
    "# keys = list(data.files)\n",
    "# # key = keys[s]\n",
    "# # features = data[key]\n",
    "# # features = features.reshape(-1, features.shape[-2], 1, 1)\n",
    "# # features.extend([torch.from_numpy(feat) for feat in features])\n",
    "# x = data['64601.jpg']\n",
    "# print(x.shape)\n",
    "# x = x.reshape(7, -1, x.shape[-1])\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "338bf5bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T09:47:24.567524Z",
     "iopub.status.busy": "2025-04-05T09:47:24.567279Z",
     "iopub.status.idle": "2025-04-05T09:47:24.570338Z",
     "shell.execute_reply": "2025-04-05T09:47:24.569539Z"
    },
    "papermill": {
     "duration": 0.009344,
     "end_time": "2025-04-05T09:47:24.571612",
     "exception": false,
     "start_time": "2025-04-05T09:47:24.562268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm -rf /kaggle/working/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f44fcf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T09:47:24.581206Z",
     "iopub.status.busy": "2025-04-05T09:47:24.581009Z",
     "iopub.status.idle": "2025-04-05T09:47:24.583716Z",
     "shell.execute_reply": "2025-04-05T09:47:24.583053Z"
    },
    "papermill": {
     "duration": 0.008916,
     "end_time": "2025-04-05T09:47:24.585008",
     "exception": false,
     "start_time": "2025-04-05T09:47:24.576092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# if torch.cuda.is_available():\n",
    "#     print(f'CUDA is available. Number of GPUs: {torch.cuda.device_count()}')\n",
    "# else:\n",
    "#     print('CUDA is not available. Running on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65adb4cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T09:47:24.594848Z",
     "iopub.status.busy": "2025-04-05T09:47:24.594643Z",
     "iopub.status.idle": "2025-04-05T09:47:24.597466Z",
     "shell.execute_reply": "2025-04-05T09:47:24.596858Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1737277710529,
     "user": {
      "displayName": "Singaraju B V Sreedakshinya .",
      "userId": "04308658734104705074"
     },
     "user_tz": -330
    },
    "id": "OSI0IAeW2-xV",
    "papermill": {
     "duration": 0.009266,
     "end_time": "2025-04-05T09:47:24.598677",
     "exception": false,
     "start_time": "2025-04-05T09:47:24.589411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# param_size = 0\n",
    "# for param in model.parameters():\n",
    "#     param_size += param.nelement() * param.element_size()\n",
    "# buffer_size = 0\n",
    "# for buffer in model.buffers():\n",
    "#     buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "# size_all_mb = (param_size + buffer_size) / 2**20\n",
    "# print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMxghNI2VhDEcGW6CEWyMDr",
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7054235,
     "sourceId": 11282824,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7054265,
     "sourceId": 11282863,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 292040,
     "modelInstanceId": 271053,
     "sourceId": 321584,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 126.833846,
   "end_time": "2025-04-05T09:47:27.219407",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-05T09:45:20.385561",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
