# HueShift: Breathing Life into Every Frame üñºÔ∏è
Hueshift is a video colorization pipeline designed for frame-wise colorization of grayscale videos using a diffusion-based model, with a post-processing step to ensure temporal consistency. It operates on videos converted into individual frames and leverages keyframe detection and deflickering techniques to enhance both quality and coherence.

# Methodology
## Dataset Preparation
Source: [UCF101 action recognition dataset](https://www.kaggle.com/datasets/matthewjansen/ucf101-action-recognition) <br>
Videos were converted into frames.
Keyframes were selected using a video-keyframe-detector to focus computation on representative content.

## Saliency Map-guided Colorization with Generative Adversarial Networks

We used a Saliency-Map-guided-GAN to extract important regions and to color each of the keyframes extracted in the pre-processing stage. This was then followed by a video-reconstruction step (direct conversion was possible by virtue of the consistent coloring of the model).

### Saliency Maps
A saliency map is a visual representation that highlights the most important regions in an image. We used a Pyramid Feature Attention network to focus on effective extraction of high-level context features and low-level spatial structural features from each image.
![image](https://github.com/user-attachments/assets/85d92f93-e751-42df-a790-b6c717a8cf54)

### Generator
We adopted the generator architecture from the SCGAN framework as the foundational skeleton for our model. However, as an optimization, we deviated from the original design by reducing the number of channels as the network depth increased‚Äîcontrary to SCGAN‚Äôs approach of increasing them‚Äîto improve efficiency and reduce overfitting.
![image](https://github.com/user-attachments/assets/faf0c881-0de5-4356-bb33-de4f69ae701f)

### Discriminator
The model uses two discriminators based on the 70√ó70 PatchGAN architecture to enhance local detail and high-frequency accuracy. The first discriminator compares the generated colorized image with the ground truth, while the second‚Äîan attention-based discriminator‚Äîevaluates saliency-weighted versions of the images to better focus on important regions. This dual-discriminator setup improves realism and visual quality in the colorization results.
![image](https://github.com/user-attachments/assets/af0c9466-4ebd-4087-85c1-97622c133e96)

## Final Model Architecture (proposed)
![image](https://github.com/user-attachments/assets/cc021758-3391-41dc-961f-89abb19713c7)

### Optical Flow (proposed)
Optical flow is a technique used in computer vision to estimate the motion of objects or pixels between two consecutive frames of a video. It assumes that the intensity of a point in the image remains constant as it moves from one frame to the next. By analyzing how pixels shift in position over time, optical flow generates a vector field that represents the direction and speed of movement in the scene.

We had planned to adopt the methodology from the _UnDIVE paper_ to enhance temporal consistency in our video colorization pipeline. Specifically, we intended to use optical flow on the colorized outputs generated by our SCGAN-based model for keyframe and next-frame pairs identified during pre-processing. Using FastFlowNet, a real-time and efficient optical flow estimator, we aimed to compute the flow between the colorized keyframe (after warping) and the colorized next frame. This motion information would help align objects across frames more accurately. By applying a loss function between the warped keyframe and the next colorized frame, we sought to encourage smoother motion, consistent color transitions, and improved structural coherence throughout the video.

## References
- [Keyframe detection](https://github.com/joelibaceta/video-keyframe-detector)
- [Pyramid Feature Attention Network for Saliency detection](https://arxiv.org/abs/1903.00179)
- [Saliency Map-guided Colorization with Generative Adversarial Network](https://arxiv.org/abs/2011.11377)
- [UnDIVE: Generalized Underwater Video Enhancement Using Generative Priors](https://openaccess.thecvf.com/content/WACV2025/papers/Srinath_UnDIVE_Generalized_Underwater_Video_Enhancement_using_Generative_Priors_WACV_2025_paper.pdf)




